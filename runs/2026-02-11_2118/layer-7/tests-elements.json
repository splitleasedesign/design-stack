{
  "run_id": "2026-02-11_2118",
  "layer": 7,
  "layer_name": "Test Designer",
  "lens": {
    "host_call": "andreas-call.txt",
    "book_extract": "kahneman-part1-two-systems.txt"
  },
  "elements": [
    {
      "id": "tests-001",
      "type": "validation_strategy",
      "title": "Validate System 1 Trust Gate at Phase Transitions",
      "validates_element": "works-001",
      "journey_phases": ["discovery", "evaluation", "onboarding", "listing_creation", "pricing", "proposal_mgmt"],
      "problem": "works-001 asserts that every phase transition must pass System 1's automatic 2-second trust assessment through visual continuity, familiar language, and recognition anchors. If this fails, hosts drop off between phases even after successfully completing the prior phase. The risk is invisible dropout: hosts who completed Phase N never begin Phase N+1.",
      "solution": "Measure phase-to-phase continuation rates and correlate with the presence or absence of recognition anchors (host's property address, agent name, call-referenced terms) in the first 2 seconds of each new screen.",
      "evidence": [
        {
          "source": "andreas-call.txt, 0:09-0:50",
          "insight": "Andreas's calm, open tone at the start of the call indicates his System 1 had already approved Bryant before the conversation began, because the outreach referenced his specific property. The platform must replicate this specificity at every transition."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Associative Machine",
          "insight": "Associative coherence means phase transitions succeed when they activate the network built by the prior phase. Testing must verify that the first visual element at each transition connects to prior-phase content."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Implement event tracking at every phase boundary (evaluation-to-onboarding, onboarding-to-listing-creation, listing-creation steps 1-6, listing-to-proposal). Track two metrics per boundary: (1) continuation rate (percentage of hosts who complete Phase N and begin Phase N+1 within the same session), and (2) time-to-first-interaction on the new screen (how quickly the host performs their first action after landing). Compare continuation rates between screens that include recognition anchors (host property address, agent name) in the top visual element versus screens that open with generic content or instructions.",
      "success_criteria": "Phase-to-phase continuation rate above 70% for evaluation-to-onboarding and above 80% for onboarding-to-listing-creation. Time-to-first-interaction under 8 seconds on screens with recognition anchors. Screens with recognition anchors show at least 15% higher continuation rate than screens without.",
      "failure_meaning": "If continuation rates fall below targets despite recognition anchors being present, the anchors may not be salient enough (too small, wrong position, wrong content) or the trust gate failure is happening for reasons other than System 1 recognition (e.g., the content after the anchor is overwhelming). If time-to-first-interaction exceeds 8 seconds, the host is pausing to orient, which indicates the anchor is not providing sufficient context.",
      "implementation_hint": "Add a data attribute to the primary visual element on each transition screen (data-anchor-type='property-address' or 'agent-name' or 'none'). Track page load to first click/tap/scroll with this attribute as a dimension. Use session replay sampling on hosts who drop off at transitions to identify what the host saw last before leaving."
    },
    {
      "id": "tests-002",
      "type": "validation_strategy",
      "title": "Validate Depleted User Design Across Multi-Step Processes",
      "validates_element": "works-002",
      "journey_phases": ["onboarding", "listing_creation", "pricing", "proposal_mgmt"],
      "problem": "works-002 asserts that every phase after the first must be designed for a partially depleted System 2, with smart defaults, save-and-resume, and front-loaded hard decisions. The risk is that depleted hosts either abandon or accept bad defaults, producing incomplete or low-quality outcomes.",
      "solution": "Track step-by-step completion rates and default acceptance rates across the listing wizard, looking for the characteristic depletion curve (declining quality and increasing default acceptance in later steps).",
      "evidence": [
        {
          "source": "kahneman-part1-two-systems.txt, The Busy and Depleted System 2",
          "insight": "The parole judge study shows approval rates dropping from 65% to near zero as judges deplete. The listing wizard should show a similar pattern if depletion is not addressed: higher engagement and customization in early steps, declining to default-acceptance in later steps."
        },
        {
          "source": "andreas-call.txt, 0:15 and 5:44-6:11",
          "insight": "Andreas arrives already busy (building a bar) and by call end the exchange is brief and logistical. The platform faces a depleted user from the start."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "For each step of the listing wizard (Steps 1-6), track: (1) completion rate (percentage who finish the step), (2) time spent on the step, (3) default acceptance rate (percentage of fields left at default values), (4) customization depth (number of fields the host actively modified). Plot these four metrics across the 6 steps to visualize the depletion curve. After implementing decompression moments between Steps 2-3 and Steps 4-5, compare the curve to the pre-intervention baseline.",
      "success_criteria": "Wizard completion rate above 75% (hosts who begin Step 1 and complete Step 6). Time-to-completion under 5 minutes for 80% of hosts. Default acceptance rate in Steps 4-6 should not exceed default acceptance rate in Steps 1-3 by more than 20 percentage points. The depletion curve should flatten after decompression moments are introduced.",
      "failure_meaning": "If the completion rate is below 75%, hosts are abandoning mid-wizard, likely at the depletion peak (Steps 3-4). If default acceptance in later steps is dramatically higher than in early steps (more than 20pp difference), the smart defaults are carrying too much weight and should be audited for quality. If decompression moments do not flatten the curve, they may not be positioned at the right steps or may not feel like genuine relief.",
      "implementation_hint": "Add per-step analytics events: step_entered, step_completed, field_modified (with field name), default_accepted (with field name). Calculate depletion index = (default_rate_step_N - default_rate_step_1) / default_rate_step_1. Track depletion index as a key metric. Use Hotjar or similar session recording on a sample of hosts who abandon mid-wizard."
    },
    {
      "id": "tests-003",
      "type": "validation_strategy",
      "title": "Validate Platform Speed Matches Conversational Speed",
      "validates_element": "works-003",
      "journey_phases": ["evaluation", "onboarding", "listing_creation", "proposal_mgmt"],
      "problem": "works-003 asserts that the platform must match or exceed the conversational efficiency of the phone call. Andreas described two units in 7 seconds. If the platform requires significantly more time for equivalent information capture, hosts will perceive it as inferior to the human interaction.",
      "solution": "Measure time-to-first-listing and compare against the benchmark: hosts who have had a phone call should publish their first listing in under 10 minutes.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:03-1:10",
          "insight": "Andreas conveys 5 data points (unit count, floor levels, building type, availability) in approximately 7 seconds of natural speech. The platform must achieve comparable input speed."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Lazy Controller",
          "insight": "System 2 has a natural speed -- a comfortable stroll. The phone call operates at this speed. The platform must not force a sprint."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Track time-to-first-listing: the elapsed time from the host's first platform interaction (landing page or dashboard load) to a published listing. Segment by hosts who had a prior phone call versus hosts who did not. Additionally, measure time-per-step in the listing wizard and compare each step's time to the equivalent conversational exchange time from the call transcript (unit description: 7 seconds in call, target: under 30 seconds on platform; pricing: 14 seconds in call, target: under 60 seconds on platform).",
      "success_criteria": "Time-to-first-listing under 10 minutes for 80% of hosts who have already had a phone call. No individual wizard step exceeds 90 seconds for 80% of hosts. The ratio of platform time to conversational time for equivalent information capture does not exceed 4:1 (the platform gets a 4x time premium over conversation, not more).",
      "failure_meaning": "If time-to-first-listing exceeds 10 minutes, the platform is creating more friction than the phone call. Identify which step is the bottleneck. If the ratio exceeds 4:1 for any step, that step has interaction design problems (too many fields, unclear labels, confusing input mechanisms). If hosts without prior phone calls take significantly longer, the phone call provides context that the platform fails to replicate independently.",
      "implementation_hint": "Track timestamps at each wizard step entry and exit. Calculate per-step duration. Flag sessions where any single step exceeds 120 seconds for qualitative review. Segment all timing data by has_phone_call (boolean from CRM data) to isolate the call-to-platform speed gap."
    },
    {
      "id": "tests-004",
      "type": "validation_strategy",
      "title": "Validate Human-to-Digital Trust Bridge Through Continuity Priming",
      "validates_element": "works-004",
      "journey_phases": ["evaluation", "onboarding", "listing_creation", "retention"],
      "problem": "works-004 identifies the human-to-digital handoff as the highest-risk transition. The host's trust is with Bryant, not Split Lease. The platform arrives as an unprimed stimulus. If the digital experience does not echo the human interaction, System 1 evaluates the platform as a stranger.",
      "solution": "Measure first-visit engagement rate after the agent's follow-up email and test whether personalized first-visit screens (agent name, property address, call references) outperform generic onboarding screens.",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:27-6:11",
          "insight": "The call ends with personal promises and no platform mention. The host's primed expectation is 'Bryant will email me.' A platform signup link instead of a personal email violates the primed expectation."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Marvels of Priming",
          "insight": "Priming is not restricted to concepts and words. Emotions primed by the call transfer to the platform experience. If the call did not prime for the platform, the platform enters unprimed."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "A/B test two onboarding email variants: (A) Email from the agent's personal address (or appears to be), referencing the call, the specific property discussed, and linking to a personalized landing page showing the agent's name and photo with the host's property address pre-filled. (B) Email from noreply@splitlease.com with generic onboarding copy and a link to the standard signup page. Measure: email open rate, click-through rate, first-visit engagement rate (completing at least one action on the platform), and time from email receipt to first platform action.",
      "success_criteria": "First-visit engagement rate above 60% for the personalized variant (A). Personalized variant shows at least 25% higher click-through rate than generic variant (B). Time from email receipt to first platform action under 24 hours for the personalized variant.",
      "failure_meaning": "If the personalized variant does not significantly outperform generic, the trust transfer mechanism may need to be stronger (e.g., the agent previewing the platform during the call, not just in the email). If first-visit engagement is below 60% even with personalization, the platform itself may be creating friction that the email cannot overcome -- investigate the landing page experience.",
      "implementation_hint": "Use the CRM to populate the email template with agent name, property address, and call date. The personalized landing page should use a unique token in the URL that pre-loads the host's data. Track the entire funnel: email_sent -> email_opened -> link_clicked -> page_loaded -> first_action_completed."
    },
    {
      "id": "tests-005",
      "type": "validation_strategy",
      "title": "Validate Host-as-Advisor Moment Drives Commitment",
      "validates_element": "works-005",
      "journey_phases": ["evaluation", "listing_creation", "proposal_mgmt", "active_lease"],
      "problem": "works-005 asserts that hosts who shift from evaluator to advisor (contributing expertise rather than just data) experience psychological commitment that reduces dropout. The platform must create opportunities for this shift.",
      "solution": "Measure whether hosts who complete the advisor prompt (contributing a tip or recommendation) show higher completion rates and retention than hosts who skip it.",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:12-5:27",
          "insight": "Andreas's unprompted shift from asking questions to advising on furnishing marks the psychological commitment point. After this moment, he does not question the arrangement further."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Associative Machine",
          "insight": "When the host advises, System 1 constructs a causal story where the host is part of the outcome, creating forward momentum."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Track the advisor prompt interaction across eligible phases (listing creation after Step 2, proposal management after review, active lease check-ins). For each advisor prompt, measure: (1) engagement rate (percentage who type any response), (2) response quality (word count as a proxy), (3) downstream completion rate (percentage who complete the remaining wizard steps after the prompt). Compare downstream outcomes between hosts who engaged with the advisor prompt versus hosts who skipped it. Control for selection bias by examining the skip cohort's behavior before the prompt (if they were already showing abandonment signals, the advisor prompt is not the causal factor).",
      "success_criteria": "Host-contributed content rate above 40% (listings that include at least one host-authored recommendation). Hosts who engage with the advisor prompt show at least 15% higher listing completion rate than those who skip. Correlation between advisor-moment completion and 30-day retention rate is positive and statistically significant.",
      "failure_meaning": "If fewer than 40% of hosts engage, the prompt may be poorly timed, poorly worded, or positioned as another form field rather than an expertise solicitation. If engagement does not correlate with higher completion or retention, the advisor moment may not be creating the commitment effect predicted by the theory -- or the effect exists but is too small to measure in the current sample. Review the prompt copy and timing against the validation-before-question sequence specified in the element.",
      "implementation_hint": "Track advisor_prompt_shown, advisor_prompt_engaged (any keystroke), advisor_prompt_submitted (response saved), advisor_prompt_skipped. Calculate correlation between advisor_prompt_submitted and listing_completed, proposal_accepted, and lease_renewed. Use propensity score matching to control for selection bias in the comparison."
    },
    {
      "id": "tests-006",
      "type": "validation_strategy",
      "title": "Validate Mental Model Translation Eliminates Pricing Errors",
      "validates_element": "works-006",
      "journey_phases": ["listing_creation", "pricing", "proposal_mgmt"],
      "problem": "works-006 identifies a vocabulary mismatch between the host's mental model (monthly rent, walk-up descriptions) and the platform's data model (nightly rates, category dropdowns). This mismatch forces costly System 2 translation and produces pricing errors.",
      "solution": "Measure pricing accuracy: the alignment between the host's intended monthly income and the platform's calculated monthly income, before and after implementing native-format input with automatic translation.",
      "evidence": [
        {
          "source": "andreas-call.txt, 3:02-3:16",
          "insight": "Andreas processes pricing as a sequential sum in his native format (monthly rent, months of upfront cost). He does not think in nightly rates."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Lazy System 2",
          "insight": "The bat-and-ball problem shows that even simple translations fail when an intuitive but wrong answer is available. Hosts forced to convert between monthly and nightly pricing will produce errors."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Track pricing accuracy: compare the host's entered monthly rent (input in their native format) to the platform's calculated monthly equivalent (derived from nightly rate x occupied days). If the platform requires nightly rate input, calculate the reverse: what monthly income does the nightly rate produce, and does it match what the host likely intended? Track: (1) pricing-related support tickets (hosts reporting incorrect pricing), (2) pricing edits within 48 hours of listing publication (hosts correcting mistakes after seeing the live listing), (3) the ratio of intended monthly income to actual platform-displayed monthly income. After implementing native-format input (host enters monthly rent, platform converts to nightly), compare all three metrics to the pre-implementation baseline.",
      "success_criteria": "Pricing accuracy rate above 90% (host's intended monthly income matches platform's calculated monthly income within 10% margin). Pricing-related support tickets decrease by 50% after native-format input is implemented. Pricing edits within 48 hours decrease by 40%.",
      "failure_meaning": "If pricing accuracy remains below 90% after native-format input, the translation logic may be incorrect or the host may not understand what 'monthly rent' means in the context of periodic tenancy (e.g., the unit is only rented 4 nights per week, so 'monthly rent' is different from traditional rent). If support tickets do not decrease, the confusion may be about the pricing model itself, not just the input format.",
      "implementation_hint": "Add a hidden field that captures the host's native-format input alongside the platform's calculated values. Run a daily comparison report: native_monthly_input vs. calculated_monthly_equivalent. Flag any listing where the discrepancy exceeds 10% for manual review. Track the pricing_support_ticket tag in the support system."
    },
    {
      "id": "tests-007",
      "type": "validation_strategy",
      "title": "Validate Anchor-Then-Advance Information Hierarchy",
      "validates_element": "communicates-001",
      "journey_phases": ["onboarding", "listing_creation", "proposal_mgmt"],
      "problem": "communicates-001 asserts that every screen's primary information element must be a reflection of what the host has already provided or knows. Recognition (System 1) before new content (System 2). Without this, hosts face blank forms and generic dashboards that trigger disorientation.",
      "solution": "Use usability testing to verify that hosts recognize themselves in the screen's primary element within 2 seconds of page load.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:03-1:10",
          "insight": "Andreas describes his units fluently from known information with zero cognitive effort. The platform should present this same information back as the anchor at every step."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Associative Machine",
          "insight": "Showing the host their own data first activates their existing associative network, priming coherent engagement with new content."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Conduct moderated usability tests with 5-8 hosts across 3 key screens: (1) first dashboard visit after phone call, (2) listing wizard Step 2 opening (after Step 1 completion), (3) proposal review screen. For each screen, use eye-tracking or think-aloud protocol to answer: What does the host look at first? What do they say when they land on the screen? Do they verbalize recognition ('oh, that is my place') or confusion ('what is this?')? Time from page load to first verbalization of recognition. After the test, ask hosts to rate on a 5-point scale how much the screen felt like 'a continuation of my conversation with my agent' versus 'a new system I need to learn.'",
      "success_criteria": "At least 6 of 8 hosts verbalize recognition within 3 seconds of page load. Average 'continuation' rating is 4.0 or above on the 5-point scale. The first eye fixation on screens with host data as the primary element is on the host data (property address, agent name) at least 70% of the time.",
      "failure_meaning": "If hosts do not verbalize recognition quickly, the anchor may not be prominent enough (too small, wrong position, buried below other content). If the 'continuation' rating is low, the screen may have the right data but the wrong tone or visual treatment. If eye fixation goes elsewhere first, the visual hierarchy is not directing attention to the anchor element.",
      "implementation_hint": "For moderated tests, use a think-aloud protocol where hosts narrate their first impressions as each screen loads. Record screen and audio. For unmoderated follow-up, use a post-screen micro-survey: 'When this screen loaded, did you immediately see information about your property? Yes/No.' Track the yes-rate as a proxy for anchor effectiveness."
    },
    {
      "id": "tests-008",
      "type": "validation_strategy",
      "title": "Validate Single-Sum Financial Display Reduces Confusion",
      "validates_element": "communicates-002",
      "journey_phases": ["pricing", "proposal_mgmt"],
      "problem": "communicates-002 asserts that financial information must lead with a single total in the host's native format (monthly rent) before any breakdown. Multiple numbers at similar visual weight cause System 1 to anchor on the wrong figure.",
      "solution": "A/B test single-sum-first financial display versus multi-format simultaneous display, measuring pricing comprehension and decision speed.",
      "evidence": [
        {
          "source": "andreas-call.txt, 3:02-3:16",
          "insight": "Andreas processes pricing as a single sum: 'Three months. Okay. Got it.' He does not ask for per-night breakdowns."
        },
        {
          "source": "kahneman-part1-two-systems.txt, Attention and Effort",
          "insight": "Maintaining multiple ideas simultaneously in memory requires effort that competes with decision-making capacity."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "A/B test on the pricing confirmation screen and the proposal review screen. Variant A: single large monthly total at the top (28px IBM Plex Mono), with nightly rate as secondary annotation below, and itemized breakdown on demand. Variant B: all pricing formats displayed simultaneously (nightly rate, weekly total, monthly equivalent) at similar visual weight. Measure: (1) time to complete the pricing step, (2) time to accept or counter a proposal, (3) pricing comprehension (post-completion micro-survey: 'What will you earn per month from this listing?'), (4) pricing-related support tickets.",
      "success_criteria": "Variant A shows at least 20% faster pricing step completion. Post-completion pricing comprehension accuracy above 85% in Variant A (host correctly states their monthly income). Proposal acceptance time under 30 seconds in Variant A for hosts who accept without modification.",
      "failure_meaning": "If Variant A does not improve speed, hosts may already be ignoring secondary numbers. If comprehension is not higher, the single-sum display may not be showing the right number (e.g., showing gross instead of net, or monthly instead of the metric the host actually cares about). If proposal acceptance time is not faster, the financial display is not the bottleneck in proposal review -- look at other elements (trust signals, tenant information).",
      "implementation_hint": "Use feature flags to control which pricing display variant is shown. Track pricing_step_duration, proposal_review_duration, and proposal_action (accept/counter/dismiss) with the variant as a dimension. The micro-survey is a single question shown immediately after the host confirms pricing or accepts a proposal."
    },
    {
      "id": "tests-009",
      "type": "validation_strategy",
      "title": "Validate Trust Credential Stack: Mechanism Before Promise",
      "validates_element": "communicates-003",
      "journey_phases": ["discovery", "evaluation", "onboarding", "proposal_mgmt"],
      "problem": "communicates-003 asserts that trust signals must follow a fixed sequence: verifiable mechanism first, specific instance second, guarantee conclusion third. Bare claims ('Guaranteed!') without mechanisms trigger System 1 skepticism.",
      "solution": "Test whether hosts perceive mechanism-first trust displays as more credible than claim-first displays through qualitative usability testing.",
      "evidence": [
        {
          "source": "andreas-call.txt, 3:28-3:51",
          "insight": "Bryant's trust-building sequence: credit check, vetting, pre-authorization, then the guarantee conclusion. The mechanism precedes the promise."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Associative Machine",
          "insight": "A causal chain (credit check causes pre-authorization causes guarantee) is a story System 1 can process. A bare claim is an assertion requiring faith."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Show 6-8 hosts two versions of the proposal trust section: Version A (mechanism-first): 'Credit verified. Pre-authorized. Payment secured.' with specific tenant details below. Version B (claim-first): 'Payment Guaranteed!' with mechanism details below. Use think-aloud protocol to capture immediate reactions. Follow with structured questions: 'How confident are you that you will receive payment?' (1-5 scale). 'What makes you confident or not confident?' (open-ended). 'Which version did you find more believable?' (forced choice).",
      "success_criteria": "At least 5 of 8 hosts rate confidence at 4+ on the 5-point scale for Version A. At least 6 of 8 hosts choose Version A as more believable in the forced-choice comparison. Qualitative analysis reveals that hosts reference the specific mechanism (credit check, pre-authorization) in their open-ended responses, not just the claim.",
      "failure_meaning": "If hosts prefer the claim-first version, they may value emotional reassurance over process evidence, suggesting the target audience is not exclusively analytical investor hosts. If confidence ratings are low for both versions, the trust problem is deeper than presentation order -- the mechanism itself may not be convincing (e.g., hosts do not understand what 'pre-authorization' means).",
      "implementation_hint": "Create two static mockups or interactive prototypes for the A/B comparison. Test with hosts who have had a phone call (to match the Andreas profile) and separately with hosts who have not (to test the mechanism-first pattern without call priming). Compare results between groups."
    },
    {
      "id": "tests-010",
      "type": "validation_strategy",
      "title": "Validate Seven-Second Unit Description Input Speed",
      "validates_element": "communicates-004",
      "journey_phases": ["listing_creation"],
      "problem": "communicates-004 asserts that the core unit description (type, location, floor, bedrooms, availability) should be capturable in under 30 seconds, matching the conversational density of the phone call where Andreas conveyed 5 data points in 7 seconds.",
      "solution": "Time how long hosts take to complete the unit snapshot (Step 1) using quick-select chips and type-ahead inputs versus traditional form fields.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:03-1:18",
          "insight": "Andreas conveys 6 data points (unit count, building type, floor levels, distinguishing feature, availability, scheduling preference) in 15 seconds."
        },
        {
          "source": "kahneman-part1-two-systems.txt, Attention and Effort",
          "insight": "First-time platform use has maximum cognitive cost. The platform must compensate by reducing the number and complexity of actions required."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Conduct a timed usability test with 6-8 hosts. Provide them with their property details (as if from a prior call) and ask them to complete the unit snapshot step. Test two input variants: (A) Quick-select chips for building type, toggle switches for binary features, type-ahead for address, and a single-view layout showing all 5 essential fields simultaneously. (B) Traditional dropdown menus, multi-select lists, and sequential fields spread across scrollable sections. Measure time-to-completion and perceived effort (post-task 1-5 scale: 'How easy was it to describe your unit?').",
      "success_criteria": "Variant A unit snapshot completion in under 30 seconds for at least 6 of 8 hosts. Perceived effort rating of 4+ (easy) on the 5-point scale for Variant A. Variant A is at least 40% faster than Variant B.",
      "failure_meaning": "If hosts exceed 30 seconds even with chips and toggles, the 5 essential fields may still be too many for a single view, or the chip labels may not match the vocabulary hosts use. If perceived effort is low despite fast completion, the quick-select options may feel constraining (not matching the host's actual property type). Review whether the chip options cover the most common property types.",
      "implementation_hint": "Build a clickable prototype with both variants. Use a stopwatch or screen recording timestamp from the moment the step loads to the moment the host clicks 'Continue.' After each variant, ask the perceived effort question. Randomize variant order across hosts to control for learning effects."
    },
    {
      "id": "tests-011",
      "type": "validation_strategy",
      "title": "Validate Pre-Packaged Proposal Presentation Reduces Decision Time",
      "validates_element": "communicates-005",
      "journey_phases": ["proposal_mgmt"],
      "problem": "communicates-005 asserts that proposals should be presented as pre-packaged confirmations (tenant narrative first, matched terms second, single confirm action third) rather than constructions requiring the host to build a response from scratch.",
      "solution": "Measure proposal response time and acceptance rate when proposals are presented as confirmation-ready packages versus when they require navigation through multiple review screens.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:58-2:19",
          "insight": "Bryant presents Ariel and Amber as a complete package: matched, located, use-case defined. Andreas evaluates the whole, not the parts."
        },
        {
          "source": "kahneman-part1-two-systems.txt, Plot Synopsis",
          "insight": "System 2 adopts System 1's suggestions with little modification when things go smoothly. A well-structured proposal IS a System 1 suggestion."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Track proposal interaction metrics: (1) time from proposal screen load to host action (accept, counter, or dismiss), (2) action distribution (percentage accept vs. counter vs. dismiss vs. no action), (3) steps required to complete the action (how many taps/clicks from proposal view to confirmed action). Compare these metrics before and after implementing the pre-packaged proposal design (tenant narrative leading, single-tap accept button, inline counter-offer). Also track: percentage of proposals that receive any response within 48 hours.",
      "success_criteria": "Proposal response time (load-to-action) under 30 seconds for hosts who accept. At least 70% of proposals receive a response within 48 hours. One-tap acceptance path requires no more than 2 interactions (view proposal, tap accept). Counter-offer path requires no more than 5 interactions.",
      "failure_meaning": "If response time exceeds 30 seconds for acceptors, the proposal may not be providing enough information for a quick decision -- the tenant narrative or financial summary may be insufficient. If the 48-hour response rate is below 70%, the notification design (behaves-006) may be failing to reach hosts. If counter-offer path exceeds 5 interactions, the inline modification controls are not working as designed.",
      "implementation_hint": "Track proposal_screen_loaded, proposal_action_taken (with action type), and proposal_action_completed. Calculate time delta. Segment by action type. Use funnel analysis to identify where hosts drop off in the counter-offer path."
    },
    {
      "id": "tests-012",
      "type": "validation_strategy",
      "title": "Validate Agent Continuity Frame Across Platform Communications",
      "validates_element": "communicates-006",
      "journey_phases": ["discovery", "onboarding", "listing_creation", "retention"],
      "problem": "communicates-006 asserts that every platform communication should be attributed to the agent (Bryant), not the brand (Split Lease). The platform is the tool; the agent is the face. Without this, the trust built with the person is not transferred to the digital experience.",
      "solution": "Test whether agent-attributed communications (emails, notifications, dashboard messages) produce higher engagement than brand-attributed communications.",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:27-5:44 and 6:03-6:11",
          "insight": "Every commitment at call close is person-to-person: Bryant to Andreas. The platform must honor this frame."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Marvels of Priming",
          "insight": "The host's emotions were primed by Bryant's competence and warmth. Agent-attributed messages activate this primed network."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "A/B test email communications across the host journey. Variant A: emails sent from 'Bryant at Split Lease' (agent's name as sender) with agent photo in header, conversational tone, and property-specific references. Variant B: emails sent from 'Split Lease' with brand logo, professional but impersonal tone. Measure: open rate, click-through rate, and response rate (if the email includes a call-to-action like 'review your listing'). Apply the same test to in-platform notifications: agent-attributed ('Bryant: New proposal for your unit') versus system-attributed ('New proposal received').",
      "success_criteria": "Agent-attributed emails show at least 20% higher open rate and 30% higher click-through rate than brand-attributed emails. Agent-attributed notifications show at least 25% higher tap-through rate. Qualitative feedback (post-interaction micro-survey) shows hosts rate the agent-attributed experience as 'more personal' at 4+ on a 5-point scale.",
      "failure_meaning": "If agent-attributed communications do not outperform, the hosts may not have formed a strong enough personal connection with the agent during the call (the call quality matters). Alternatively, the agent attribution may feel fake if the content clearly reads as automated despite the agent's name. The content must genuinely match the agent's conversational register.",
      "implementation_hint": "Use email marketing segmentation to route hosts into A/B groups. Ensure Variant A emails use the agent's actual name and photo from the CRM. Track all engagement metrics with the variant as a dimension. For in-platform notifications, use a feature flag to control attribution display."
    },
    {
      "id": "tests-013",
      "type": "validation_strategy",
      "title": "Validate Advisor Prompt Information Architecture",
      "validates_element": "communicates-007",
      "journey_phases": ["listing_creation", "proposal_mgmt", "active_lease"],
      "problem": "communicates-007 specifies the information architecture of the advisor prompt: validation statement first, then a single question, then an open response field. The prompt must feel like a conversation, not a form. The question must be genuinely useful and the response must be visibly incorporated.",
      "solution": "Test the advisor prompt sequence through qualitative usability testing, measuring whether the validation-question-response flow feels natural and whether hosts perceive their contribution as valued.",
      "evidence": [
        {
          "source": "andreas-call.txt, 4:06-4:26 and 5:12-5:27",
          "insight": "The advisor shift was primed by validation: Bryant validated Andreas's credit standard, and 46 seconds later Andreas was advising on furnishing."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Lazy Controller",
          "insight": "Advising from experience is flow-adjacent: concentrated but not controlled, drawing on System 1 pattern recognition."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "During moderated usability tests of the listing wizard, observe the host's reaction at the advisor prompt moment (after Step 2). Use think-aloud protocol to capture: (1) Does the host read the validation statement and visibly relax or show positive affect? (2) Does the host understand the question without re-reading it? (3) Does the host respond from experience (using specific, personal knowledge) or from obligation (generic, minimal response)? (4) When the response appears in the listing preview, does the host express satisfaction? After the test, ask: 'How did that question feel compared to the form fields you just completed?' (open-ended) and 'Did you feel your answer would be used or ignored?' (1-5 scale).",
      "success_criteria": "At least 5 of 8 hosts respond with property-specific, experience-based content (not generic descriptions). At least 6 of 8 hosts rate their answer as likely to be used (4+ on the 5-point scale). Qualitative analysis shows hosts describe the prompt as 'like a conversation' or 'different from the form' rather than 'another question I had to answer.'",
      "failure_meaning": "If hosts give generic responses, the question is too broad or does not tap into unique expertise. If hosts feel their answer will be ignored, the preview integration is not visible enough or does not clearly show the contribution in context. If the prompt feels like 'another form field,' the visual differentiation (accent-light background, serif typography for the prompt) is not creating the mode shift.",
      "implementation_hint": "Build the advisor prompt into the listing wizard prototype with the full visual treatment (accent-light background, serif validation text, generous text field). Test with hosts who have real properties to describe, not hypothetical scenarios. Record both screen and audio for qualitative analysis."
    },
    {
      "id": "tests-014",
      "type": "validation_strategy",
      "title": "Validate Warm Surface Continuity Creates Positive First Impression",
      "validates_element": "looks-001",
      "journey_phases": ["onboarding", "listing_creation", "proposal_mgmt"],
      "problem": "looks-001 asserts that the warm off-white background (#f6f4f0) must signal 'safe, personal space' within 500ms of page load. If the visual environment signals 'corporate software' or 'unfamiliar website,' System 1 flags the context as foreign.",
      "solution": "Test first-impression reactions to the warm surface palette versus a cold/stark palette through rapid-exposure usability testing.",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:27-6:11",
          "insight": "The call ends with personal warmth. The platform's visual temperature must match this register."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Characters of the Story",
          "insight": "System 1 processes visual information and derives emotional meaning simultaneously, before conscious evaluation. Color temperature is processed in under 200ms."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Rapid-exposure test with 8-10 participants (mix of hosts and potential hosts). Show each participant two versions of the dashboard for exactly 3 seconds each: Version A: warm off-white background (#f6f4f0), white surface cards, warm-tinted shadows, agent photo. Version B: stark white background (#ffffff), gray cards, blue-gray shadows, brand logo only. After each 3-second exposure, ask: 'In one word, how does this feel?' (open-ended), 'Would you trust this with your property? (1-5 scale)', and 'Does this feel personal or corporate? (slider from 1=corporate to 5=personal).'",
      "success_criteria": "Version A receives average trust rating of 3.5+ and average personal rating of 3.5+. Version A's open-ended descriptors cluster around warm/calm/professional rather than generic/cold/corporate. At least 7 of 10 participants rate Version A as more personal than Version B.",
      "failure_meaning": "If Version A does not outperform on warmth/trust, the color temperature difference may not be perceptible in 3 seconds (the difference between #f6f4f0 and #ffffff is subtle). The warm surface may need to be reinforced with other warmth signals (agent photo, conversational copy) to achieve its effect. If both versions rate similarly, surface color alone may not drive the trust impression -- content and layout may matter more.",
      "implementation_hint": "Create high-fidelity mockups for both versions with identical content and layout, varying only the background color, shadow tint, and border colors. Use an online testing tool (UsabilityHub or similar) for the 3-second exposure test to reach a larger sample. Analyze open-ended responses with affinity mapping."
    },
    {
      "id": "tests-015",
      "type": "validation_strategy",
      "title": "Validate Typographic Trust Triad Guides Cognitive Processing",
      "validates_element": "looks-002",
      "journey_phases": ["onboarding", "listing_creation", "pricing", "proposal_mgmt"],
      "problem": "looks-002 asserts that three typefaces (Instrument Serif for recognition, Outfit for action, IBM Plex Mono for verification) guide the host's eye through three distinct cognitive modes. If the typefaces do not create perceptible lanes, the host reads everything with equal attention, violating the law of least effort.",
      "solution": "Use eye-tracking or think-aloud testing to verify that hosts process serif headlines faster (recognition), spend moderate time on sans body text (comprehension), and slow down on mono data (verification).",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:03-1:10 and 3:02-3:16",
          "insight": "Andreas processes identity information at conversational speed but financial information with focused, sequential attention. Typography must signal which mode is needed."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Marvels of Priming",
          "insight": "Typography is a visual prime. Serif primes recognition, sans primes action, mono primes precision."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Moderated usability test with 6 hosts on a proposal review screen that uses all three typefaces. Use think-aloud protocol to capture what hosts notice first (expected: serif headline = property/tenant name), what they read at moderate speed (expected: sans-serif description of terms), and where they slow down to verify (expected: mono financial figures). Ask after the test: 'Did anything on this screen feel like it was asking you to look more carefully?' If the host identifies the mono financial section, the triad is working. Compare dwell time on serif, sans, and mono sections if eye-tracking is available.",
      "success_criteria": "At least 4 of 6 hosts identify the mono financial section as requiring more careful attention in the post-test question. If eye-tracking is used: dwell time on mono sections is at least 50% longer per character than on serif sections. Hosts verbalize recognition of property or agent name (serif content) within 2 seconds of screen exposure.",
      "failure_meaning": "If hosts do not differentiate between typographic zones, the three typefaces may be too similar in weight or size, or the content differences (not the typography) may be driving attention. If mono data does not attract verification behavior, the host may not associate monospace with precision -- this association may need to be learned through consistent use across the platform.",
      "implementation_hint": "Build a high-fidelity prototype of the proposal review screen using the exact typefaces and sizes specified. Recruit hosts who have had a phone call (to match the Andreas profile). If eye-tracking is not available, use think-aloud dwell time as a proxy -- ask hosts to read the screen aloud and time each section."
    },
    {
      "id": "tests-016",
      "type": "validation_strategy",
      "title": "Validate Single-Number Financial Focal Point on Money Screens",
      "validates_element": "looks-003",
      "journey_phases": ["pricing", "proposal_mgmt", "active_lease"],
      "problem": "looks-003 asserts that every financial screen must have exactly one visually dominant number (28px IBM Plex Mono, monthly total) with all other numbers subordinate. If multiple numbers compete visually, System 1 cannot perform the gut-check.",
      "solution": "Test whether hosts can correctly identify 'what they earn' within 3 seconds on screens with a single focal number versus screens with multiple numbers at similar weights.",
      "evidence": [
        {
          "source": "andreas-call.txt, 3:02-3:16",
          "insight": "Andreas processes one number and concludes: 'Three months. Okay. Got it.' One number, one evaluation, one decision."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Lazy System 2",
          "insight": "When multiple numbers are present, System 1 latches onto the most salient one and may generate a wrong interpretation."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Rapid-exposure test with 8 participants. Show two versions of the pricing confirmation screen for 5 seconds each. Version A: one large monthly total (28px, ink, top of card) with secondary nightly rate below in smaller, muted text. Version B: nightly rate, weekly total, and monthly equivalent all displayed at similar size and weight. After each 5-second exposure, immediately ask: 'How much will you earn per month from this listing?' Record accuracy and response time. Then show both versions side by side and ask: 'Which screen is easier to understand?'",
      "success_criteria": "At least 7 of 8 participants correctly state the monthly total after viewing Version A. Fewer than 5 of 8 correctly state the monthly total after viewing Version B (demonstrating that multiple numbers cause confusion). At least 6 of 8 identify Version A as easier to understand in the forced-choice comparison. Response time for Version A is at least 30% faster than Version B.",
      "failure_meaning": "If Version A does not produce better comprehension, the single number may not be the right number (e.g., hosts care more about nightly rate or total upfront). If both versions produce similar accuracy, the format may matter less than the content -- hosts may be able to extract information from either layout. This would suggest the element's visual hierarchy is less critical than assumed.",
      "implementation_hint": "Create two mockups with identical financial data but different visual treatments. Use an online testing tool for the 5-second exposure test. Ensure the financial figures are realistic for Washington Heights properties to avoid confusion from implausible numbers."
    },
    {
      "id": "tests-017",
      "type": "validation_strategy",
      "title": "Validate Agent Identity as Persistent Visual Trust Anchor",
      "validates_element": "looks-004",
      "journey_phases": ["onboarding", "listing_creation", "proposal_mgmt", "retention"],
      "problem": "looks-004 asserts that the agent's photo and name must be a persistent visual element answering System 1's question 'Do I know this?' before any content is processed. Without the agent's face, the platform activates an unprimed associative network.",
      "solution": "Test whether the presence of the agent's photo and name on the dashboard affects the host's first-impression trust rating and willingness to proceed.",
      "evidence": [
        {
          "source": "kahneman-part1-two-systems.txt, The Characters of the Story",
          "insight": "Faces are processed by System 1 faster and more deeply than any other visual element. A photo activates facial recognition, emotional association, and trust memory in under 500ms."
        },
        {
          "source": "andreas-call.txt, 5:27-5:44",
          "insight": "Andreas establishes a personal channel with Bryant. The platform must visually maintain this personal relationship."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "A/B test on the host dashboard for new hosts arriving for the first time after a phone call. Variant A: agent photo (64px, accent border) and name prominently displayed in the top-left with a personalized message ('Bryant set up your listing for [address]'). Variant B: standard dashboard with Split Lease logo, no agent photo, and a generic welcome message. Measure: (1) bounce rate (hosts who leave without taking any action), (2) time-to-first-action, (3) post-visit micro-survey: 'How personal did this experience feel?' (1-5 scale).",
      "success_criteria": "Variant A shows at least 20% lower bounce rate than Variant B. Time-to-first-action is at least 30% faster in Variant A. Personalization rating is 4+ for at least 70% of Variant A hosts.",
      "failure_meaning": "If the agent photo does not reduce bounce rate, the trust transfer from call to platform may require more than visual recognition -- it may require the agent to explicitly preview the platform during the call (as works-004 recommends). If time-to-first-action is not faster, the agent identity may be recognized but the content around it is not action-oriented enough.",
      "implementation_hint": "Use the CRM to populate the dashboard with the assigned agent's photo and name. Ensure the photo is a professional headshot, not an avatar or icon. The personalized message should reference the specific property discussed in the call, pulled from call notes. Track bounce, first_action, and trigger the micro-survey after the first session."
    },
    {
      "id": "tests-018",
      "type": "validation_strategy",
      "title": "Validate Green Trust Badges Create Learned Trust Signal",
      "validates_element": "looks-005",
      "journey_phases": ["evaluation", "onboarding", "proposal_mgmt"],
      "problem": "looks-005 reserves the accent green (#2d5a3d) exclusively for verified/trust elements: badges, agent borders, and commitment actions. The scarcity of green is what gives it signal power. If green is used for decoration, its semantic value is diluted.",
      "solution": "Test whether hosts learn to associate green badges with verified information and whether this association speeds up trust evaluation over repeated exposures.",
      "evidence": [
        {
          "source": "kahneman-part1-two-systems.txt, The Associative Machine",
          "insight": "When green consistently accompanies verified information, an associative link forms: green = verified = trustworthy. This becomes automatic (System 1)."
        },
        {
          "source": "andreas-call.txt, 4:06",
          "insight": "Andreas requires 'excellent credit on every single tenant.' A green badge representing this verified status must be instantly recognizable."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Multi-screen usability test with 6 hosts. Walk them through 4 screens in sequence: onboarding (agent with green border), listing creation (no green badges), proposal review (tenant names with green 'Credit Verified' badges), and a second proposal (different tenants, same green badges). At the fourth screen, ask: 'Without reading the text, what does the green label tell you?' If the host says 'verified' or 'checked' without reading the badge text, the association has formed. Also measure: on the proposal screens, do hosts fixate on the green badges early in their scan, or do they read the badge text only after scanning other content?",
      "success_criteria": "At least 4 of 6 hosts correctly identify the meaning of green badges by the fourth screen without reading the text. Green badges are among the first 3 elements hosts fixate on during proposal screen scanning (if eye-tracking is used). Qualitative feedback confirms that hosts perceive green as 'something that has been checked' rather than decoration.",
      "failure_meaning": "If hosts do not learn the green association after 4 screens, the green may not be salient enough (too small, too similar to other colors on the page) or the badge may need a supporting icon (checkmark) to reinforce the meaning. If green is perceived as decoration, the color may be appearing in too many non-trust contexts (check for green leakage into navigation or general UI).",
      "implementation_hint": "Build a sequential prototype with 4 screens. Ensure green appears ONLY on trust-related elements and the agent border. No green in navigation, buttons, or decorative elements during the test. Use think-aloud protocol throughout and a specific probe question at Screen 4."
    },
    {
      "id": "tests-019",
      "type": "validation_strategy",
      "title": "Validate Progressive Disclosure Through Visual Weight Tiers",
      "validates_element": "looks-006",
      "journey_phases": ["listing_creation", "pricing", "proposal_mgmt"],
      "problem": "looks-006 defines three tiers of visual weight (ink, ink-soft, ink-muted) corresponding to three disclosure levels. The concern is whether the visual weight difference is perceptible enough to guide attention without making secondary content illegible.",
      "solution": "Test whether hosts naturally process primary content first, notice secondary content without effort, and find tertiary triggers when needed.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:58-2:54",
          "insight": "Bryant presents information in layers: primary (tenants), secondary (matching process), tertiary (agreement details). Andreas engages actively with primary, passively with secondary."
        },
        {
          "source": "kahneman-part1-two-systems.txt, Attention and Effort",
          "insight": "You dispose of a limited budget of attention. Visual weight is the mechanism by which the platform allocates the host's limited attention budget."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Screen-scanning test with 6 hosts on a proposal detail screen with all three weight tiers present. Ask hosts to 'tell me everything important about this proposal' (to see what they naturally prioritize). Then ask 'is there anything else on this screen you did not mention?' (to see if they noticed secondary content). Finally, ask them to find a specific piece of tertiary information ('Can you find the detailed payment breakdown?'). Measure: (1) what hosts mention first (should be primary-weight content), (2) what they mention second (should be secondary-weight), (3) whether they can find tertiary triggers without help. Also check: is ink-muted text at 13px readable for all participants?",
      "success_criteria": "At least 5 of 6 hosts mention primary-weight content first. At least 4 of 6 mention secondary-weight content without prompting. At least 5 of 6 find the tertiary trigger within 10 seconds when asked. No host reports difficulty reading any text on the screen (accessibility check).",
      "failure_meaning": "If hosts mention secondary before primary, the visual weight ratio is not strong enough -- increase the size or weight differential. If hosts cannot find tertiary triggers, the ink-muted color may be too light or the trigger text may lack sufficient affordance (add underline or icon as the element itself recommends). If any host reports readability issues with ink-muted at 13px, the contrast falls below acceptable levels and needs supplementation.",
      "implementation_hint": "Build a high-fidelity prototype of a proposal screen with realistic data. Include a 'View details' tertiary trigger. Test on actual devices (not just monitors) since mobile screens may render the contrast differently. Include at least one participant over 45 to test readability at the lower contrast levels."
    },
    {
      "id": "tests-020",
      "type": "validation_strategy",
      "title": "Validate Motion Timing: Sub-100ms Acknowledgment and 300ms Settle",
      "validates_element": "looks-007",
      "journey_phases": ["listing_creation", "pricing", "proposal_mgmt"],
      "problem": "looks-007 defines a motion framework: sub-100ms acknowledgment (instant state change), 350ms settle (layout transitions), and 500ms celebrate (milestone completions). If acknowledgment is too slow, the host doubts input was received. If settle is too fast or too slow, transitions feel jarring or sluggish.",
      "solution": "Test perceived responsiveness by comparing the defined motion timing against variants with slower acknowledgment and faster/slower settle animations.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:10, 3:16, 4:26",
          "insight": "Bryant's acknowledgments (Yep, Got it, Yup) are instant and minimal, confirming receipt before the host's working memory releases the input."
        },
        {
          "source": "kahneman-part1-two-systems.txt, Attention and Effort",
          "insight": "Every animation that draws the eye consumes from the limited attention budget. The motion must confirm, not distract."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Interactive prototype test with 6 hosts performing common actions (toggling a feature, entering a price, accepting a proposal). Test three timing variants in randomized order: Variant A: 0ms acknowledgment, 350ms settle, 500ms celebrate (the spec). Variant B: 200ms acknowledgment, 350ms settle, 500ms celebrate (slower acknowledgment). Variant C: 0ms acknowledgment, 150ms settle, 250ms celebrate (faster everything). After each variant, ask: 'Did the interface feel responsive?' (1-5 scale) and 'Did anything feel too slow or too fast?' (open-ended). At the end, show all three and ask: 'Which felt the best?'",
      "success_criteria": "Variant A is selected as 'best feel' by at least 4 of 6 hosts. Variant A receives an average responsiveness rating of 4+ on the 5-point scale. Variant B is described as 'sluggish' or 'laggy' by at least 3 of 6 hosts (confirming that sub-100ms acknowledgment is perceptibly better). No host describes Variant A as 'too fast' or 'jarring.'",
      "failure_meaning": "If Variant C (faster) is preferred, the settle and celebrate animations may be too long and should be shortened. If Variant B (slower acknowledgment) is not perceptibly worse, the sub-100ms target may be unnecessarily strict for the actual devices hosts use. If no variant is clearly preferred, motion timing may not meaningfully affect the host experience and should be deprioritized relative to content and layout work.",
      "implementation_hint": "Build interactive prototypes (not static mockups) for all three variants. Use CSS custom properties (--duration-acknowledge, --duration-settle, --duration-celebrate) to switch between variants without code changes. Test on the devices hosts actually use (likely phones and laptops, not high-refresh-rate monitors)."
    },
    {
      "id": "tests-021",
      "type": "validation_strategy",
      "title": "Validate Instant Acknowledgment on Every Host Input",
      "validates_element": "behaves-001",
      "journey_phases": ["listing_creation", "pricing", "proposal_mgmt", "onboarding"],
      "problem": "behaves-001 asserts that every interactive element must provide a state change within 100ms, before any server response. Without this, the host's working memory holds the input while waiting for confirmation, depleting cognitive resources.",
      "solution": "Measure actual acknowledgment latency across all interactive elements and correlate with task completion rates.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:10, 3:02, 3:16, 4:26",
          "insight": "Bryant's acknowledgment cadence: approximately once per 90 seconds of host input, each under 500ms, containing no new information."
        },
        {
          "source": "kahneman-part1-two-systems.txt, Attention and Effort",
          "insight": "Holding an unacknowledged input in working memory consumes from the limited attention budget."
        }
      ],
      "priority": "high",
      "validation_method": "automated",
      "test_description": "Implement automated performance testing across all interactive elements (form fields, toggles, buttons, chips). For each element, measure the time from user interaction (click, tap, keypress) to the first visible state change (border color shift, position change, color change). Run this automated test in a CI/CD pipeline on every deployment. Additionally, use Real User Monitoring (RUM) to capture acknowledgment latency in production. Alert when any element's P95 acknowledgment latency exceeds 100ms.",
      "success_criteria": "100% of interactive elements achieve sub-100ms acknowledgment latency at P95 in automated testing. Production RUM data shows P95 acknowledgment latency under 100ms across all device categories. No host-reported issues of 'unresponsive' or 'laggy' interactions in support tickets.",
      "failure_meaning": "If acknowledgment latency exceeds 100ms, the likely cause is either: (1) the state change depends on JavaScript execution that is blocked by other work, (2) the CSS transition has a non-zero delay, or (3) the element's interaction is tied to a server call that blocks rendering. Each cause has a different fix: debundle JS, remove transition-delay, or decouple local acknowledgment from server confirmation.",
      "implementation_hint": "Use the Performance Observer API in the browser to measure time from input events to the next paint. For CI/CD testing, use Playwright or similar to interact with each element and measure first visual change via screenshot comparison. Set up RUM dashboards segmented by element type and device category."
    },
    {
      "id": "tests-022",
      "type": "validation_strategy",
      "title": "Validate Sequential Disclosure One-Cluster-at-a-Time Flow",
      "validates_element": "behaves-002",
      "journey_phases": ["listing_creation", "onboarding", "proposal_mgmt"],
      "problem": "behaves-002 asserts that input fields should be presented one cluster at a time, following the call's topic order. Each cluster reveals only after the prior is completed. The risk is that this sequential pattern may feel slow for experienced hosts or overly restrictive for hosts who want to skip ahead.",
      "solution": "Compare task completion and satisfaction between sequential disclosure and traditional all-fields-visible forms.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:29 through 5:27",
          "insight": "The call's topic sequence processes one topic fully before moving to the next. Bryant never asks about two things at once."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Busy and Depleted System 2",
          "insight": "A form with multiple visible fields creates meta-decision conflict: which field first? Sequential disclosure eliminates this meta-conflict."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "A/B test on the listing wizard (Steps 2-6; Step 1 uses density per the coherence report's resolution). Variant A: sequential disclosure with one cluster revealed at a time, completed clusters collapsed into summary strips above. Variant B: traditional form with all fields visible simultaneously in a scrollable page. Measure: (1) completion rate, (2) time to complete, (3) error rate (fields left blank or with invalid data), (4) post-completion satisfaction: 'How clear was the process?' (1-5 scale).",
      "success_criteria": "Variant A shows at least 10% higher completion rate. Error rate is at least 30% lower in Variant A. Satisfaction rating is 4+ for Variant A. Time to complete is within 20% of Variant B (sequential should not be significantly slower due to reduced confusion and fewer errors to correct).",
      "failure_meaning": "If Variant B shows higher completion, sequential disclosure may be creating frustration by hiding information that hosts want to see upfront. This would indicate that the host population includes more experienced users who prefer overview-first. If error rates are similar, the meta-decision conflict may not be a real problem for this audience. If sequential is significantly slower, the transition animations between clusters may need to be faster.",
      "implementation_hint": "Implement both variants with feature flags. Ensure both variants contain identical fields and validation rules. Track per-field error events and timing. Run the test for at least 2 weeks to achieve statistical significance given expected host volume."
    },
    {
      "id": "tests-023",
      "type": "validation_strategy",
      "title": "Validate Confirm-Not-Construct Proposal Interaction",
      "validates_element": "behaves-003",
      "journey_phases": ["proposal_mgmt"],
      "problem": "behaves-003 asserts that proposals should be accepted with a single tap and counter-offered through inline modifications, never a blank form. The default path (acceptance) must require minimal effort while the secondary path (counter-offer) preserves the host's role as modifier, not constructor.",
      "solution": "Track the full proposal interaction funnel to verify one-tap acceptance works as designed and counter-offer does not require excessive interactions.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:58-2:19",
          "insight": "Bryant presents tenants as a complete package that the host evaluates as a whole, not as parts."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Busy and Depleted System 2",
          "insight": "Depleted users default to the easier action. If acceptance is one tap, depleted hosts accept. If acceptance and rejection are equally weighted, they close the browser."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Instrument the proposal review screen to track the complete interaction funnel: proposal_loaded -> first_scroll (if any) -> detail_expanded (if any) -> accept_tapped OR adjust_tapped OR page_left. For accept path: track time from page load to accept tap and number of intermediate interactions. For counter-offer path: track adjust_tapped -> fields_modified (which ones) -> counter_submitted, counting total interactions. Compare acceptance rate and counter-offer rate before and after implementing the confirm-not-construct design.",
      "success_criteria": "Accept path completes in 2 or fewer interactions (view proposal, tap accept). At least 50% of accepting hosts complete the accept path in under 30 seconds. Counter-offer path completes in 5 or fewer interactions. Proposal response rate (any action taken) is above 70% within 48 hours.",
      "failure_meaning": "If the accept path requires more than 2 interactions, there is unnecessary gating (scroll required, confirmation dialog, login prompt) between the proposal view and acceptance. If under 30 seconds is not achieved, the proposal summary may not contain enough information for a quick decision. If the counter-offer path exceeds 5 interactions, the inline modification controls are too complex or require too many steps.",
      "implementation_hint": "Use analytics events at each step of the funnel. Calculate interaction_count per session per proposal. Segment by outcome (accept, counter, dismiss, abandon). Create a funnel visualization dashboard. Alert on any week where the 48-hour response rate drops below 60%."
    },
    {
      "id": "tests-024",
      "type": "validation_strategy",
      "title": "Validate Save-and-Resume Preserves Progress Across Sessions",
      "validates_element": "behaves-004",
      "journey_phases": ["listing_creation", "onboarding", "pricing"],
      "problem": "behaves-004 asserts that every input must be auto-saved in real time and the platform must reopen at the exact point of departure when the host returns. Without this, every interruption becomes a dropout event because depleted hosts will not restart from scratch.",
      "solution": "Test save-and-resume reliability through automated testing and track the return rate of hosts who interrupt mid-process.",
      "evidence": [
        {
          "source": "andreas-call.txt, 0:15",
          "insight": "Andreas is building a bar in his house -- he arrives already cognitively busy and will be interrupted during any multi-step process."
        },
        {
          "source": "kahneman-part1-two-systems.txt, Attention and Effort",
          "insight": "Auto-save IS 'committing to paper.' The platform serves as the host's external memory."
        }
      ],
      "priority": "high",
      "validation_method": "automated",
      "test_description": "Automated regression test: Simulate a host completing Steps 1-3 of the listing wizard, then closing the browser. Reopen to the listing wizard and verify: (1) the platform resumes at Step 4 (the next unfinished step), (2) all Step 1-3 data is preserved exactly as entered, (3) summary strips for completed steps show the correct data, (4) the welcome-back message appears with the correct step reference. Run this test after every deployment. Additionally, track in production: (1) session_interrupted events (host leaves mid-wizard), (2) session_resumed events (host returns to a previously interrupted wizard), (3) the return rate (resumed / interrupted). Track time-to-return (how long between interruption and resumption).",
      "success_criteria": "Automated test passes 100% of the time -- no data loss on session interruption. Production return rate above 50% (at least half of hosts who interrupt mid-wizard return to complete). Median time-to-return under 48 hours. Data integrity on resume: 100% of saved fields contain the exact values entered before interruption.",
      "failure_meaning": "If the automated test fails, auto-save has a reliability bug that must be fixed before launch. If the return rate is below 50%, the save-and-resume is working technically but the re-engagement mechanism (email reminder, welcome-back message) is not compelling enough. If time-to-return exceeds 48 hours, the follow-up notification may need to be sent sooner or through a different channel.",
      "implementation_hint": "Automated tests should use Playwright to fill fields, close the page, reopen, and assert field values. In production, track save events with a hash of the saved data to verify integrity on restore. The re-engagement email should be sent 4 hours after interruption, attributed to the agent, with a direct link to resume."
    },
    {
      "id": "tests-025",
      "type": "validation_strategy",
      "title": "Validate Advisor Trigger Shifts Host from Data Entry to Expertise Sharing",
      "validates_element": "behaves-005",
      "journey_phases": ["listing_creation", "proposal_mgmt", "active_lease"],
      "problem": "behaves-005 specifies the interaction pattern for the advisor trigger: validation, question, open response, with a visual mode shift (accent-light background, serif typography). The trigger must feel like a conversation, not a form field, and the host's response must be visibly incorporated.",
      "solution": "Observe whether the visual mode shift and the validation-question sequence produce a perceptible change in host behavior (from clicking/selecting to reflecting/contributing).",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:12-5:27",
          "insight": "Andreas shifts from asking questions to volunteering 4 practical recommendations without being asked. The advisor trigger aims to replicate this shift digitally."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Associative Machine",
          "insight": "When the host advises, System 1 constructs a story where the host is part of the outcome, making disengagement psychologically inconsistent."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Moderated usability test with 6 hosts during listing creation. Observe the moment when the advisor trigger appears (after Step 2 completion). Capture through think-aloud: (1) Does the host notice the visual shift (background color change, typography change)? (2) Does the host pause to read the validation statement? (3) Does the host's language shift from task-oriented ('what do I click?') to reflective ('well, in my experience...')? (4) How long does the host spend on the advisor prompt compared to the average form field? After the test, ask: 'How did that last question feel different from the form fields?' and 'Do you think your answer will show up on your listing?'",
      "success_criteria": "At least 4 of 6 hosts notice the visual shift (mention background change or different feel). At least 4 of 6 show a language shift from task-oriented to reflective. Average time spent on the advisor prompt is at least 3x the average time spent on a standard form field (indicating genuine reflection, not quick input). At least 5 of 6 believe their answer will be used.",
      "failure_meaning": "If the visual shift is not noticed, the accent-light background and serif typography may not be differentiated enough from the standard form appearance. If no language shift occurs, the question may not be tapping into experiential knowledge -- it may read as another data collection question. If time spent is not longer, hosts are treating the advisor prompt as a form field, which means the mode shift failed.",
      "implementation_hint": "Record screen, face (if consent is given), and audio. The face recording can capture micro-expressions at the moment the advisor trigger appears (relaxation, engagement, or confusion). The timing data should be captured from the analytics events: advisor_prompt_shown to advisor_prompt_submitted or advisor_prompt_skipped."
    },
    {
      "id": "tests-026",
      "type": "validation_strategy",
      "title": "Validate Notification as Continuation, Not Interruption",
      "validates_element": "behaves-006",
      "journey_phases": ["proposal_mgmt", "active_lease", "retention"],
      "problem": "behaves-006 asserts that notifications must be self-contained action units (who, what, key number, direct action link) attributed to the agent and delivered through the host's preferred channel (email for Andreas). If notifications require a full context switch to act on, hosts will ignore them.",
      "solution": "Measure notification engagement rates and compare self-contained notifications (with enough info for triage) against pointer notifications ('You have a new proposal, log in to view').",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:44",
          "insight": "Andreas explicitly establishes email as his channel: 'You can just email me at any time.'"
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Characters of the Story",
          "insight": "Inattentional blindness: a host focused on other tasks will not notice platform notifications unless they arrive through the right channel with enough context."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "A/B test proposal notification emails. Variant A (self-contained): Subject from agent name, body includes tenant names, verified status, monthly total, and a one-tap link directly to the proposal screen. Variant B (pointer): Subject from Split Lease, body says 'You have a new proposal. Log in to review.' with a link to the dashboard. Measure: (1) email open rate, (2) link click rate, (3) time from email delivery to proposal action (accept/counter/dismiss), (4) percentage of proposals that receive a response within 48 hours.",
      "success_criteria": "Variant A shows at least 30% higher click rate than Variant B. Time from email to proposal action is under 2 hours for 50% of Variant A recipients. 48-hour response rate is above 70% for Variant A. Variant A open rate is at least 15% higher.",
      "failure_meaning": "If Variant A does not outperform, the notification content may not match what hosts need for triage (e.g., the monthly total may not be enough -- they may need dates or duration). If open rates are similar, the sender name (agent vs. brand) may not be the differentiator -- subject line content may matter more. If 48-hour response rate is still low, the channel may be wrong (some hosts may prefer SMS or push despite stating email preference).",
      "implementation_hint": "Use email marketing A/B testing functionality. Ensure Variant A deep-links directly to the authenticated proposal screen (use magic link or session token to avoid a login barrier). Track the full funnel from email_delivered to proposal_action_taken."
    },
    {
      "id": "tests-027",
      "type": "validation_strategy",
      "title": "Validate Multi-Unit Batch Operations for Portfolio Hosts",
      "validates_element": "behaves-007",
      "journey_phases": ["listing_creation", "pricing", "active_lease", "retention"],
      "problem": "behaves-007 asserts that multi-unit hosts like Andreas (two units in the same building) should be able to duplicate-and-modify rather than create each listing from scratch. Without this, the second listing requires double the effort for 20% new information.",
      "solution": "Measure the time and effort reduction for second-listing creation when the duplicate-and-modify pattern is available versus requiring a fresh listing creation.",
      "evidence": [
        {
          "source": "andreas-call.txt, 1:03-1:18",
          "insight": "Andreas describes both units as variations of each other: same building type, different floors. He naturally presents them together."
        },
        {
          "source": "kahneman-part1-two-systems.txt, Attention and Effort",
          "insight": "Requiring duplication of effort that produces no new information is a pure cost. The law of least effort predicts the host will postpone the second listing indefinitely."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "For multi-unit hosts (identified by having more than one listing), track: (1) time to create Listing 2 after publishing Listing 1, (2) number of fields modified in Listing 2 (versus fields carried forward from Listing 1), (3) second-listing creation rate (percentage of multi-unit hosts who create Listing 2 within 7 days of Listing 1), (4) time to complete Listing 2 versus time to complete Listing 1. Compare these metrics before and after implementing the duplicate-and-modify pattern.",
      "success_criteria": "Listing 2 creation time is under 2 minutes (compared to under 5 minutes for Listing 1). At least 70% of multi-unit hosts create Listing 2 within 7 days of Listing 1. Number of fields modified in Listing 2 is under 30% of total fields (confirming 70%+ carry-forward). Second listing creation rate increases by at least 25% after implementing duplicate-and-modify.",
      "failure_meaning": "If Listing 2 time is not significantly less than Listing 1, the carry-forward may not be working correctly (too many fields requiring modification, or the modification flow is as complex as the creation flow). If second-listing creation rate does not improve, the prompt to create a second listing may not be reaching hosts at the right moment, or multi-unit hosts may have reasons beyond effort to delay the second listing.",
      "implementation_hint": "Tag listings with building_id to identify units in the same building. Track listing_created events with is_duplicate boolean and source_listing_id. Calculate field_modification_rate per listing. The prompt to add a second unit should appear immediately after the first listing is published, while the host is still engaged."
    },
    {
      "id": "tests-028",
      "type": "validation_strategy",
      "title": "Validate Cognitive Ease as the Foundation of Trust at First Touch",
      "validates_element": "feels-001",
      "journey_phases": ["discovery", "evaluation", "onboarding"],
      "problem": "feels-001 asserts that the first 2 seconds of every phase transition must generate calm through cognitive ease. If the first sensory input is unfamiliar, System 1 flags the context as foreign and the host's trust from the call is not transferred.",
      "solution": "Measure emotional first impressions using rapid-exposure testing and qualitative interviews to verify that the platform generates calm rather than confusion or anxiety.",
      "evidence": [
        {
          "source": "andreas-call.txt, 0:09-0:50",
          "insight": "Andreas's calm, open tone indicates his System 1 had already approved Bryant. The platform must replicate this cognitive ease."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Lazy Controller and Illusions",
          "insight": "Cognitive ease produces the feeling of familiarity, which System 1 interprets as safety and truth."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Post-call usability study with 6-8 hosts who have just completed a phone call with an agent. Within 1 hour of the call, send them the onboarding link and observe their first platform visit remotely (screen recording + audio). Capture: (1) first words spoken when the page loads (think-aloud), (2) time to first positive or negative verbalization, (3) body language if video is available (leaning in = engagement, leaning back = caution). After a 5-minute platform exploration, ask: 'How did the platform compare to your phone call with [agent name]?' (open-ended) and 'On a scale of 1-5, how calm or anxious did you feel when the page first loaded?'",
      "success_criteria": "At least 5 of 8 hosts verbalize recognition or positive affect within 3 seconds of page load ('Oh, there is Bryant' or 'That is my address'). Calm rating averages 4+ on the 5-point scale. Qualitative comparison to the phone call clusters around 'felt like the same thing' rather than 'felt like a different experience.'",
      "failure_meaning": "If hosts do not verbalize recognition quickly, the first-screen content is not anchored to their call experience. If calm ratings are low, the visual environment may be generating cognitive strain (too many elements, unfamiliar layout, corporate feel). If hosts describe the platform as a 'different experience' from the call, associative coherence is broken and the full trust bridge needs strengthening.",
      "implementation_hint": "Coordinate with the sales team to schedule platform follow-ups within 1 hour of calls. Use remote usability testing tools (Lookback, UserTesting) that capture screen and audio. The timing is critical -- testing days after the call does not capture the warm priming state."
    },
    {
      "id": "tests-029",
      "type": "validation_strategy",
      "title": "Validate Relief of Guaranteed Income Through Tension-Relief Arc",
      "validates_element": "feels-002",
      "journey_phases": ["evaluation", "pricing", "proposal_mgmt"],
      "problem": "feels-002 asserts that financial screens must follow a deliberate emotional sequence: show the total (tension: 'is this enough?'), then show the guarantee mechanism (relief: 'it is safe'). The 300ms delay between the number and the trust badges is critical for the tension-relief arc.",
      "solution": "Test whether the tension-relief sequence (number first, then badges) produces higher reported confidence than simultaneous display or badge-first display.",
      "evidence": [
        {
          "source": "andreas-call.txt, 3:02-3:51",
          "insight": "Andreas's emotional arc: analytical questioning about deposits and fees, then relief when Bryant explains credit checks and pre-authorization guarantee."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Associative Machine",
          "insight": "The payment guarantee works because it is a causal story. The narrative, not the claim, produces the emotion of relief."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Show 8 hosts three versions of a proposal financial display: Version A (tension-relief): Monthly total appears first, then after a 300ms delay trust badges fade in below. Version B (simultaneous): Total and badges appear together. Version C (badge-first): Trust badges appear first, then the number. After each version, rate: 'How confident are you that you will receive this payment?' (1-5) and 'How did this screen make you feel?' (open-ended). Final forced-choice: 'Which version felt the most reassuring?'",
      "success_criteria": "Version A (tension-relief) is selected as most reassuring by at least 5 of 8 hosts. Version A confidence rating averages at least 0.5 points higher than Version B or C. Open-ended responses for Version A include words like 'safe,' 'solid,' 'clear.' Version C (badge-first) shows lower confidence than Version A, confirming that claim-before-evidence triggers skepticism.",
      "failure_meaning": "If simultaneous display (B) is preferred, the 300ms delay may feel unnecessary or patronizing. If badge-first (C) performs equally well, hosts may not be skeptical of claims the way the Kahneman framework predicts -- they may value reassurance regardless of order. If confidence ratings are low across all versions, the trust mechanism itself (credit verified, pre-authorized) may not be understood or believed.",
      "implementation_hint": "Build three interactive mockups with identical data but different reveal timing. Use CSS animations with configurable delays. Test with hosts who have real financial stakes (property owners) rather than test participants without investment context."
    },
    {
      "id": "tests-030",
      "type": "validation_strategy",
      "title": "Validate Advisor Glow Creates Confidence and Commitment",
      "validates_element": "feels-003",
      "journey_phases": ["evaluation", "listing_creation", "proposal_mgmt"],
      "problem": "feels-003 asserts that the advisor moment should produce the specific warmth of being consulted -- the 'glow' of expertise recognized. This emotional state creates commitment because the host has invested their knowledge.",
      "solution": "Measure the emotional and behavioral impact of the advisor moment through qualitative sentiment analysis and downstream engagement metrics.",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:12-5:27",
          "insight": "Andreas's emotional peak: contributing practical recommendations. His tone is collaborative, not transactional. This is where evaluation becomes partnership."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Lazy Controller",
          "insight": "Advising is flow-adjacent: concentrated but not controlled, drawing on System 1 pattern recognition. This is why advising is energizing while form-filling is depleting."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "During moderated usability tests, use a self-reported emotional probe at three points: (1) after completing Step 2 of the listing wizard (before the advisor prompt), (2) immediately after the advisor prompt response (if given), (3) after completing the full wizard. At each point, ask: 'Pick the word that best describes how you feel right now: tired, neutral, engaged, confident, accomplished.' Track whether the advisor prompt shifts the self-reported emotion in a positive direction. Also ask after the prompt: 'How did answering that question make you feel about your listing?' (open-ended).",
      "success_criteria": "At least 4 of 6 hosts report a more positive emotion after the advisor prompt than before it (e.g., shift from 'neutral' or 'tired' to 'engaged' or 'confident'). Open-ended responses include language about ownership, expertise, or contribution ('my place,' 'I know this building,' 'glad they asked'). Hosts who engage with the advisor prompt are more likely to complete the remaining wizard steps than those who skip.",
      "failure_meaning": "If no emotional shift is detected, the advisor prompt may not be distinctive enough from regular form fields -- the visual mode shift may not be working. If hosts report negative emotions after the prompt ('annoyed,' 'confused'), the question may be poorly phrased or the timing may be wrong (too early in the wizard, before the host feels established). If the prompt does not affect completion rates, the commitment mechanism may be weaker than the theory predicts for this audience.",
      "implementation_hint": "Use in-session micro-surveys (a single emoji or word selector that appears inline, not as a pop-up) at each measurement point. Keep the interruption minimal to avoid the survey itself affecting the emotional state. Compare advisor-prompt engagers to skippers using propensity score matching."
    },
    {
      "id": "tests-031",
      "type": "validation_strategy",
      "title": "Validate Depletion-Aware Pacing with Decompression Moments",
      "validates_element": "feels-004",
      "journey_phases": ["onboarding", "listing_creation", "pricing"],
      "problem": "feels-004 asserts that the listing wizard must follow a pacing pattern that front-loads hard decisions, includes decompression moments, and frames remaining work as easy. Without this, the host experiences mounting pressure and declining decision quality.",
      "solution": "Measure whether decompression moments (inserted after Steps 2-3 and 4-5) reduce the depletion curve observed in analytics.",
      "evidence": [
        {
          "source": "kahneman-part1-two-systems.txt, The Busy and Depleted System 2",
          "insight": "The parole judges' approval rate spikes after each meal and drops to near zero before the next. Decompression moments are the listing wizard's meal breaks."
        },
        {
          "source": "andreas-call.txt, 0:15 and 5:44-6:11",
          "insight": "Andreas arrives already busy and by call end is fatigued. The platform faces a depleted user from the start."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "A/B test the listing wizard with and without decompression moments. Variant A: after Step 2 and Step 4, a decompression screen appears showing progress summary ('Your unit details and pricing are set. The hard part is done.') with a 500ms pause before the next step reveals. Variant B: standard wizard with immediate transitions between all steps. Measure: (1) completion rate, (2) per-step time (to detect where depletion causes slowdown), (3) default acceptance rate per step (to detect where depletion causes mindless defaults), (4) post-completion satisfaction: 'How exhausting was this process?' (1-5, where 5 is not at all exhausting).",
      "success_criteria": "Variant A completion rate is at least 10% higher than Variant B. Default acceptance rate in Steps 4-6 is no more than 15pp higher than Steps 1-3 in Variant A (compared to potentially 20pp+ in Variant B). Satisfaction rating averages 3.5+ for Variant A. The depletion curve (default acceptance by step) is measurably flatter in Variant A.",
      "failure_meaning": "If decompression moments do not improve completion rate, they may be positioned at the wrong steps (the depletion peak may be at a different point than predicted). If satisfaction is not higher, the decompression moments may feel like delays rather than relief -- the copy may need to be more rewarding. If the depletion curve does not flatten, the hard decisions may not be sufficiently front-loaded in the step order.",
      "implementation_hint": "Build the decompression screens as lightweight interstitials (not full pages) that appear inline between steps. The 500ms pause is a CSS animation delay, not a loading state. Track all per-step metrics with the variant as a dimension. Run for at least 3 weeks to reach statistical significance."
    },
    {
      "id": "tests-032",
      "type": "validation_strategy",
      "title": "Validate Associative Warmth: Emotional Rhyme Across Touchpoints",
      "validates_element": "feels-005",
      "journey_phases": ["onboarding", "listing_creation", "proposal_mgmt", "active_lease", "retention"],
      "problem": "feels-005 asserts that every touchpoint must emotionally rhyme with the phone call's register: warm, professional, specific, personal. Emotional inconsistency between the call and the platform triggers System 1 suspicion.",
      "solution": "Audit all platform copy (emails, notifications, error messages, empty states) for tonal consistency with the phone call's register, using a combination of manual review and user perception testing.",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:27-6:11",
          "insight": "The call's emotional fingerprint: warm, personal, competent, proactive. Every subsequent touchpoint is measured against this fingerprint."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Associative Machine and The Marvels of Priming",
          "insight": "Emotional associations transfer from one context to another. Incoherence triggers System 1 alarm."
        }
      ],
      "priority": "high",
      "validation_method": "manual_review",
      "test_description": "Conduct a comprehensive copy audit of all host-facing text across the platform, organized by touchpoint: (1) onboarding emails, (2) dashboard welcome message, (3) wizard step instructions, (4) error messages, (5) empty states, (6) notification text, (7) proposal display text, (8) active lease dashboard text. For each touchpoint, evaluate against the call register criteria: does it sound like something Bryant would say? Is it warm without being casual? Is it specific without being overwhelming? Is it personal without being intimate? Score each touchpoint 1-5 on register match. Then test with 4-6 hosts: show them the phone call transcript and then the platform copy, and ask 'Does this feel like the same person or company?' (1-5 scale).",
      "success_criteria": "All touchpoints score 3.5+ on register match in the internal audit. At least 4 of 6 hosts rate the platform experience as 'the same person or company' at 4+ on the 5-point scale. No individual touchpoint scores below 3 (any score below 3 indicates a tonal break that needs immediate correction). Error messages and empty states -- traditionally the most corporate-sounding copy -- score at least 3.5.",
      "failure_meaning": "If register match is low for specific touchpoints, those touchpoints are using corporate, technical, or impersonal language that breaks the warm chain. The fix is copy rewriting, not design change. If hosts perceive the platform as a different entity from the call, the emotional warmth is not carrying through the digital transition -- the copy, the visual warmth, and the agent presence may all need strengthening in concert.",
      "implementation_hint": "Create a copy audit spreadsheet with columns for: touchpoint location, current copy text, register match score (1-5), recommended revision (if score below 4), and the 'Bryant test' (would Bryant say this? Yes/No). Schedule quarterly copy audits as new features are added."
    },
    {
      "id": "tests-033",
      "type": "validation_strategy",
      "title": "Validate Forward Momentum at Every Interaction Close",
      "validates_element": "feels-006",
      "journey_phases": ["evaluation", "onboarding", "listing_creation", "proposal_mgmt"],
      "problem": "feels-006 asserts that every interaction conclusion must include a confirmation of what was accomplished AND a preview of what comes next. Dead-end completion screens ('Done.') dissipate forward momentum.",
      "solution": "Measure whether next-step previews at completion points increase same-session continuation to the next phase.",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:44-6:11",
          "insight": "The call ends with 3 clear forward-looking elements: open channel, enthusiasm for the next step, promise of continuation."
        },
        {
          "source": "kahneman-part1-two-systems.txt, The Associative Machine",
          "insight": "When the host sees a preview of the next step, System 1 begins preparing for it, reducing cognitive startup cost."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "A/B test completion screens at two key points: (1) after listing publication and (2) after proposal acceptance. Variant A (forward momentum): completion confirmation + specific next-step preview ('Your listing is live. Bryant will share it with matched tenants this week. You will get a notification when someone sends a proposal.'). Variant B (dead end): completion confirmation only ('Your listing has been published.' with 'View Listing' and 'Return to Dashboard' buttons). Measure: (1) same-session continuation rate (does the host do anything else before leaving?), (2) time-to-return for the next action (how long before the host comes back?), (3) next-phase engagement (does the host respond to the first proposal faster if they were in the forward-momentum group?).",
      "success_criteria": "Variant A shows at least 20% higher same-session continuation rate. Time-to-return for the next action is at least 25% shorter for Variant A. Qualitative: hosts in Variant A describe the completion as 'I know what happens next' rather than 'I guess I am done now.'",
      "failure_meaning": "If forward momentum copy does not increase continuation, the host may be genuinely done for the session (depleted after the wizard) and no copy can overcome that. In this case, the forward momentum is better invested in the re-engagement notification (behaves-006) rather than the completion screen. If time-to-return is similar, the preview may not be creating urgency or curiosity -- the next step may not be interesting enough to pull the host back.",
      "implementation_hint": "Use feature flags for the completion screen variants. Track session_continued (any action after completion screen view within the same session) and next_action_timestamp (first action in the next session). The forward-momentum copy should reference the specific next step with a time estimate."
    },
    {
      "id": "tests-034",
      "type": "validation_strategy",
      "title": "Validate Depleted User Dignity: Invisible Smart Defaults",
      "validates_element": "feels-007",
      "journey_phases": ["listing_creation", "pricing", "proposal_mgmt"],
      "problem": "feels-007 asserts that smart defaults must be invisible accommodations, not labeled recommendations. The host should feel they are confirming research-based suggestions, not outsourcing their judgment. Highlighting that defaults are pre-filled diminishes the host's sense of agency.",
      "solution": "Test whether different framings of default values affect the host's perceived ownership of their listing.",
      "evidence": [
        {
          "source": "kahneman-part1-two-systems.txt, The Busy and Depleted System 2",
          "insight": "Depleted hosts CAN make good decisions but will not unless the cost is very low. Smart defaults make the cost near-zero while preserving dignity."
        },
        {
          "source": "andreas-call.txt, 3:02-3:16",
          "insight": "Andreas processes at full capacity early in the call but has less energy by the end. The platform must handle this transition gracefully."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Show 6 hosts two versions of the pricing step (Step 4) with identical default values. Version A (invisible dignity): Fields pre-populated with 'Average for Washington Heights walk-ups: $2,400/mo' as the source label, no mention of 'recommended' or 'pre-filled.' Version B (highlighted recommendation): Banner at top: 'We have pre-filled your pricing based on similar listings. Review our suggestions below.' After completing each version, ask: 'Who decided the pricing for your listing -- you or the platform?' (open-ended) and 'How much ownership do you feel over your listing?' (1-5 scale).",
      "success_criteria": "In Version A, at least 4 of 6 hosts describe the pricing decision as their own ('I decided' or 'I confirmed'). In Version B, at least 3 of 6 attribute the decision to the platform ('they suggested' or 'it was pre-filled'). Ownership rating for Version A is at least 0.7 points higher than Version B on the 5-point scale.",
      "failure_meaning": "If both versions produce similar ownership feelings, the framing difference may be too subtle to affect perception -- hosts may not care whether defaults are labeled as recommendations or data. If Version B produces higher ownership (because it makes the defaults visible for review), transparency may be more important than invisibility for this audience.",
      "implementation_hint": "Build two interactive prototypes of Step 4 with identical default values and identical modification affordances. The only difference is the framing copy and the presence/absence of the 'pre-filled' banner. Test with hosts who have real pricing context (actual property owners) to ensure the default values feel realistic."
    },
    {
      "id": "tests-035",
      "type": "validation_strategy",
      "title": "Validate Human Thread: Agent Accessibility as Psychological Safety Net",
      "validates_element": "feels-008",
      "journey_phases": ["onboarding", "listing_creation", "proposal_mgmt", "active_lease"],
      "problem": "feels-008 asserts that the host must always feel one step from a real person. The 'Message Bryant' action must be within 2 taps from any screen. The agent's presence is a safety net -- most hosts will not use it, but knowing it is there reduces anxiety.",
      "solution": "Test whether the persistent visibility of the agent contact action reduces confusion-related abandonment and support tickets.",
      "evidence": [
        {
          "source": "andreas-call.txt, 5:44",
          "insight": "Andreas establishes a permanent human channel: 'You can just email me at any time.' The platform must never replace this expectation."
        },
        {
          "source": "kahneman-part1-two-systems.txt, Illusions",
          "insight": "The feeling of connection to Bryant is genuine. The platform risks creating a cognitive illusion by showing the photo without real accessibility."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Track: (1) visibility of the 'Message [agent name]' action (is it accessible within 2 taps from every authenticated screen?), (2) usage rate (what percentage of hosts use the agent messaging action?), (3) correlation between agent-message visibility and confusion-related abandonment (do hosts who encounter the agent contact at a confusion point continue versus hosts who encounter a FAQ or help center?), (4) support ticket analysis: are tickets phrased as 'I could not reach my agent' or 'the system did not work'? The first indicates the human thread is valued but broken; the second indicates it was never relied upon.",
      "success_criteria": "Agent contact action is accessible in 2 or fewer taps from 100% of authenticated screens (automated accessibility audit). Usage rate is between 5-15% (low usage indicates the safety net is working -- hosts feel safe without needing to actually use it). Confusion-related abandonment is at least 20% lower on screens where the agent contact is visible versus screens where it is not. Support tickets mentioning the agent by name indicate relationship (positive), not frustration (negative).",
      "failure_meaning": "If usage rate exceeds 20%, the platform itself may be too confusing, forcing hosts to rely on human support. This is a platform design problem, not a safety net success. If usage rate is 0%, the agent contact may be too hidden or hosts may not realize it connects to a real person. If abandonment is not affected by agent visibility, the human thread may not be the safety mechanism for this audience -- they may prefer self-service help content.",
      "implementation_hint": "Run an automated audit of all authenticated pages to verify the 2-tap agent-access constraint. Track agent_message_opened events with the originating page as context. Create a dashboard showing usage rate by page to identify screens where hosts most need the human safety net."
    },
    {
      "id": "tests-036",
      "type": "validation_strategy",
      "title": "Journey-Level Validation: End-to-End Conversion from Call to Published Listing",
      "validates_element": "journey-level",
      "journey_phases": ["evaluation", "onboarding", "listing_creation", "pricing"],
      "problem": "Individual element validations test specific design decisions, but the host journey is a connected sequence where each phase's success depends on the prior phase. The end-to-end conversion from completed phone call to published listing is the ultimate test of whether the design stack's principles work together: System 1 trust gates pass, depletion is managed, speed matches the call, trust transfers from human to digital, and the host completes the full wizard.",
      "solution": "Track the end-to-end funnel from phone call completion to published listing as the primary success metric for the entire design stack.",
      "evidence": [
        {
          "source": "Layer 6 coherence report, reinforcement clusters",
          "insight": "8 reinforcement clusters show that multiple layers converge on the same principles. The end-to-end funnel tests whether these converging principles actually produce the intended outcome: a host who went from call to published listing without dropping off."
        },
        {
          "source": "Layer 0 journey context, cross-phase patterns",
          "insight": "System 1 trust gating, progressive depletion, priming transfer, and the human-to-digital handoff are all cross-phase patterns that can only be validated by measuring the entire journey, not individual phases."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Build a cohort-based funnel analysis tracking every host from phone call completion through published listing. Funnel stages: (1) phone call completed, (2) onboarding email sent, (3) onboarding email opened, (4) first platform visit, (5) listing wizard started, (6) listing wizard Step 3 reached (past the typical depletion dropout point), (7) listing published. Track conversion rate at each stage, time between stages, and the total time from call to published listing. Segment by agent, property type, and number of units. Run weekly cohort reports to track improvement as design elements are implemented.",
      "success_criteria": "End-to-end conversion rate (call completed to listing published) above 40%. Time from call to published listing under 48 hours for 60% of converting hosts. No single funnel stage shows a conversion rate below 60% (no stage is a black hole). The highest dropout stage should be identifiable and addressable.",
      "failure_meaning": "If end-to-end conversion is below 40%, identify which stage is the primary bottleneck. If Stage 3 (email opened) to Stage 4 (first platform visit) is the weakest link, the trust bridge (works-004, communicates-006) is failing. If Stage 5 (wizard started) to Stage 6 (Step 3 reached) is weakest, depletion design (works-002, feels-004) is failing. If Stage 6 to Stage 7 (published) is weakest, the later wizard steps need simplification. The funnel diagnosis points directly to which design element cluster needs attention.",
      "implementation_hint": "Integrate CRM data (phone call events) with platform analytics (user actions) using a shared host identifier. Build the funnel in the analytics platform with stage timestamps. Create a weekly dashboard showing the funnel with conversion rates, median time between stages, and week-over-week trends. Flag any stage where conversion drops below 50% for immediate investigation."
    },
    {
      "id": "tests-037",
      "type": "validation_strategy",
      "title": "Journey-Level Validation: Emotional Arc Consistency from Discovery to Active Lease",
      "validates_element": "journey-level",
      "journey_phases": ["discovery", "evaluation", "onboarding", "listing_creation", "pricing", "proposal_mgmt", "active_lease"],
      "problem": "Layer 5 defines a progressive emotional arc: calm (discovery) -> relief (evaluation/pricing) -> confidence (listing creation) -> momentum (wizard) -> safety (active lease). If any phase breaks the emotional progression -- a warm call followed by a cold platform, or a confident listing creation followed by an anxious proposal experience -- the associative coherence is disrupted and the host's cumulative trust erodes.",
      "solution": "Conduct longitudinal sentiment tracking across the full host journey, measuring whether the emotional arc progresses as designed or breaks at specific transitions.",
      "evidence": [
        {
          "source": "Layer 5 emotional_arc_summary",
          "insight": "The designed arc is: calm -> relief -> confidence -> momentum -> safety -> curiosity. Each phase builds on the emotional foundation of the prior phase."
        },
        {
          "source": "Layer 6 emotional_arc_check",
          "insight": "The coherence auditor confirms the arc is progressive and conflict-free. No phase targets an emotion that contradicts a prior phase."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Longitudinal qualitative study with 4-6 hosts tracked from phone call through first proposal acceptance (approximately 2-4 weeks). At each phase transition, conduct a brief check-in (5 minutes, via the host's preferred channel -- likely text or email): 'One word: how are you feeling about the process right now?' and 'Is there anything about the experience that surprised you -- positively or negatively?' Compile the single-word emotional descriptors across phases for each host and map them against the designed arc. At the final check-in (after proposal acceptance), conduct a 15-minute retrospective interview: 'Walk me through the journey from your phone call to now. Where did things feel smooth? Where did things feel off?'",
      "success_criteria": "At least 3 of 5 hosts show an emotional trajectory that matches the designed arc (calm -> positive growth, not calm -> frustration). No host reports an emotional regression (e.g., confident at listing creation but anxious at proposal review) at any transition. Retrospective interviews identify no more than 1 significant emotional break point per host. The most common single-word descriptors across all hosts are neutral-to-positive ('easy,' 'fine,' 'good,' 'smooth') rather than negative ('confusing,' 'frustrating,' 'slow').",
      "failure_meaning": "If hosts report emotional regression at a specific transition, that transition is the weakest link in the emotional arc and the design elements governing it need revision. If multiple hosts identify the same break point, the problem is systematic (a design flaw) rather than individual (a host-specific circumstance). If the most common descriptors are negative, the overall experience is failing regardless of individual element quality -- the gestalt is broken even if parts work.",
      "implementation_hint": "Recruit hosts at the phone call stage (coordinate with the sales team). Schedule check-ins at natural phase boundaries: immediately after the call, after first platform visit, after listing publication, after first proposal notification, after proposal action. Use text-based check-ins to minimize effort. Record the retrospective interview for qualitative analysis. This is a high-touch study that should run quarterly with small cohorts."
    }
  ],
  "element_count": 37,
  "coverage": {
    "layer_1_works": {
      "works-001": "tests-001",
      "works-002": "tests-002",
      "works-003": "tests-003",
      "works-004": "tests-004",
      "works-005": "tests-005",
      "works-006": "tests-006"
    },
    "layer_2_communicates": {
      "communicates-001": "tests-007",
      "communicates-002": "tests-008",
      "communicates-003": "tests-009",
      "communicates-004": "tests-010",
      "communicates-005": "tests-011",
      "communicates-006": "tests-012",
      "communicates-007": "tests-013"
    },
    "layer_3_looks": {
      "looks-001": "tests-014",
      "looks-002": "tests-015",
      "looks-003": "tests-016",
      "looks-004": "tests-017",
      "looks-005": "tests-018",
      "looks-006": "tests-019",
      "looks-007": "tests-020"
    },
    "layer_4_behaves": {
      "behaves-001": "tests-021",
      "behaves-002": "tests-022",
      "behaves-003": "tests-023",
      "behaves-004": "tests-024",
      "behaves-005": "tests-025",
      "behaves-006": "tests-026",
      "behaves-007": "tests-027"
    },
    "layer_5_feels": {
      "feels-001": "tests-028",
      "feels-002": "tests-029",
      "feels-003": "tests-030",
      "feels-004": "tests-031",
      "feels-005": "tests-032",
      "feels-006": "tests-033",
      "feels-007": "tests-034",
      "feels-008": "tests-035"
    },
    "journey_level": ["tests-036", "tests-037"]
  },
  "validation_method_distribution": {
    "analytics": 10,
    "usability_test": 15,
    "a_b_test": 7,
    "automated": 2,
    "manual_review": 1,
    "by_element_type": {
      "process_pattern": "analytics (4), a_b_test (1), usability_test (1)",
      "info_architecture": "usability_test (4), a_b_test (2), analytics (1)",
      "visual_pattern": "usability_test (6), a_b_test (1)",
      "interaction_pattern": "analytics (3), usability_test (2), a_b_test (1), automated (1)",
      "emotional_element": "usability_test (5), a_b_test (2), manual_review (1)",
      "journey_level": "analytics (1), usability_test (1)"
    }
  },
  "prioritization_summary": {
    "high_priority": ["tests-001", "tests-002", "tests-003", "tests-004", "tests-006", "tests-007", "tests-008", "tests-009", "tests-010", "tests-011", "tests-012", "tests-014", "tests-016", "tests-017", "tests-021", "tests-022", "tests-023", "tests-024", "tests-026", "tests-028", "tests-029", "tests-031", "tests-032", "tests-036"],
    "medium_priority": ["tests-005", "tests-013", "tests-015", "tests-018", "tests-019", "tests-020", "tests-025", "tests-027", "tests-030", "tests-033", "tests-034", "tests-035", "tests-037"],
    "high_count": 24,
    "medium_count": 13
  },
  "implementation_sequence_recommendation": [
    {
      "phase": "Phase 1: Foundational Analytics (Week 1-2)",
      "tests": ["tests-001", "tests-002", "tests-003", "tests-021", "tests-024", "tests-036"],
      "rationale": "Instrument the core funnel and automated performance checks first. These provide baseline data that informs all subsequent testing."
    },
    {
      "phase": "Phase 2: Core A/B Tests (Week 3-6)",
      "tests": ["tests-004", "tests-008", "tests-012", "tests-022", "tests-026", "tests-031"],
      "rationale": "Run the highest-impact A/B tests: trust bridge, financial display, agent attribution, sequential disclosure, notifications, decompression moments. These test the 6-layer reinforcement clusters."
    },
    {
      "phase": "Phase 3: Qualitative Usability (Week 4-8)",
      "tests": ["tests-007", "tests-009", "tests-010", "tests-014", "tests-016", "tests-028"],
      "rationale": "Conduct moderated usability tests on information architecture, visual hierarchy, and emotional first impressions. These require recruited participants and controlled settings."
    },
    {
      "phase": "Phase 4: Refinement Tests (Week 8-12)",
      "tests": ["tests-005", "tests-013", "tests-015", "tests-018", "tests-019", "tests-020", "tests-025", "tests-029", "tests-030", "tests-033", "tests-034"],
      "rationale": "Test medium-priority elements: advisor moments, typographic triad, green badge learning, motion timing, emotional arc details. These refine the design rather than validate its foundation."
    },
    {
      "phase": "Phase 5: Longitudinal Journey Validation (Week 8-16)",
      "tests": ["tests-037"],
      "rationale": "The longitudinal emotional arc study requires 2-4 weeks per host cohort. Start recruiting in Week 8 and complete by Week 16."
    },
    {
      "phase": "Ongoing: Automated and Continuous",
      "tests": ["tests-006", "tests-011", "tests-017", "tests-023", "tests-027", "tests-032", "tests-035"],
      "rationale": "Analytics-based validations that run continuously in production, monitoring for regressions and tracking improvement over time."
    }
  ],
  "methodology_notes": "37 validation strategies produced: one per element from Layers 1-5 (35 elements) plus 2 journey-level validations. Validation methods are matched to element types: process patterns primarily use analytics (measurable outcomes), information architecture uses usability testing (comprehension and scan patterns), visual patterns use usability testing (perception and first impressions), interaction patterns use a mix of automated testing (performance) and analytics (behavior), and emotional elements use qualitative usability testing (sentiment and self-report). A/B tests are reserved for elements where a clear binary comparison exists and sufficient traffic is expected. The implementation sequence prioritizes foundational analytics first (to establish baselines), then core A/B tests (to validate the highest-impact design decisions), then qualitative refinement (to tune the details). The two journey-level validations bookend the testing program: the funnel analytics (tests-036) runs from Day 1, while the longitudinal emotional study (tests-037) runs as a capstone after the core elements are implemented."
}