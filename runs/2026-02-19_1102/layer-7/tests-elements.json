{
  "lens": {
    "host_call": "john-call.txt",
    "book_extract": "cialdini-social-proof.txt"
  },
  "elements": [
    {
      "id": "tests-001",
      "type": "validation_strategy",
      "title": "Similar-Other Proof Effectiveness on Payment Objection Rate",
      "validates_element": "works-001",
      "journey_phases": ["evaluation", "onboarding", "pricing"],
      "problem": "If the peer proof card is poorly matched (wrong city, wrong experience level, wrong property tier), it produces the 33% compliance effect instead of the 70% effect. Worse, mismatched proof could highlight the host's isolation rather than reduce it, actively increasing rejection.",
      "solution": "A/B test three conditions during onboarding calls: (A) no peer proof presented, (B) generic peer proof ('hosts on our platform'), (C) similarity-matched peer proof ('NYC landlords with 10+ years'). Measure payment-terms objection rate across all three conditions.",
      "evidence": [
        {
          "source": "cialdini-social-proof.txt, Monkey Me Monkey Do section (wallet experiment)",
          "type": "book",
          "quote": "Only 33 percent of the wallets were returned when the first finder was seen as dissimilar, but fully 70 percent were returned when he was thought to be a similar other.",
          "insight": "The 70% vs. 33% gap is the core hypothesis. If similarity-matched proof does not significantly outperform generic proof, the entire peer proof architecture needs rethinking."
        },
        {
          "source": "john-call.txt, 07:44-08:08",
          "type": "host_call",
          "quote": "What we do in New York city as when you sign the lease, you give the checks.",
          "insight": "John's rejection was explicitly normative. If peer proof from similar NYC landlords does not reduce this type of objection, the social proof mechanism is weaker than the established norm for this host segment."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "Across 60+ onboarding calls (20 per condition), agents are trained to present payment terms under three conditions: (A) no peer reference, (B) generic peer reference ('hosts on our platform accept this'), (C) matched peer reference ('John M., an NYC landlord with 8 years experience, accepted these terms and received payment in 18 hours'). Record whether the host raises an explicit objection to payment timing.",
      "success_criteria": "Condition C produces a statistically significant reduction in explicit payment-timing objections compared to Condition A (target: 40%+ reduction). Condition C also outperforms Condition B by at least 15 percentage points.",
      "failure_meaning": "If Condition C does not outperform Condition B, similarity matching is less important than mere social proof presence, and the investment in a segmented proof library may not be justified. If neither B nor C outperforms A, social proof is not the primary lever for overcoming payment-timing objections, and the root cause is structural (the payment model itself) rather than informational.",
      "implementation_hint": "Train agents on the three scripts. Use call recording software to tag calls by condition. Analyst reviews recordings to code objection/no-objection. Minimum 20 calls per condition for statistical validity. Run for 4-6 weeks."
    },
    {
      "id": "tests-002",
      "type": "validation_strategy",
      "title": "Two-Layer Evaluation Separation Impact on Proposal Acceptance",
      "validates_element": "works-002",
      "journey_phases": ["evaluation", "proposal_mgmt"],
      "problem": "If the two-layer separation is too rigid (a hard gate that blocks platform terms until confirmation), hosts may feel manipulated or perceive a bait-and-switch when terms are eventually revealed. The separation must feel natural, not engineered.",
      "solution": "Usability test the two-layer proposal flow with 8-10 hosts. Measure whether hosts who complete the tenant evaluation and confirm interest before seeing platform terms have a higher proposal acceptance rate than hosts who see both simultaneously.",
      "evidence": [
        {
          "source": "john-call.txt, 02:12-03:50 vs. 05:40-08:08",
          "type": "host_call",
          "quote": "And do they have a job here in New York city? [...] No, it wasn't no work.",
          "insight": "The behavioral shift in John's call from cooperative (tenant evaluation) to resistant (platform terms) is the core evidence. If separating these layers preserves the cooperative state, the pattern is validated."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 8-10 hosts (preferably with 5+ years landlord experience). Show them a prototype proposal flow with two conditions: (A) combined flow where tenant details and payment terms appear on the same screen, (B) separated flow where tenant details are presented first, a confirmation gate is passed, and then payment terms appear on a visually distinct surface. Measure: (1) time spent on tenant evaluation, (2) self-reported satisfaction with tenants (1-5 scale), (3) reaction to payment terms (1-5 scale), (4) overall proposal acceptance/rejection.",
      "success_criteria": "Condition B produces higher tenant satisfaction scores (4+ average) AND the payment-terms reaction is no worse than Condition A. If the separation preserves the positive tenant evaluation without worsening the terms reaction, it is validated. Ideal outcome: tenant satisfaction is higher AND terms reaction is also improved because the positive evaluation creates momentum.",
      "failure_meaning": "If Condition B produces lower overall acceptance than Condition A, the separation may be creating a perceived bait-and-switch: hosts feel they were shown the good news first to soften them for bad news. If tenant satisfaction is the same in both conditions, the separation adds complexity without emotional benefit. In either case, a softer separation (visual distinction without a hard gate) should be tested as an alternative.",
      "implementation_hint": "Build a Figma prototype with both flows. Use a moderated usability test format (Zoom or in-person). Ask hosts to think aloud as they review the proposal. Record facial expressions at the transition from tenant details to payment terms. Code emotional valence (positive/neutral/negative) at the transition point."
    },
    {
      "id": "tests-003",
      "type": "validation_strategy",
      "title": "Directed Next Step Closure Impact on Second-Touchpoint Rate",
      "validates_element": "works-003",
      "journey_phases": ["onboarding", "proposal_mgmt"],
      "problem": "If the directed closure card is implemented but the platform cannot deliver on its promises (e.g., the agent names a specific time but then misses it), the overdue state destroys more trust than a vague handoff would. The pattern is only as good as the organization's ability to fulfill its commitments.",
      "solution": "Track second-touchpoint rate (host responds to follow-up within 48 hours) for calls that end with (A) vague handoffs ('I'll talk to our team') vs. (B) directed closures (named person, specific action, specific timeline). Compare rates across the two conditions.",
      "evidence": [
        {
          "source": "john-call.txt, 08:08-08:33",
          "type": "host_call",
          "quote": "I can talk to our team and see if that's something we can make an exception for.",
          "insight": "Bryant's vague closure is the baseline condition. Any host who received this type of closure and did not respond to follow-up represents the cost of diffused responsibility."
        },
        {
          "source": "cialdini-social-proof.txt, Devictimizing Yourself section",
          "type": "book",
          "quote": "Stare, speak, and point directly at that person and no one else: 'You, sir, in the blue jacket, I need help.'",
          "insight": "Cialdini's directed-action principle predicts that specific assignments produce action. If directed closures do not improve second-touchpoint rates, the host's disengagement is deeper than vague handoffs -- it may be a fundamental objection that no follow-up structure can overcome."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "For all onboarding and proposal calls over a 6-week period, train agents on the directed closure protocol (name themselves, state specific action, commit to specific timeline, assign host a micro-action). Track two metrics: (1) whether the host responds to follow-up within 48 hours, (2) whether the host ultimately accepts the proposal. Compare to the prior 6-week baseline of vague-closure calls.",
      "success_criteria": "Directed closures produce a 30%+ improvement in 48-hour second-touchpoint rate compared to the vague-closure baseline. Secondarily, hosts who receive directed closures have a higher ultimate proposal acceptance rate.",
      "failure_meaning": "If second-touchpoint rates do not improve, the host's disengagement is not caused by vague handoffs but by fundamental objections (the payment model itself, lack of trust, better alternatives). In that case, the directed closure is cosmetic -- it makes the call ending feel better but does not change outcomes. The team should then focus on resolving the underlying objection (payment model flexibility) rather than improving the follow-up structure.",
      "implementation_hint": "Add fields to the CRM for closure type (vague vs. directed), specific commitment made, timeline promised, and whether the commitment was fulfilled on time. Analytics dashboard tracks second-touchpoint rate by closure type. Flag any overdue commitments for immediate supervisor escalation."
    },
    {
      "id": "tests-004",
      "type": "validation_strategy",
      "title": "Social Norm Framing vs. Economic Framing for Financial Advantages",
      "validates_element": "works-004",
      "journey_phases": ["pricing", "proposal_mgmt"],
      "problem": "If social norm framing is used without actual social proof to back it (e.g., 'NYC hosts typically hold double deposits' when the platform has few NYC hosts), hosts may perceive the framing as deceptive. The norm framing must be authentic, not aspirational.",
      "solution": "A/B test two versions of the deposit explanation during calls: (A) economic framing ('Split Lease matches your $3,000 deposit to $6,000'), (B) social norm framing ('NYC hosts on our platform typically hold double the standard deposit -- your $3,000 becomes $6,000'). Measure host engagement with the deposit benefit.",
      "evidence": [
        {
          "source": "john-call.txt, 05:56-06:34",
          "type": "host_call",
          "quote": "Split lease would match that. So it would be 6,000, which we'd hold in total.",
          "insight": "John completely ignored the doubled deposit when presented in economic framing. If social norm framing produces even modest engagement, it validates the reframing approach."
        },
        {
          "source": "cialdini-social-proof.txt, Truths Are Us section (canned laughter research)",
          "type": "book",
          "quote": "Canned laughter is most effective for poor jokes.",
          "insight": "Social proof framing should be most effective for the platform's weakest proposition (payment timing). If it works for the deposit (a relatively strong proposition), it should work even better for payment timing."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "Across 40+ calls (20 per condition), agents use either economic framing or social norm framing when introducing the deposit match. Measure: (1) whether the host asks a follow-up question about the deposit (engagement signal), (2) whether the host references the deposit positively later in the call, (3) whether the host's overall objection includes or excludes the deposit (if the host objects only to payment timing but not the deposit, the framing worked for the deposit at least).",
      "success_criteria": "Social norm framing (B) produces at least 30% higher engagement (follow-up questions about the deposit) than economic framing (A). Hosts in Condition B are also more likely to exclude the deposit from their objections (objecting only to payment timing, not to the deposit structure).",
      "failure_meaning": "If social norm framing does not increase engagement with the deposit benefit, the issue is not framing but salience -- the deposit benefit may simply be invisible when payment timing dominates the host's attention. In that case, the solution is not better framing but better timing: present the deposit benefit at a moment when payment timing is not simultaneously under discussion.",
      "implementation_hint": "Train agents on both scripts. Use call recording to code engagement. Track deposit-specific engagement as a binary (asked follow-up yes/no) and objection scope (payment timing only vs. payment timing AND deposit). 20 calls per condition over 4 weeks."
    },
    {
      "id": "tests-005",
      "type": "validation_strategy",
      "title": "Social Proof Generator Conversion Rate at Lease Completion",
      "validates_element": "works-005",
      "journey_phases": ["active_lease", "retention"],
      "problem": "If the testimonial collection process is too intrusive or badly timed, it damages the host's post-lease satisfaction. If it is too passive, collection rates are too low to build a meaningful proof library. The process must balance collection efficiency with relationship preservation.",
      "solution": "Implement a post-lease testimonial collection flow triggered within 48 hours of successful lease completion. Measure collection rate (percentage of hosts who provide a usable, segmented testimonial) and post-collection satisfaction (does the collection process itself harm the relationship).",
      "evidence": [
        {
          "source": "john-call.txt, 06:51-07:07",
          "type": "host_call",
          "quote": "I've been renting my apartment almost 12 years, back and forth.",
          "insight": "Experienced hosts like John are both the hardest to convert AND the most valuable testimonial sources. If the collection process fails to capture experienced-host testimonials, the proof library will be populated by less-credible novice-host testimonials that do not move the target segment."
        },
        {
          "source": "cialdini-social-proof.txt, Truths Are Us section (Bandura research)",
          "type": "book",
          "quote": "The most effective type of clips were those depicting not one but a variety of other children interacting with their dogs.",
          "insight": "Variety in the proof library matters. The collection system must capture testimonials from diverse host profiles, not just the most willing responders."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "At lease completion, trigger a 2-question text survey within 48 hours: (1) 'Would you recommend Split Lease to another NYC landlord? Yes/No' (2) 'What was the biggest difference from your usual renting process?' Tag each response with host similarity dimensions (city, experience years, property tier). Track: collection rate, response quality (usable vs. generic), and host retention rate post-collection (does the collection itself cause any drop in re-listing behavior).",
      "success_criteria": "60%+ collection rate for the binary question. 40%+ provide a usable qualitative response that can be segmented by similarity dimensions. Post-collection retention rate is no lower than pre-collection baseline (collection does not harm the relationship).",
      "failure_meaning": "If collection rate is below 40%, the timing or channel is wrong (text survey may need to be replaced by agent-mediated collection). If qualitative responses are generic and unsegmentable, the questions need to be more specific ('What would you tell another Upper East Side landlord about the payment process?'). If retention drops post-collection, the process is too intrusive and needs to be made lighter.",
      "implementation_hint": "Use an SMS automation platform to send the 2-question survey 48 hours after the last payment is processed for a completed lease. Auto-tag responses with host profile data from the CRM. Human reviewer codes qualitative responses as 'usable' or 'generic' and extracts the key quote for the proof library."
    },
    {
      "id": "tests-006",
      "type": "validation_strategy",
      "title": "Payment Model Preview Reduces Surprise Objection Spike",
      "validates_element": "works-006",
      "journey_phases": ["discovery", "evaluation", "onboarding"],
      "problem": "If the pre-call payment timing preview causes hosts to decline the call entirely (they see 'payment after check-in' in the email and never respond), the preview has moved the objection earlier without solving it. The preview must seed the concept without triggering premature rejection.",
      "solution": "A/B test two pre-call email templates: (A) current template with no payment timing detail, (B) revised template including 'guaranteed payment within 24 hours of check-in' as one of three bullet points. Measure two metrics: call acceptance rate (does the preview reduce the number of hosts who take the call?) and on-call objection rate (does the preview reduce the payment-timing surprise objection during the call?).",
      "evidence": [
        {
          "source": "john-call.txt, 01:31 vs. 05:56",
          "type": "host_call",
          "quote": "Guaranteed payments to you as the landlord [...] the day after the first night that they stayed.",
          "insight": "The 4-minute gap between 'guaranteed payments' and the actual timing created a false expectation that was contradicted. If the preview eliminates this gap, on-call objections should decrease."
        },
        {
          "source": "cialdini-social-proof.txt, Cause of Death: Uncertainty section",
          "type": "book",
          "quote": "When we are unsure of ourselves, when the situation is unclear or ambiguous, when uncertainty reigns, we are most likely to look to and accept the actions of others as correct.",
          "insight": "Early, low-stakes exposure reduces the uncertainty spike. If the preview works, hosts arrive at the call with the timing already incorporated into their mental model."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "Split outreach emails 50/50 between Template A (no timing) and Template B (with timing preview). Track: (1) email open rate, (2) call scheduling rate (does the host agree to a call?), (3) on-call objection to payment timing (did the host raise an explicit objection?). Run for 8 weeks with minimum 50 emails per condition.",
      "success_criteria": "Template B produces no more than a 10% decline in call scheduling rate (small cost of preview) AND at least a 25% reduction in on-call payment-timing objections (large benefit of pre-seeding). The net effect is positive: even if slightly fewer hosts take calls, those who do are better prepared and more likely to convert.",
      "failure_meaning": "If Template B reduces call scheduling by more than 20%, the preview is causing premature rejection -- hosts see the timing and opt out before the agent can provide social proof context. In that case, the preview needs to be paired with social proof in the email itself ('NYC hosts receive guaranteed payment within 24 hours -- here is how one described it'). If on-call objections do not decrease, the preview is not being processed or remembered by hosts, and a different preview format (video, visual, phone pre-call text) should be tested.",
      "implementation_hint": "Use the existing email outreach system. Create Template B variant with the payment timing bullet added alongside 'matched roommates' and 'doubled deposit.' Track opens with email analytics. Code call scheduling and on-call objections from CRM records and call recordings. Agent tags each call with 'host was sent Template A or B.'"
    },
    {
      "id": "tests-007",
      "type": "validation_strategy",
      "title": "Peer Proof Card Scanability and Similarity Recognition",
      "validates_element": "communicates-001",
      "journey_phases": ["evaluation", "pricing"],
      "problem": "If the peer proof card is not scannable in under 5 seconds, it becomes another block of text that the host skips. If the similarity markers are not immediately recognizable, the host processes the proof as generic rather than matched, producing the 33% effect.",
      "solution": "Run a 5-second exposure usability test with 8-10 participants. Show them the peer proof card for 5 seconds, then remove it. Ask: (1) Who was the proof from? (2) What city were they in? (3) What was their experience? (4) What happened to them?",
      "evidence": [
        {
          "source": "cialdini-social-proof.txt, Monkey Me Monkey Do section",
          "type": "book",
          "quote": "The principle of social proof operates most powerfully when we are observing the behavior of people just like us.",
          "insight": "If similarity markers are not recalled after 5 seconds of exposure, they are not being processed as the primary information, which means the visual hierarchy is wrong."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 8-10 participants (ideally landlords or property managers). Show them a peer proof card for exactly 5 seconds. Remove the card. Ask four recall questions: (1) city, (2) experience years, (3) what term the host accepted, (4) their outcome. Score recall accuracy for each question. Also ask: 'How similar to you was the person described on the card?' (1-5 scale).",
      "success_criteria": "80%+ of participants correctly recall the city. 60%+ correctly recall the experience years. 50%+ correctly recall the outcome. Average similarity rating is 3.5+ on a 5-point scale. If city and experience are recalled at high rates, the similarity markers are in the correct visual hierarchy position.",
      "failure_meaning": "If city recall is below 60%, the similarity markers are not visually prominent enough -- the mono font and green accent may need to be larger or more distinctly positioned. If outcome recall is below 30%, the serif outcome statement is being overshadowed by the similarity markers. If the similarity rating is low despite correct recall, the participants do not feel the described host is like them, indicating a similarity-matching algorithm issue rather than a visual design issue.",
      "implementation_hint": "Create 3 proof card variants in Figma with different host profiles. Use a moderated remote test (Zoom screen share). Flash each card for 5 seconds using a timed slide. Record verbal responses. Score recall accuracy against the actual card content."
    },
    {
      "id": "tests-008",
      "type": "validation_strategy",
      "title": "Tenant Quality Layer Preserves Positive Evaluation Through Terms Transition",
      "validates_element": "communicates-002",
      "journey_phases": ["evaluation", "proposal_mgmt"],
      "problem": "If the two-layer information architecture is implemented but the transition feels like a bait-and-switch, hosts may distrust the entire proposal. The visual separation must feel like natural progression, not manipulation.",
      "solution": "Usability test the two-layer proposal screen with think-aloud protocol. Measure whether the host's sentiment about the tenants remains positive after they encounter the platform terms layer.",
      "evidence": [
        {
          "source": "john-call.txt, 02:12-03:50 vs. 05:40-07:44",
          "type": "host_call",
          "quote": "And do they have a job here in New York city? [...] No, it wasn't no work.",
          "insight": "In the call, the positive tenant sentiment did not survive the terms discussion. The test must verify that the visual separation preserves it."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 6-8 hosts. Show them a prototype proposal screen with separated layers. After reviewing Layer A (tenant details), ask: 'On a scale of 1-5, how interested are you in these tenants?' After reviewing Layer B (platform terms), ask the same question again. Also ask: 'Did the way this information was presented feel natural or manipulative?' (open-ended). Record think-aloud commentary at the transition point.",
      "success_criteria": "Average tenant interest score remains at 3.5+ after Layer B exposure (no more than 0.5 point decline from the Layer A score). No more than 1 of 8 participants uses the word 'manipulative' or synonyms when describing the flow.",
      "failure_meaning": "If tenant interest drops by more than 1 full point after Layer B, the terms are still contaminating the tenant evaluation despite visual separation -- the separation is structural but not emotional. In that case, the transition needs stronger emotional bridging (connecting the terms to the specific tenants: 'Here is how we make this match happen for Ariel and Amber'). If participants feel manipulated, the confirmation gate is too heavy-handed and should be replaced with a softer visual transition.",
      "implementation_hint": "Figma prototype with a clickable confirmation gate between layers. Moderated Zoom test. Pre/post scoring on the same 5-point scale. Code think-aloud for emotional valence at the transition point."
    },
    {
      "id": "tests-009",
      "type": "validation_strategy",
      "title": "Next Step Closure Card Visibility and Recall",
      "validates_element": "communicates-003",
      "journey_phases": ["onboarding", "proposal_mgmt"],
      "problem": "If the closure card blends into the page or is scrolled past, the host leaves without registering the specific commitment. The card must be impossible to miss and impossible to misunderstand.",
      "solution": "Eye-tracking or click-tracking analysis of the closure card position on a proposal management page. Verify that the card receives visual attention and that hosts can recall the WHO/WHAT/WHEN after 10 seconds.",
      "evidence": [
        {
          "source": "cialdini-social-proof.txt, Devictimizing Yourself section",
          "type": "book",
          "quote": "Stare, speak, and point directly at that person.",
          "insight": "The closure card is the visual equivalent of pointing at a specific person. If the host's eye does not land on it, the direction is lost."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Show 6-8 participants a complete proposal page with the closure card at the bottom or as a sticky footer. After 10 seconds of free browsing, remove the page and ask: (1) Who is following up? (2) What will they do? (3) When? Score recall accuracy. Also use Hotjar or similar click-tracking on the live implementation to measure whether hosts interact with or scroll past the card.",
      "success_criteria": "80%+ of participants recall the agent's name (WHO). 60%+ recall the action (WHAT). 70%+ recall the timeline (WHEN). On the live implementation, fewer than 20% of hosts scroll past the card without a pause (indicating visual registration).",
      "failure_meaning": "If WHO recall is below 60%, the agent name is not bold enough or not in the primary visual position. If WHEN recall is below 50%, the mono timestamp is not distinctive enough. If most hosts scroll past without pausing, the card needs to be repositioned as a sticky footer or given a more prominent visual treatment (larger, different background color, animation on entry).",
      "implementation_hint": "Figma prototype for the usability test. For live implementation, add Hotjar scroll heatmap tracking on the proposal page. Monitor the scroll-depth at which the closure card position falls relative to the average scroll depth."
    },
    {
      "id": "tests-010",
      "type": "validation_strategy",
      "title": "Peer Proof Card Visual Distinctiveness from Standard Cards",
      "validates_element": "looks-001",
      "journey_phases": ["evaluation", "pricing"],
      "problem": "If the peer proof card is not visually distinct enough from standard UI cards, hosts will process it as platform marketing rather than peer evidence. The entire value of the card depends on the host recognizing it as 'evidence from someone like me' rather than 'platform promotional content.'",
      "solution": "Rapid visual sorting test. Show participants a mixed set of 6 cards (3 standard UI cards, 3 peer proof cards) and ask them to sort by 'platform information' vs. 'what other people experienced.' Measure sorting accuracy and speed.",
      "evidence": [
        {
          "source": "john-call.txt, 07:44-08:08",
          "type": "host_call",
          "quote": "What we do in New York city as when you sign the lease, you give the checks.",
          "insight": "John distinguished between platform assertions ('payments are guaranteed') and peer norms ('what we do'). The visual system must enable this distinction at a glance."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 8 participants. Print or screen-display 6 cards: 3 standard content cards (white background, sans-serif, platform feature descriptions) and 3 peer proof cards (warm background, green left border, mono similarity markers, serif outcome). Ask participants to sort them into two piles: 'information from the company' and 'experiences from real people.' Time the sorting. Then ask: 'What visual cues helped you sort?'",
      "success_criteria": "90%+ sorting accuracy (proof cards correctly identified as 'experiences from real people'). Average sorting time under 30 seconds for all 6 cards. Participants spontaneously mention the green border, mono text, or warm background as distinguishing features.",
      "failure_meaning": "If sorting accuracy is below 75%, the visual distinction is insufficient -- the proof card looks too much like a marketing card. The green left border may need to be thicker, the background warmer, or the typography contrast more pronounced. If participants cannot articulate what made the cards different, the distinction is subliminal rather than conscious, which may still work for System 1 processing but fails for deliberate trust evaluation.",
      "implementation_hint": "Create card mockups in Figma. Print on cardstock for in-person sorting or use an unmoderated tool like Maze for remote card sorting. Record sort time and verbal explanations."
    },
    {
      "id": "tests-011",
      "type": "validation_strategy",
      "title": "Peer Proof Injection Timing Sequence Effectiveness",
      "validates_element": "behaves-001",
      "journey_phases": ["evaluation", "pricing"],
      "problem": "If the 400ms dwell between proof card and term appearance is too short, the host does not process the proof before the term arrives. If it is too long, the sequence feels slow and engineered. The timing must feel natural while ensuring proof is processed first.",
      "solution": "Time-variant usability test with 3 dwell conditions: 200ms, 400ms, and 700ms. Measure whether hosts process the proof card content before evaluating the term.",
      "evidence": [
        {
          "source": "cialdini-social-proof.txt, Monkey Me Monkey Do section",
          "type": "book",
          "quote": "The principle of social proof operates most powerfully when we are observing the behavior of people just like us.",
          "insight": "If the proof does not register before the term, the social proof cannot prime the host's evaluation. Timing is the mechanism that ensures proof primes evaluation."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 9 participants (3 per condition). Show them a screen transition where a peer proof card appears, dwells for 200ms/400ms/700ms, then the payment term appears below. After the full sequence, ask: (1) What did you notice first? (2) What was the proof card about? (3) How did the payment term feel? Rate the overall sequence as 'too fast,' 'just right,' or 'too slow.'",
      "success_criteria": "At 400ms, 80%+ of participants report noticing the proof card first. The 400ms condition is rated 'just right' by at least 2 of 3 participants. At 200ms, fewer than 50% notice the proof first (too fast). At 700ms, at least 1 of 3 participants rates it 'too slow.'",
      "failure_meaning": "If 400ms is rated 'too fast' by most participants, increase to 500-600ms. If 400ms is rated 'too slow,' reduce to 300ms. If no dwell time produces reliable proof-first processing, the sequential approach may be wrong -- consider showing proof and term simultaneously but with a visual hierarchy that ensures the proof occupies the primary attention position.",
      "implementation_hint": "Build 3 prototype variants in Figma with different transition timings. Moderated test with screen recording. Verbal report after each sequence. Small sample per condition is acceptable for this directional test."
    },
    {
      "id": "tests-012",
      "type": "validation_strategy",
      "title": "Two-Phase Evaluation Gate Interaction Naturalness",
      "validates_element": "behaves-002",
      "journey_phases": ["evaluation", "proposal_mgmt"],
      "problem": "If the confirmation gate feels like a forced survey question or a progress-blocking popup, hosts will perceive it as friction rather than as a natural checkpoint. The gate must feel like a pause for reflection, not a mandatory click-through.",
      "solution": "Usability test the confirmation gate in a clickable prototype. Observe whether hosts interact with it naturally or try to bypass it. Measure sentiment at the gate moment.",
      "evidence": [
        {
          "source": "john-call.txt, 05:30",
          "type": "host_call",
          "quote": "It depends if they like it, you know, I liked them. Yeah. Because it's both ways.",
          "insight": "John naturally expressed confirmation of the tenants ('I liked them') without being asked. The gate should mirror this natural moment of assessment."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 6 hosts. Prototype a proposal flow where tenant details are followed by a confirmation prompt: 'Does this look like a good match for your space?' with a confirm button. Observe: (1) Does the host pause naturally at the gate or try to scroll past? (2) Does the host express any frustration at being blocked from seeing more? (3) What does the host say (think-aloud) at the gate moment? Rate the gate experience on a 3-point scale: helpful, neutral, annoying.",
      "success_criteria": "At least 5 of 6 hosts rate the gate as 'helpful' or 'neutral.' No more than 1 participant attempts to bypass the gate. Think-aloud commentary at the gate is neutral or positive ('Yeah, they seem good' or 'Let me think...' rather than 'Why do I have to click this?').",
      "failure_meaning": "If 2+ participants find the gate annoying, it is too heavy. Replace the hard gate with a soft visual transition: the terms section fades in gradually after a scroll depth trigger rather than requiring explicit confirmation. If participants bypass attempts are frequent, the gate is perceived as a blocking mechanism rather than a natural reflection point.",
      "implementation_hint": "Clickable Figma prototype. Moderated test. Record screen and audio. Code think-aloud at the gate as positive/neutral/negative. Track whether participants click confirm immediately or pause to review."
    },
    {
      "id": "tests-013",
      "type": "validation_strategy",
      "title": "Directed Closure Card Countdown Timer Anxiety Check",
      "validates_element": "behaves-003",
      "journey_phases": ["onboarding", "proposal_mgmt"],
      "problem": "A countdown timer could create anxiety rather than confidence if it makes the host feel pressured or if it implies the platform is unreliable (needing a timer to ensure follow-through). The timer must feel like accountability, not urgency.",
      "solution": "A/B test the closure card with and without the countdown timer. Measure host sentiment and whether the timer affects second-touchpoint rate.",
      "evidence": [
        {
          "source": "cialdini-social-proof.txt, Cause of Death: Uncertainty section",
          "type": "book",
          "quote": "When uncertainty reigns, we are most likely to look to and accept the actions of others as correct.",
          "insight": "The timer is designed to reduce uncertainty. If it instead increases anxiety, it is producing the opposite of the intended effect."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "On the proposal management page, show hosts either (A) a closure card with the three fields (WHO/WHAT/WHEN) but no countdown, or (B) the same card with a live per-minute countdown showing time remaining. After 48 hours, survey hosts: 'How confident are you that [agent name] will follow up as promised?' (1-5 scale). Track second-touchpoint rate for both conditions.",
      "success_criteria": "Condition B produces equal or higher confidence scores than Condition A (timer does not create anxiety). Condition B produces equal or higher second-touchpoint rates. If Condition B produces higher confidence AND higher second-touchpoint rates, the timer is validated. If confidence is the same but second-touchpoint rate is higher, the timer works through behavioral accountability (the agent sees it too) rather than host-facing sentiment.",
      "failure_meaning": "If Condition B produces lower confidence scores, the timer creates anxiety -- hosts interpret it as 'this platform needs to count down because it might not follow through.' Remove the host-facing timer and keep only the agent-facing accountability timer. If both conditions produce the same results, the timer is neutral -- not harmful but not beneficial enough to justify the implementation cost.",
      "implementation_hint": "Implement both card variants as a feature flag in the proposal page. Randomly assign hosts. Send the confidence survey via email 48 hours after the card is shown. Track second-touchpoint from CRM data."
    },
    {
      "id": "tests-014",
      "type": "validation_strategy",
      "title": "Norm Validation Copy Calibration",
      "validates_element": "feels-001",
      "journey_phases": ["evaluation", "onboarding", "pricing"],
      "problem": "If the norm validation is too strong ('you are absolutely right, upfront payment is the best approach'), it reinforces the host's position so firmly that introducing the alternative becomes impossible. If it is too weak ('we understand your concern'), it feels dismissive. The calibration must acknowledge the norm without strengthening the host's commitment to it.",
      "solution": "Test 3 copy variants with 9 hosts: (A) no validation -- straight to platform terms, (B) strong validation -- 'Most experienced NYC landlords require payment at signing, and that makes perfect sense given your experience', (C) bridging validation -- 'Most experienced NYC landlords start with upfront payment at signing -- a lot of them said the same thing before they saw how the guarantee works.' Measure receptivity to the platform's payment model after each variant.",
      "evidence": [
        {
          "source": "john-call.txt, 06:51-07:07",
          "type": "host_call",
          "quote": "I've been renting my apartment almost 12 years, back and forth.",
          "insight": "John's professional pride is the emotional core. The validation must honor this pride while creating space for the alternative. Too much validation locks in the pride; too little dismisses it."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 9 hosts (3 per condition). Read each host a scenario where an agent introduces payment terms. Condition A goes straight to terms. Condition B validates strongly, then introduces terms. Condition C validates with a bridge ('landlords said the same thing... then they saw how it works'), then introduces terms. After the script, ask: (1) 'How open are you to this payment model?' (1-5 scale). (2) 'Did you feel the agent understood your perspective?' (1-5 scale). (3) 'Would you want to hear more?' (yes/no).",
      "success_criteria": "Condition C produces the highest openness score (3+ average) while maintaining a high understanding score (4+ average). Condition B produces the highest understanding score but a lower openness score (the host feels understood but more committed to their position). Condition A produces the lowest understanding score.",
      "failure_meaning": "If Condition C produces lower openness than Condition A, the bridge validation is backfiring -- mentioning that other landlords 'said the same thing' may be normalizing the objection rather than bridging past it. If Condition B produces the highest openness, strong validation creates enough safety that the host is willing to consider alternatives. Test a fourth variant that combines strong validation with a longer pause before introducing terms.",
      "implementation_hint": "Role-play scenario test. Recruit hosts who are not current Split Lease users. Read the scenario aloud (or via recorded audio). Collect ratings immediately after each script. Small sample is acceptable for directional signal."
    },
    {
      "id": "tests-015",
      "type": "validation_strategy",
      "title": "First Payment Notification Emotional Impact",
      "validates_element": "feels-006",
      "journey_phases": ["active_lease"],
      "problem": "If the first payment notification is too celebratory, it may feel patronizing to an experienced landlord ('they are celebrating that they did what they promised?'). If it is too clinical, it wastes the highest-leverage emotional moment. The tone must feel earned and genuine, not manufactured.",
      "solution": "A/B test two first-payment notification formats with hosts who complete their first lease: (A) standard payment confirmation ('Payment of $5,000 processed'), (B) ceremonial confirmation ('Your first payment has arrived: $5,000, deposited 16 hours after check-in -- 8 hours ahead of our 24-hour guarantee'). Measure host sentiment and re-listing intent.",
      "evidence": [
        {
          "source": "john-call.txt, 07:13",
          "type": "host_call",
          "quote": "I give him my apartment completely. I trust him and they get, trust me too.",
          "insight": "For John, the first payment is the proof that his trust was warranted. The notification must honor this moment without being presumptuous."
        },
        {
          "source": "cialdini-social-proof.txt, Devictimizing Yourself section (car accident)",
          "type": "book",
          "quote": "Not only was this help rapid and solicitous, it was infectious.",
          "insight": "The first successful experience should create an infectious emotional cascade. The notification design must catalyze this cascade."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "For all hosts completing their first lease, randomly assign to Notification A (standard) or Notification B (ceremonial). Within 72 hours, send a 2-question survey: (1) 'How satisfied are you with your Split Lease experience?' (1-5), (2) 'How likely are you to list your property again through Split Lease?' (1-5). Also track: whether the host opens the notification, time spent on the notification page, and re-listing rate over 90 days.",
      "success_criteria": "Notification B produces higher satisfaction scores (0.5+ point improvement) and higher re-listing intent (0.5+ point improvement) compared to Notification A. Notification B also produces higher open rates and longer engagement time. 90-day re-listing rate for Notification B is at least 10% higher.",
      "failure_meaning": "If Notification B produces lower satisfaction, the ceremonial tone is perceived as patronizing by experienced hosts. In that case, reduce the ceremonial elements and focus on precision: state the exact timing and amount without the celebratory framing. If both notifications produce the same results, the payment itself is the emotional event, not the notification format -- in which case, investment should shift to ensuring the payment arrives early rather than designing the notification.",
      "implementation_hint": "Implement as a notification template A/B test. Use the existing payment processing webhook to trigger the notification. Track opens with analytics. Survey via email 72 hours post-notification. 90-day re-listing tracking from CRM data."
    },
    {
      "id": "tests-016",
      "type": "validation_strategy",
      "title": "End-to-End Journey Arc Validation: Social Proof Pipeline",
      "validates_element": "works-001",
      "journey_phases": ["discovery", "evaluation", "onboarding", "pricing", "proposal_mgmt", "active_lease", "retention"],
      "problem": "Individual elements may test well in isolation but fail when combined into the full journey. The social proof strategy relies on a pipeline: preview in first touch -> peer proof at evaluation -> norm validation at pricing -> directed closure -> first payment celebration -> testimonial collection. If any link in this pipeline breaks, the downstream elements lose their foundation.",
      "solution": "Track a cohort of 20+ hosts through the full journey, from first outreach email through first lease completion, with the full social proof pipeline implemented. Measure conversion rate at each phase transition and compare to the historical baseline.",
      "evidence": [
        {
          "source": "john-call.txt (entire call)",
          "type": "host_call",
          "quote": "Full call trajectory: cooperative -> resistant -> firm -> dismissive -> limbo",
          "insight": "John's journey failed at a single point (pricing) but the failure cascaded forward, killing all downstream phases. The full pipeline must be tested as a system, not as individual elements, because a failure at any point cascades."
        },
        {
          "source": "cialdini-social-proof.txt, Truths Are Us section",
          "type": "book",
          "quote": "We view a behavior as more correct in a given situation to the degree that we see others performing it.",
          "insight": "Social proof is a system effect, not an element effect. Each element reinforces the others. A host who sees peer proof in the email, hears it on the call, and sees it on the platform accumulates a stronger social proof signal than any single touchpoint provides."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Implement the full social proof pipeline for a cohort of hosts entering the funnel over an 8-week period. Track phase-by-phase conversion: (1) email to call (discovery -> evaluation), (2) call to proposal acceptance (evaluation -> proposal_mgmt), (3) proposal to active lease (proposal_mgmt -> active_lease), (4) active lease to re-listing (retention). Compare each transition rate to the 8-week baseline before the pipeline was implemented.",
      "success_criteria": "At least one transition rate improves by 20%+ compared to baseline. Overall discovery-to-active-lease conversion rate improves by 15%+. No transition rate worsens by more than 5% (the pipeline does not harm any phase).",
      "failure_meaning": "If overall conversion does not improve, the individual elements are not compounding as expected. The pipeline may need tighter integration (proof from the email referenced on the call, proof from the call referenced on the platform). If one specific transition worsens, that link is the weak point -- investigate whether the social proof element at that transition is mismatched, mistimed, or missing.",
      "implementation_hint": "Requires coordination across email, call scripts, and platform UI. Use a feature flag to route a cohort through the pipeline while a control group sees the existing flow. 8-week minimum for statistical validity with 20+ hosts per group. Weekly conversion rate dashboards by phase."
    }
  ]
}