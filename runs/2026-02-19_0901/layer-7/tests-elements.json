{
  "lens": {
    "host_call": "Walaa customer call.txt",
    "book_extract": "100things-attention-memory-decisions.txt"
  },
  "elements": [
    {
      "id": "tests-0901-001",
      "type": "validation_strategy",
      "title": "Recognition-First Interface Comprehension Test for ESL Guests",
      "validates_element": "works-001",
      "journey_phases": ["listing_evaluation", "proposal_creation", "move_in", "active_lease"],
      "problem": "The icon-plus-text paired system may not actually reduce comprehension time for language-constrained guests. Icons that seem universal to designers may be ambiguous to guests from different cultural backgrounds.",
      "solution": "Conduct moderated usability tests with 5-8 non-native English speaking guests (varied proficiency levels) comparing icon-plus-text interfaces against text-only interfaces across key screens.",
      "evidence": [
        {
          "source": "Walaa customer call.txt, 0:41-0:53",
          "type": "host_call",
          "quote": "Um, no, you weren't asked how you should assist me or What was that?",
          "insight": "Walaa's spoken English comprehension failures predict even greater difficulty with written platform text. The test must measure whether icons successfully bypass this bottleneck."
        },
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 22",
          "type": "book",
          "quote": "It's easier to recognize information than recall it.",
          "insight": "Weinschenk predicts recognition-based interfaces outperform recall-based ones. The test validates whether this holds for the specific icon set chosen."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Present 5-8 ESL guests with two versions of the listing evaluation page, Stays Manager, and House Manual: one with icon-plus-text pairs, one with text-only labels. Measure time-to-first-correct-action, error rate, and subjective comprehension confidence. Use a within-subjects design with counterbalanced order.",
      "success_criteria": "Icon-plus-text version achieves (a) 30%+ faster time-to-first-correct-action, (b) 50%+ fewer errors, and (c) higher self-reported comprehension confidence (4+ on 5-point scale) compared to text-only, across all tested screens.",
      "failure_meaning": "If icons do not improve comprehension, the specific icon set may be culturally ambiguous or the icons may not map to the concepts they represent. This would require icon redesign and re-testing, not abandonment of the recognition-first principle.",
      "implementation_hint": "Recruit from SplitLease's existing ESL guest pool. Use think-aloud protocol to capture qualitative data on which icons are clear vs. confusing. Test the House Manual icon grid separately since it has the highest icon density."
    },
    {
      "id": "tests-0901-002",
      "type": "validation_strategy",
      "title": "Stays Manager Weekly Habit Formation Rate Tracking",
      "validates_element": "works-002",
      "journey_phases": ["active_lease"],
      "problem": "The graduated habit loop (single-tap weeks 1-2, micro-task weeks 3-4, full routine weeks 5+) is a theoretical design based on Weinschenk's habit formation principles. Whether guests actually form the habit -- and at what rate -- is unknown.",
      "solution": "Track weekly return rate to the Stays Manager across the first 8 weeks of active leases, segmented by the graduated introduction schedule.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 60",
          "type": "book",
          "quote": "Forming a habit takes a long time and requires small steps.",
          "insight": "The theory predicts gradual adoption. The analytics will reveal whether the specific graduation schedule (2 weeks simple, 2 weeks intermediate, ongoing full) matches actual behavior."
        },
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 42",
          "type": "book",
          "quote": "Well-practiced skills don't require conscious attention.",
          "insight": "Automaticity by week 5 is the target. Time-per-visit metrics will show whether the interaction is becoming automatic (decreasing time) or remaining effortful (stable or increasing time)."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Instrument the Stays Manager to track: (a) weekly return rate (% of active-lease weeks where guest opens Stays Manager at least once), (b) time-per-visit (seconds from open to last interaction), (c) action completion rate (% of visits where the single action is completed), (d) off-platform communication rate (texts/calls to host that bypass platform). Track all metrics weekly for the first 13 weeks of each lease. Segment by guest language proficiency if intake data is available.",
      "success_criteria": "By week 4: 70%+ weekly return rate. By week 8: 85%+ weekly return rate with average time-per-visit under 60 seconds. By week 6: action completion rate above 80%. Off-platform communication rate should decrease 30% week-over-week during weeks 1-8.",
      "failure_meaning": "If weekly return rate does not reach 70% by week 4, the graduated introduction may be too slow (guests lose interest before habit forms) or the single-action focus may not provide enough value to motivate return visits. If time-per-visit does not decrease, the interface is not becoming automatic and may need simplification.",
      "implementation_hint": "This is the single most important metric in the entire guest journey. The Stays Manager is visited 13+ times per lease -- it IS the platform relationship for active guests. Instrument from day one. Create a real-time dashboard showing cohort-level habit curves."
    },
    {
      "id": "tests-0901-003",
      "type": "validation_strategy",
      "title": "Flexible-Duration Proposal Completion Rate A/B Test",
      "validates_element": "works-003",
      "journey_phases": ["proposal_creation", "acceptance"],
      "problem": "Pre-selecting the Flexible duration option may reduce proposal completion for guests who DO know their duration, by adding an unnecessary selection step. The tradeoff between accommodating uncertainty and streamlining certainty is unknown.",
      "solution": "A/B test two proposal creation flows: (A) Flexible pre-selected as default, (B) No pre-selection with all options equally weighted. Measure proposal completion rate and average time-to-complete for both groups.",
      "evidence": [
        {
          "source": "Walaa customer call.txt, 5:11-5:22",
          "type": "host_call",
          "quote": "I don't know. I don't know because no, I I'm not fixing yet in this worker, so I don't know how to be.",
          "insight": "Walaa represents the duration-uncertain guest segment. The A/B test will reveal whether this segment is large enough to justify the Flexible-default design."
        },
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 57",
          "type": "book",
          "quote": "People are inherently lazy.",
          "insight": "Weinschenk predicts that the pre-selected option will be accepted by most guests regardless of their actual preference. The test measures whether this default acceptance produces good outcomes (completed proposals) or bad ones (mismatched expectations)."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "Randomly assign guests entering proposal creation to Group A (Flexible pre-selected) or Group B (no pre-selection). Track: (a) proposal completion rate, (b) time from entering proposal flow to submission, (c) which duration option was ultimately selected, (d) host counter-offer rate (do hosts counter-offer more when guests select Flexible?), (e) average tenure of guests who selected Flexible vs. fixed. Run for 4 weeks or until 200+ proposals per group.",
      "success_criteria": "Group A (Flexible default) achieves (a) equal or higher proposal completion rate than Group B, (b) no more than 10% higher host counter-offer rate, and (c) Flexible-selecting guests have equal or longer average tenure than fixed-selecting guests.",
      "failure_meaning": "If Group A has lower completion rates, the Flexible default may confuse guests who know their duration and feel the pre-selection is 'wrong.' If host counter-offer rates spike, hosts may be rejecting Flexible proposals at higher rates, creating guest frustration. Either outcome would require redesigning the duration selector while preserving the Flexible option.",
      "implementation_hint": "Ensure the A/B test segments exclude guests whose agents have already discussed specific durations (this data should be in CRM notes). These guests have external anchors that override the interface default."
    },
    {
      "id": "tests-0901-004",
      "type": "validation_strategy",
      "title": "Agent Context Bar Trust Transfer Measurement",
      "validates_element": "works-004",
      "journey_phases": ["discovery", "search", "listing_evaluation", "proposal_creation"],
      "problem": "The agent context bar ('Bryant picked this for you') is theorized to transfer trust from the human interaction to the platform. But the bar may be ignored, dismissed, or irrelevant if the guest has already formed a positive platform impression independently.",
      "solution": "Measure listing page engagement metrics with and without the agent context bar for agent-referred guests.",
      "evidence": [
        {
          "source": "Walaa customer call.txt, 4:46-4:59",
          "type": "host_call",
          "quote": "I can share some listings with you just over text, if that works and you can let me know if any of them interests you.",
          "insight": "Bryant established the text-to-platform handoff. The context bar bridges this transition. Analytics will show whether guests who see the bar engage more deeply with the listing."
        },
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 79",
          "type": "book",
          "quote": "People use look and feel as their first indicator of trust.",
          "insight": "Trust is established in the first seconds. The test must compare early engagement signals (time on page, scroll depth, action button interaction) between bar-present and bar-absent conditions."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "For agent-referred listing page visits (identified by referral parameter in URL): Group A sees the full agent context bar with avatar and personalized message. Group B sees the standard listing page without agent context. Track: (a) time on listing page, (b) scroll depth, (c) 'Tell Bryant I'm interested' / 'Create Proposal' button tap rate, (d) bounce rate (leave without scrolling), (e) return visit rate (come back to the same listing later).",
      "success_criteria": "Group A (context bar present) shows: (a) 20%+ higher button tap rate, (b) 25%+ lower bounce rate, (c) 15%+ longer time on page. These thresholds reflect the significant trust transfer that the context bar is designed to create.",
      "failure_meaning": "If the context bar does not improve engagement, either the agent relationship is not as important as the call data suggests, or the bar's visual design does not effectively communicate agent involvement. Would require qualitative follow-up to determine which.",
      "implementation_hint": "Ensure agent photos are real, professional headshots. A generic avatar or icon will not trigger the facial recognition System 1 response that the theory depends on. Track the collapse behavior (visit 4+ transition to avatar badge) separately to validate the graduated withdrawal."
    },
    {
      "id": "tests-0901-005",
      "type": "validation_strategy",
      "title": "Monthly Budget Frame Price Display Conversion Impact",
      "validates_element": "works-005",
      "journey_phases": ["listing_evaluation", "proposal_creation"],
      "problem": "Showing monthly estimates as the primary price may reduce perceived value for listings with low nightly rates (where the monthly total looks larger than expected) or create confusion for guests who have been conditioned by other platforms to think in nightly terms.",
      "solution": "A/B test monthly-first vs. nightly-first price displays on listing pages and measure listing-to-proposal conversion.",
      "evidence": [
        {
          "source": "Walaa customer call.txt, 4:02",
          "type": "host_call",
          "quote": "It's between 600 to seven 50. This is my budget.",
          "insight": "Walaa's budget frame is monthly. The test will reveal whether monthly-first display actually improves conversion for guests who think in monthly terms."
        },
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 90",
          "type": "book",
          "quote": "People make most decisions unconsciously.",
          "insight": "The first number triggers the unconscious evaluation. If monthly is the right first number, conversion should improve because the unconscious comparison (monthly estimate vs. monthly budget) is in the same frame."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "Group A: listing pages show 'Est. $XXX/mo' as primary price with nightly rate below. Group B: listing pages show '$XX/night' as primary with monthly estimate below (current design). Track: (a) listing-to-proposal conversion rate, (b) time spent on price section of listing page, (c) price-related support tickets, (d) segment by guests who stated a monthly budget during intake vs. those who did not.",
      "success_criteria": "Group A (monthly-first) achieves 25%+ higher listing-to-proposal conversion for guests with stated monthly budgets. Group A also shows fewer price-related support tickets. For guests without stated budgets, Group A should perform at least as well as Group B.",
      "failure_meaning": "If monthly-first does not improve conversion, guests may already be performing the nightly-to-monthly conversion successfully, or the monthly total may trigger sticker shock (larger numbers feel more expensive). Would require price perception research to understand the mechanism.",
      "implementation_hint": "The monthly estimate must be personalized to the guest's schedule pattern when known. A generic monthly estimate (30 nights x nightly rate) will be inaccurate for guests like Walaa who stay 4-7 nights per week. Inaccurate estimates are worse than no estimate."
    },
    {
      "id": "tests-0901-006",
      "type": "validation_strategy",
      "title": "Post-Checkout Micro-Review Submission Rate and Timing Validation",
      "validates_element": "works-006",
      "journey_phases": ["active_lease"],
      "problem": "The 2-4 hour post-checkout micro-review prompt may arrive at inconvenient times (guest is traveling, sleeping, working). The one-tap emoji format may not capture enough useful signal to be worth the engineering investment. The variable reward (host reciprocal rating) may not actually sustain engagement.",
      "solution": "Track micro-review submission rates, timing patterns, review quality signals, and the impact of variable rewards on subsequent submission rates.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 24",
          "type": "book",
          "quote": "People reconstruct memories each time they remember them.",
          "insight": "The theory predicts fresher reviews are more accurate. Track the correlation between review timing (hours post-checkout) and review sentiment consistency across the lease."
        },
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 51",
          "type": "book",
          "quote": "Variable rewards are powerful.",
          "insight": "The 40% reciprocal reveal rate is theoretically optimal for intermittent reinforcement. Track whether guests who receive the variable reward submit reviews at higher rates in subsequent weeks."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Track for all active-lease guests: (a) micro-review submission rate per checkout (% of checkouts that generate a review within 12 hours), (b) time between predicted checkout and review submission, (c) distribution of emoji ratings across the 5-point scale, (d) optional text field usage rate, (e) submission rate in weeks following a variable reward reveal vs. weeks without, (f) end-of-lease summary confirmation rate. Run for 8+ weeks across 50+ active leases.",
      "success_criteria": "(a) 60%+ of checkout events generate a review within 12 hours by week 6 of each lease. (b) Median time-to-review under 4 hours. (c) Rating distribution is not all-5-stars (which would indicate low signal). (d) Optional text usage increases from <10% in weeks 3-4 to >20% by weeks 8+. (e) Submission rate is 15%+ higher in weeks following a variable reward. (f) End-of-lease summary confirmation rate above 80%.",
      "failure_meaning": "If submission rates stay below 40%, the notification timing may be wrong or the one-tap format may not feel worth the interruption. If all ratings are 5-stars, the emoji scale is not capturing real variation and may need recalibration (perhaps 3 options instead of 5). If variable rewards do not affect subsequent behavior, the reciprocal reveal may need to be more salient.",
      "implementation_hint": "Start with the graduated schedule: no reviews weeks 1-2, emoji-only weeks 3-4, full weeks 5+. If week 3-4 submission rates are low, consider making the first micro-review a prompted in-app experience (not push notification) during the first Stays Manager visit after checkout."
    },
    {
      "id": "tests-0901-007",
      "type": "validation_strategy",
      "title": "Alternating-Schedule Configuration Accuracy Test",
      "validates_element": "works-007",
      "journey_phases": ["proposal_creation"],
      "problem": "The two-step guided configuration (weekdays block + every-other-weekend question) may be intuitive for Walaa's specific pattern but may not generalize to other irregular schedules (e.g., 3 days on / 4 days off, rotating weekends, random patterns).",
      "solution": "Usability test the schedule configuration flow with 6-10 guests who have non-standard schedules, measuring first-attempt accuracy.",
      "evidence": [
        {
          "source": "Walaa customer call.txt, 2:43-3:17",
          "type": "host_call",
          "quote": "Monday to Friday, each week and then every other weekend... I traveled to another state every other weekend.",
          "insight": "Walaa's alternating pattern is the design target. The test must verify that this specific pattern is captured correctly AND that other irregular patterns are also handled."
        },
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 88",
          "type": "book",
          "quote": "People make predictable types of errors.",
          "insight": "The test should identify the specific error types that occur: over-selection (every weekend), under-selection (no weekends), wrong starting weekend, or inability to represent the pattern at all."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 6-10 guests with diverse non-standard schedules: (a) 2-3 with biweekly alternating patterns (like Walaa), (b) 2-3 with rotating shift patterns, (c) 2-3 with random/unpredictable schedules. Ask each to configure their schedule using the two-step flow. Measure: first-attempt accuracy (does the 4-week preview match their actual schedule?), time to complete, error types, and whether the 'every other weekend' question covers their specific pattern.",
      "success_criteria": "85%+ first-attempt accuracy for biweekly alternating patterns. 70%+ first-attempt accuracy for rotating patterns. For random/unpredictable schedules, the system should gracefully fall back to manual date selection without error. No usability test participant should feel 'stuck' (unable to represent their schedule at all).",
      "failure_meaning": "If alternating-pattern accuracy is below 85%, the two-step flow needs refinement (perhaps the 'starting this weekend or next?' question is confusing). If rotating patterns score below 70%, the flow may need a third pathway ('rotating shift') that the current design does not support.",
      "implementation_hint": "Include at least 2 ESL participants to test whether the weekday/weekend block labels are comprehensible without strong English. The visual 4-week preview strip (color-coded week types) should be tested as a standalone verification tool -- can guests recognize whether the preview matches their actual schedule?"
    },
    {
      "id": "tests-0901-008",
      "type": "validation_strategy",
      "title": "Criteria-Match Scan Pattern Effectiveness on Listing Pages",
      "validates_element": "communicates-001",
      "journey_phases": ["listing_evaluation"],
      "problem": "The four-criterion match summary (budget, location, roommate, schedule) at the top of listing pages may compete with photos for attention. Guests may skip the match summary entirely and scroll to photos, defeating the purpose of the criteria-first design.",
      "solution": "Eye-tracking or scroll-depth analysis on listing pages with the criteria-match summary at top vs. photos at top.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 40",
          "type": "book",
          "quote": "Attention is selective.",
          "insight": "Walaa will selectively attend to her criteria -- but only if the match summary is the first thing she sees. If photos capture attention first, criteria evaluation is deferred or skipped."
        },
        {
          "source": "Walaa customer call.txt, 4:02",
          "type": "host_call",
          "quote": "It's between 600 to seven 50. This is my budget.",
          "insight": "Budget is Walaa's primary filter. The test must verify that the budget match indicator is the first data point guests process on the listing page."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "Group A: listing page leads with the 4-criterion match summary above photos. Group B: listing page leads with photos, with criteria-match summary below. Track: (a) scroll depth before first interaction, (b) time spent on match summary section, (c) listing-to-proposal conversion, (d) time spent on listing page overall. If eye-tracking is available, track first fixation point and scan path.",
      "success_criteria": "Group A (criteria-first) shows: (a) higher time spent on the match summary section, (b) equal or higher listing-to-proposal conversion, (c) faster time-to-decision (less total time on page with equal or higher conversion means the match summary accelerated evaluation).",
      "failure_meaning": "If criteria-first has lower conversion, photos may be essential for establishing emotional connection before rational evaluation. The match summary would then need to be repositioned alongside (not above) photos, combining emotional and rational evaluation simultaneously.",
      "implementation_hint": "The match summary icons must be tested with the icon comprehension test (tests-0901-001) BEFORE this A/B test launches. If icons are ambiguous, the match summary will fail regardless of position."
    },
    {
      "id": "tests-0901-009",
      "type": "validation_strategy",
      "title": "Uncertainty-Tolerant Duration Display Comprehension Test",
      "validates_element": "communicates-004",
      "journey_phases": ["proposal_creation", "acceptance", "active_lease"],
      "problem": "Framing the commitment as 'renews monthly' instead of 'lease for X months' may confuse guests who expect a traditional lease model. The word 'stay' (replacing 'lease') may seem informal or unserious to some guests.",
      "solution": "Survey-based comprehension test comparing guest understanding of the flexible commitment framing across different copy treatments.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 82",
          "type": "book",
          "quote": "People overestimate reactions to future events.",
          "insight": "The 'renews monthly' framing is designed to reduce perceived commitment magnitude. The test measures whether guests actually perceive less commitment or are simply confused about what they are agreeing to."
        },
        {
          "source": "Walaa customer call.txt, 5:11-5:22",
          "type": "host_call",
          "quote": "I don't know. I don't know because no, I I'm not fixing yet in this worker, so I don't know how to be.",
          "insight": "Walaa represents the target user for this framing. The test should include both uncertain-duration and fixed-duration guests to see if the framing helps the uncertain without confusing the certain."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Present 10-12 guests (mix of uncertain and fixed duration preferences) with the acceptance screen using 'Your stay renews monthly' framing. After viewing, ask: (a) 'In your own words, what are you agreeing to?' (b) 'How long does this last?' (c) 'What happens if you want to leave?' (d) 'How does this compare to a typical lease?' Rate comprehension accuracy on each question. Also present alternative framings ('month-to-month agreement,' 'flexible lease') and compare comprehension.",
      "success_criteria": "80%+ of participants correctly understand: (a) the commitment is ongoing until they choose to end it, (b) they can leave with 2 weeks notice, (c) no long-term lock-in. The 'renews monthly' framing should score equal or higher comprehension than alternatives.",
      "failure_meaning": "If comprehension is below 80%, the 'stay' and 'renews' language may be too informal to convey the legal and financial reality of the commitment. Would need to find a middle ground between 'lease agreement' (too rigid) and 'stay' (too casual).",
      "implementation_hint": "Include ESL participants (at least 3 of 12) to test comprehension across language proficiency levels. The word 'renews' may be unfamiliar to non-native speakers. Consider testing with a visual metaphor (a loop/cycle icon) alongside the text."
    },
    {
      "id": "tests-0901-010",
      "type": "validation_strategy",
      "title": "Three-Zone Stays Manager Layout Consistency Impact on Visit Duration",
      "validates_element": "communicates-005",
      "journey_phases": ["active_lease"],
      "problem": "The fixed three-zone architecture (This Week / Progress / History) may feel restrictive as the guest's needs evolve across the 13+ week lease. Guests may want features (messaging, payments, documents) that are excluded from the three zones.",
      "solution": "Track time-per-visit trends and feature-request rates across 13-week leases to validate whether the fixed architecture sustains efficiency or creates frustration.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 42",
          "type": "book",
          "quote": "Well-practiced skills don't require conscious attention.",
          "insight": "Automaticity requires consistency. The test validates whether the three-zone layout actually produces automaticity (decreasing visit times) or whether guests develop workarounds (seeking features elsewhere, going off-platform)."
        },
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 6",
          "type": "book",
          "quote": "People scan screens based on past experience and expectations.",
          "insight": "Spatial habits form through repetition. If the three-zone layout works, guests should develop a consistent scan pattern visible in scroll behavior data."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "For all active-lease guests, track weekly: (a) time-per-Stays-Manager-visit, (b) scroll depth (do guests consistently stay in Zone 1, or do they explore Zone 3?), (c) off-platform actions (texts/calls to host about topics the Stays Manager should handle), (d) feature requests or support tickets related to missing Stays Manager functionality, (e) heatmap of tap targets across the three zones. Run for a minimum of 8 weeks across 30+ leases.",
      "success_criteria": "Time-per-visit decreases by 40%+ from week 1 to week 6, then stabilizes. 80%+ of visits involve only Zone 1 interaction by week 4+. Off-platform communication rate about payment, schedule, or reviews decreases 50%+ by week 8. Feature requests related to Stays Manager limitations are fewer than 5% of total support tickets.",
      "failure_meaning": "If time-per-visit does not decrease, the layout is not becoming automatic. If Zone 3 engagement increases over time (guests scrolling further), the fixed architecture may be failing to surface needed information in Zone 1. If off-platform communication increases, guests are working around the Stays Manager rather than adopting it.",
      "implementation_hint": "This test runs in parallel with tests-0901-002 (habit formation rate). Both use the same instrumentation but answer different questions: -002 asks 'do guests come back?' while -010 asks 'when they come back, is the experience getting more efficient?'"
    },
    {
      "id": "tests-0901-011",
      "type": "validation_strategy",
      "title": "Icon-Grid House Manual Findability and Comprehension Test",
      "validates_element": "communicates-006",
      "journey_phases": ["move_in", "active_lease"],
      "problem": "The 6-tile icon grid assumes that guests need exactly 6 categories of information. The chosen categories (Wi-Fi, Keys, Trash, Rules, Emergency, Neighborhood) may not match what guests actually need most during move-in and active lease.",
      "solution": "Track which House Manual tiles are tapped most frequently and whether guests contact hosts or support about topics that should be covered by the manual.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 22",
          "type": "book",
          "quote": "It's easier to recognize information than recall it.",
          "insight": "The icon grid is designed for recognition-based wayfinding. Analytics will show whether guests actually use the grid (recognition works) or bypass it and contact the host (the grid fails to help them recognize what they need)."
        },
        {
          "source": "Walaa customer call.txt, 0:41-0:53",
          "type": "host_call",
          "quote": "Um, no, you weren't asked how you should assist me or What was that?",
          "insight": "Walaa's language barrier predicts she will struggle with text-based manuals. Track whether ESL guests use the House Manual more or less than native speakers."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Track for all guests with access to a House Manual: (a) tile tap frequency by category (Wi-Fi, Keys, Trash, Rules, Emergency, Neighborhood), (b) time in each tile's detail view, (c) repeat visits to the same tile (indicating the information is needed frequently but not memorized), (d) host messages with questions about topics covered by the manual (indicating the manual failed to answer the question), (e) segment all metrics by guest language proficiency. Run for 4+ weeks across 20+ move-ins.",
      "success_criteria": "Wi-Fi tile is the most-tapped tile in week 1 (confirming it should remain on the grid surface). 80%+ of guests access the House Manual at least once during move-in week. Host messages about manual-covered topics decrease 50%+ compared to pre-manual baseline. ESL guests use the manual at least as frequently as native speakers.",
      "failure_meaning": "If a specific tile is never tapped, that category may not belong on the grid (replace with a more relevant category). If host messages about manual topics remain high, the manual's information quality or findability is insufficient. If ESL guests use the manual less than native speakers, the icons are not overcoming the language barrier as designed.",
      "implementation_hint": "Show the Wi-Fi password directly on the tile surface (not behind a tap) as specified in the communicates-006 element. Track whether this exception to the tap-to-reveal pattern reduces Wi-Fi-related host messages to near zero."
    },
    {
      "id": "tests-0901-012",
      "type": "validation_strategy",
      "title": "Agent Context Bar Warm Handoff Animation Perception Test",
      "validates_element": "behaves-001",
      "journey_phases": ["discovery", "listing_evaluation"],
      "problem": "The choreographed warm handoff sequence (context bar slides down in 0-300ms, then match criteria fade in at 300-600ms) may be too fast for guests to notice or too slow for impatient guests. The animation timing is based on theoretical principles, not empirical data.",
      "solution": "A/B test three animation speeds (fast, medium, slow) for the warm handoff and measure trust-related engagement signals.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 79",
          "type": "book",
          "quote": "People use look and feel as their first indicator of trust.",
          "insight": "The warm handoff animation IS the look and feel. If the animation is too fast, it may not register. If too slow, it delays the trust evaluation. The optimal timing is empirical, not theoretical."
        },
        {
          "source": "Walaa customer call.txt, 0:03-0:16",
          "type": "host_call",
          "quote": "Hello is this, um, is it <inaudible> Hi, this is it. And I'm talking to you or am I talking to, uh, her, her friend.",
          "insight": "Walaa needs immediate clarity about who she is interacting with. The animation must resolve the 'who is behind this page?' question before confusion sets in."
        }
      ],
      "priority": "low",
      "validation_method": "a_b_test",
      "test_description": "Three groups of agent-referred listing page visits: Group A (fast: context bar instant, criteria at 150ms), Group B (medium: context bar at 150ms, criteria at 400ms -- current design), Group C (slow: context bar at 300ms, criteria at 700ms). Track bounce rate, time-to-first-interaction, and scroll depth. Sample size: 100+ per group.",
      "success_criteria": "Identify the speed that produces the lowest bounce rate and highest time-to-first-interaction. The winner should show statistically significant improvement over the other two (p < 0.05).",
      "failure_meaning": "If no speed produces significantly different results, the animation timing is not the important variable -- the agent's PRESENCE matters more than the animation's SPEED. This would simplify implementation (skip the choreography, just show the bar).",
      "implementation_hint": "This is a lower-priority test that should run only after the agent context bar itself is validated (tests-0901-004). Optimizing animation timing before validating the bar's existence would be premature."
    },
    {
      "id": "tests-0901-013",
      "type": "validation_strategy",
      "title": "Flexible-Duration Proposal Host Response Rate Monitoring",
      "validates_element": "behaves-002",
      "journey_phases": ["negotiation"],
      "problem": "While flexible-duration proposals serve guest needs, hosts may systematically reject or counter them, preferring fixed-duration commitments for predictability. If hosts reject Flexible proposals at high rates, the guest experience becomes worse, not better.",
      "solution": "Monitor host response rates to Flexible-duration proposals vs. fixed-duration proposals, segmented by host type.",
      "evidence": [
        {
          "source": "Walaa customer call.txt, 5:11-5:22",
          "type": "host_call",
          "quote": "I don't know. I don't know because no, I I'm not fixing yet in this worker, so I don't know how to be.",
          "insight": "Walaa needs the Flexible option. But if hosts reject it, Walaa faces the counter-offer screen that the system was designed to avoid. Monitoring host behavior validates whether the guest-side solution creates marketplace-level problems."
        },
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 93",
          "type": "book",
          "quote": "People think choice equals control.",
          "insight": "The Flexible option gives guests perceived control. But hosts also want control. If both sides feel their control is threatened, the negotiation becomes adversarial rather than cooperative."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "For all proposals submitted: (a) acceptance rate segmented by duration type (Flexible vs. 1-month vs. 2-month vs. 3-month), (b) counter-offer rate by duration type, (c) average time-to-host-response by duration type, (d) final agreed duration for proposals that started as Flexible, (e) tenant satisfaction score for leases that started as Flexible vs. fixed. Track for 8+ weeks across 100+ proposals.",
      "success_criteria": "Flexible-duration proposals have an acceptance rate within 15% of the average fixed-duration acceptance rate. If hosts counter-offer Flexible proposals, 70%+ of counter-offers are accepted by guests (indicating the negotiation is productive, not adversarial). Flexible-start leases show equal or higher satisfaction scores.",
      "failure_meaning": "If Flexible acceptance rate is 30%+ lower than fixed, hosts are systematically penalizing flexibility. This would require host-side education about Flexible benefits or a minimum-commitment framing ('at least 4 weeks, then month-to-month') to give hosts the predictability they need.",
      "implementation_hint": "Segment by host type: first-time hosts may reject Flexible more often due to uncertainty, while experienced hosts may accept it readily. If the rejection is concentrated among new hosts, targeted education (not redesign) may be the solution."
    },
    {
      "id": "tests-0901-014",
      "type": "validation_strategy",
      "title": "Date Change Tool Autonomy Perception and Usage Frequency",
      "validates_element": "behaves-006",
      "journey_phases": ["active_lease"],
      "problem": "The Date Change Tool is designed to feel like 'adjusting your schedule' rather than 'requesting permission.' But if hosts frequently decline changes or the optimistic UI reverts, the autonomy perception degrades rapidly. One reverted change can destroy weeks of trust.",
      "solution": "Track Date Change Tool usage patterns, host decline rates, and the emotional impact of declined changes on subsequent guest behavior.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 62",
          "type": "book",
          "quote": "People are motivated by autonomy.",
          "insight": "Autonomy requires that actions succeed. If the Date Change Tool feels autonomous but changes are frequently reversed, the tool becomes a source of frustration rather than empowerment."
        },
        {
          "source": "Walaa customer call.txt, 5:11-5:22",
          "type": "host_call",
          "quote": "I don't know. I don't know because no, I I'm not fixing yet in this worker, so I don't know how to be.",
          "insight": "Walaa's unstable work situation predicts high date-change frequency. The tool must handle frequent modifications without degrading the experience."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Track for all active-lease guests: (a) Date Change Tool usage frequency (changes per week), (b) host decline rate for requested changes, (c) guest behavior after a declined change (do they go off-platform? do they stop using the tool? do they retry?), (d) correlation between Date Change Tool usage and Stays Manager engagement (does using the tool increase or decrease Stays Manager visits?), (e) guest satisfaction scores segmented by Date Change Tool usage frequency.",
      "success_criteria": "Host decline rate below 10% (most changes are automatically acceptable). When declines do occur, 80%+ of guests use the in-app chat to resolve (rather than going off-platform). Date Change Tool users show equal or higher Stays Manager engagement than non-users. Guest satisfaction is positively correlated with Date Change Tool availability.",
      "failure_meaning": "If host decline rate exceeds 20%, the optimistic UI creates a promise-then-revoke pattern that damages trust. Would need to implement pre-validation (check host availability before showing the optimistic update) at the cost of slower response times. If guests abandon the tool after one decline, the error recovery messaging needs redesign.",
      "implementation_hint": "Track the 'just this week vs. every week going forward' selection distribution. If most changes are one-off, the pattern modification feature may be unnecessary complexity. If most changes are permanent pattern updates, the tool should nudge toward pattern modification rather than week-by-week changes."
    },
    {
      "id": "tests-0901-015",
      "type": "validation_strategy",
      "title": "Emotional Arc Journey-Level Validation: Discovery-to-Active-Lease Sentiment Tracking",
      "validates_element": "feels-001 through feels-006",
      "journey_phases": ["discovery", "evaluation", "application", "onboarding", "move_in", "active_lease"],
      "problem": "The emotional elements (feels-001 through feels-006) prescribe specific target emotions at each phase (safety, competence, autonomy, belonging, stability, relief). Whether guests actually experience these emotions in the intended sequence is unknown. The entire emotional architecture could be theoretically coherent but experientially wrong.",
      "solution": "Implement lightweight emotional check-ins at key phase transitions to track the guest's actual emotional state against the designed emotional arc.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 83",
          "type": "book",
          "quote": "People feel more positive before and after an event than during it.",
          "insight": "Emotional states fluctuate predictably. The check-ins must capture the DURING-event emotions (the moments of peak tension), not just the before-and-after states that retrospective surveys capture."
        },
        {
          "source": "Walaa customer call.txt, 0:41-0:53 through 5:11-5:22",
          "type": "host_call",
          "quote": "Various quotes showing emotional state evolution through the call",
          "insight": "Walaa's emotional journey through a single phone call shows measurable state changes. The platform journey spans weeks, giving many opportunities for emotional check-ins that do not disrupt the experience."
        }
      ],
      "priority": "medium",
      "validation_method": "manual_review",
      "test_description": "At 6 key transition points (post-first-listing-view, post-proposal-submission, post-acceptance, post-move-in-day-1, post-week-4-stays-manager, post-week-8-stays-manager), present a single-screen emotional check-in: 'How are you feeling right now?' with 5 emoji faces (same as micro-review format for consistency). Optional one-line text field. Track: (a) which emotion is selected at each transition, (b) emotional trajectory across the 6 checkpoints, (c) correlation between emotional state and subsequent phase completion/dropout.",
      "success_criteria": "The emotional trajectory matches the designed arc: (a) safety/neutral at discovery, (b) competence/positive at evaluation completion, (c) autonomy/positive at proposal submission, (d) relief/very-positive at acceptance, (e) belonging/positive at week 1, (f) stability/positive at weeks 4 and 8. Guests who report negative emotions at any checkpoint should show higher dropout rates in the next phase (validating that emotions predict behavior).",
      "failure_meaning": "If the emotional trajectory does not follow the designed arc, specific phases are creating unintended emotions. For example, if guests report anxiety at acceptance (instead of relief), the post-acceptance decompression (feels-006) is failing. Each misaligned checkpoint points to a specific emotional element that needs redesign.",
      "implementation_hint": "Use the same emoji-face format as the micro-review system so guests are familiar with the interaction pattern by the time they reach the active-lease checkpoints. Keep the check-in to ONE tap -- any additional questions will suppress participation. Ensure the check-in does not appear during the critical first Stays Manager visits (weeks 1-3) to avoid disrupting habit formation."
    },
    {
      "id": "tests-0901-016",
      "type": "validation_strategy",
      "title": "Biweekly Week-Type Color Differentiation Recognition Speed Test",
      "validates_element": "looks-004",
      "journey_phases": ["active_lease"],
      "problem": "The dual-color week type system (light purple for short weeks, deeper purple for full weeks) may not create sufficient visual contrast for instant recognition. Color-blind guests may not perceive the differentiation at all.",
      "solution": "Measure recognition speed for week types using the color-coded system, including with color vision deficiency simulation.",
      "evidence": [
        {
          "source": "100things-attention-memory-decisions.txt, Ch. 3",
          "type": "book",
          "quote": "People identify objects by recognizing patterns.",
          "insight": "The alternating color pattern should be recognizable within 2-3 exposures. The test validates whether the specific purple-shade differentiation (accent-purple vs. secondary-purple) creates sufficient visual contrast for pattern recognition."
        },
        {
          "source": "Walaa customer call.txt, 2:43-3:17",
          "type": "host_call",
          "quote": "Monday to Friday, each week and then every other weekend.",
          "insight": "Walaa must distinguish between 4-night and 7-night weeks at a glance. If color coding fails, she must read dates -- which defeats the recognition-over-recall principle."
        }
      ],
      "priority": "low",
      "validation_method": "usability_test",
      "test_description": "Show 8-10 participants a simulated Stays Manager with the 4-week lookahead strip (alternating light/dark purple week cards). Ask: (a) 'Which week type is this week?' (timed response). (b) 'Point to the next full week.' (timed response). (c) Present the same display through deuteranopia and protanopia simulation and repeat. Measure response time and accuracy for each condition.",
      "success_criteria": "Normal vision: 90%+ accuracy with under 2-second response time by the third exposure. Color vision deficiency simulation: 70%+ accuracy (the half-circle vs. full-circle icon provides a non-color differentiation that should work regardless of color perception).",
      "failure_meaning": "If normal-vision accuracy is below 90%, the purple-shade differentiation is insufficient. Would need to increase contrast (e.g., light purple vs. dark purple) or add a non-color differentiator (pattern, size, shape). If CVD accuracy is below 70%, the icon differentiation (half-circle vs. full-circle) is not prominent enough.",
      "implementation_hint": "This test should be run as a quick guerrilla test with internal team members before full user testing. If the differentiation fails internally, do not waste user recruitment on an obviously broken design."
    }
  ]
}