{
  "lens": {
    "host_call": "jessica-filomeno-call.txt",
    "book_extract": "cialdini-authority-liking-scarcity.txt"
  },
  "elements": [
    {
      "id": "tests-001",
      "type": "validation_strategy",
      "title": "Authority Credential Verification Effectiveness Test",
      "validates_element": "works-001",
      "journey_phases": ["discovery", "evaluation"],
      "problem": "If authority credentials (licenses, partner logos, property count) aren't visible or credible during discovery, professional hosts arrive at evaluation calls in defensive mode, wasting cognitive resources on legitimacy verification rather than economic evaluation. Jessica spent the first 3 minutes interrogating Split Lease's business model because Robert's opening provided no authority substance. Without validation, we can't know if front-loading credentials reduces this defensive behavior.",
      "solution": "A/B test broker partnership page with two variants. Variant A (control): Generic company description, no specific credentials above fold. Variant B (treatment): Authority badge hierarchy with licensed platform badge, '150+ properties | 200+ broker partners' metric, and recognizable partner logo above fold. Measure: (1) Time spent on page before scheduling call (hypothesis: treatment reduces research time because credentials answer questions faster). (2) First question asked on evaluation call—coded as 'legitimacy-focused' (Do you work with management companies?) vs. 'business-focused' (What's the commission structure?). Hypothesis: treatment increases business-focused first questions from baseline 30% to 70%. (3) Call-to-schedule conversion rate. Hypothesis: treatment increases conversion 15-25% by reducing authority anxiety.",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 01:16-01:51",
          "type": "host_call",
          "quote": "Jessica: 'I did look up split lease yesterday.' Then immediately: 'Do you work with management companies or is it just individual landlords?'",
          "insight": "Jessica researched Split Lease independently but still opened defensively. Her first substantive question probes legitimacy, not economics. This signals that discovery touchpoint lacked sufficient authority credentials. Test validates whether better credentials reduce this defensive opening."
        },
        {
          "source": "cialdini-authority-liking-scarcity.txt, line 334-336",
          "type": "book",
          "quote": "Titles are simultaneously the most difficult and the easiest symbols of authority to acquire. To earn one normally takes years of work and achievement. Yet it is possible for somebody who has put in none of this effort to adopt the mere label and receive a kind of automatic deference.",
          "insight": "Cialdini: authority symbols trigger automatic deference. Test validates whether visual authority symbols (badges, metrics, logos) reduce time-to-trust and enable faster progression to economic evaluation."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "50 professional hosts (brokers, property managers) per variant. Track from discovery page view through evaluation call. Code first question on call as legitimacy-focused vs. business-focused using rubric: legitimacy = questions about who Split Lease is, who they work with, their track record; business = questions about commission, pricing, conversion timelines. Measure call duration spent on legitimacy discussion (hypothesis: treatment reduces from 3+ minutes to <1 minute).",
      "success_criteria": "Treatment variant achieves: (1) 70%+ of hosts open calls with business-focused questions vs. 30% baseline. (2) <1 minute spent on legitimacy discussion vs. 3+ minutes baseline. (3) 15-25% increase in call-to-schedule conversion rate. Qualitative: Host feedback indicates 'I felt confident in Split Lease's legitimacy before the call.'",
      "failure_meaning": "If treatment shows no improvement, authority credential visibility alone doesn't reduce professional host skepticism. Alternative explanations: (1) Credentials shown aren't the right ones (brokers care about different authority signals than shown). (2) Visual hierarchy is wrong (credentials are present but not salient enough). (3) Professional hosts are inherently skeptical regardless of credentials (requires different trust-building approach like referrals or case studies).",
      "implementation_hint": "Use session recording to observe which credentials hosts view and for how long. Eye-tracking or heatmap data would reveal whether authority badges draw attention. Post-call survey: 'What information on the partnership page helped you decide to schedule this call?' If hosts don't mention credentials, visual hierarchy needs optimization."
    },
    {
      "id": "tests-002",
      "type": "validation_strategy",
      "title": "Economic Transparency Calculator Confidence Impact Test",
      "validates_element": "works-002",
      "journey_phases": ["evaluation", "pricing"],
      "problem": "Jessica spent 6+ minutes negotiating pricing because she couldn't model her ROI upfront. Without transparent economic benchmarks (typical markup percentages, conversion timelines, comparative ROI vs. traditional broker fees), hosts experience pricing anxiety: 'Am I leaving money on the table or pricing myself out of conversions?' This uncertainty delays commitment or causes dropout. Without validation, we don't know if interactive ROI calculator actually reduces economic anxiety and accelerates commitment.",
      "solution": "Controlled experiment with two onboarding cohorts. Cohort A (control): Verbal pricing explanation during call, no calculator provided. Cohort B (treatment): Access to interactive ROI calculator during evaluation phase (before call or early in call). Calculator pre-populated with host's market data (Manhattan studios show Manhattan-specific benchmarks). Measure: (1) Time spent discussing pricing on evaluation call (hypothesis: treatment reduces from 6+ minutes to <3 minutes because calculator externalizes mental math). (2) Number of pricing-related clarification questions (hypothesis: treatment reduces by 50%+). (3) Time from evaluation call to first listing creation (hypothesis: treatment accelerates by 30-50% because economic clarity removes hesitation). (4) Post-call confidence survey: 'How confident do you feel about the economics of this partnership?' 1-7 scale. Hypothesis: treatment averages 6+ vs. control 4-5.",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 06:29-12:52",
          "type": "host_call",
          "quote": "Jessica spent 6+ minutes negotiating pricing, challenging Robert's expectations, asking for benchmarks, and seeking clarification on markup model multiple times.",
          "insight": "Pricing discussion consumed 30%+ of 19-minute call. Jessica wanted data ('What's the price range?') but received autonomy ('Whatever you feel reasonable'). Test validates whether calculator provides the data hosts need, reducing negotiation time and anxiety."
        },
        {
          "source": "cialdini-authority-liking-scarcity.txt, line 318",
          "type": "book",
          "quote": "Because their positions speak of superior access to information and power, it makes great sense to comply with the wishes of properly constituted authorities.",
          "insight": "Calculator positions Split Lease as pricing authority through proprietary conversion data. Test validates whether this authority positioning increases host confidence in pricing decisions, enabling faster commitment."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "60 professional hosts (30 per cohort) matched on market, inventory type, and experience level. Control receives standard evaluation call with verbal pricing explanation. Treatment receives calculator access link 24 hours before call with prompt: 'Model your potential earnings before our call.' Track calculator usage (% who use it, time spent, inputs adjusted). Code evaluation calls for: (1) Duration of pricing discussion. (2) Count of pricing clarification questions. (3) Host confidence signals (verbal cues like 'That makes sense' vs. 'I'm not sure'). Post-call survey: 'How confident are you in your pricing strategy?' and 'How clear are the economics of this partnership?'",
      "success_criteria": "Treatment cohort shows: (1) <3 minutes spent on pricing discussion vs. 6+ minutes control. (2) 50%+ reduction in pricing clarification questions. (3) 30-50% faster time from call to first listing. (4) Confidence rating 6+ out of 7 vs. 4-5 control. Qualitative: Hosts say 'The calculator helped me understand ROI' or 'I could see the economics clearly before the call.'",
      "failure_meaning": "If treatment shows no improvement, interactive calculator doesn't reduce economic anxiety. Possible reasons: (1) Calculator is too complex or not intuitive (hosts don't use it or use it incorrectly). (2) Benchmarks shown aren't trusted (hosts skeptical of Split Lease's conversion data). (3) Hosts prefer autonomy over guidance (don't want to be told what to price). (4) Economic anxiety isn't about lack of data—it's about risk aversion that data can't resolve. Alternative: Focus on risk mitigation (guarantees, insurance) rather than transparency.",
      "implementation_hint": "Track calculator engagement metrics: % of hosts who open calculator, % who adjust inputs, average time spent, correlation between calculator use and subsequent listing creation. If hosts open calculator but don't interact, UX is the problem. If hosts don't open calculator at all, positioning/access is the problem (send earlier, explain value, make it more prominent)."
    },
    {
      "id": "tests-003",
      "type": "validation_strategy",
      "title": "Honest Constraint Feedback Trust-Building Validation",
      "validates_element": "works-003",
      "journey_phases": ["evaluation", "listing_creation"],
      "problem": "Robert built trust with Jessica by arguing against Split Lease's short-term interests: 'Pricing things out of the market would be no interest to anybody. Start with lower-cost inventory.' This honest constraint advice built relationship capital, but it's agent-dependent and doesn't scale. Without validation, we don't know if systematized constraint feedback (real-time pricing guardrails, proactive adjustment recommendations) builds the same trust or feels patronizing/intrusive.",
      "solution": "Usability test pricing slider with real-time constraint feedback (color-coded zones: green/yellow/red) with 15 hosts (mix of brokers and individual landlords). Task: 'Create your first listing and set pricing for a Manhattan studio.' Observe: (1) Do hosts notice zone colors as they drag slider? (2) When guidance card appears ('This price is 20% above comparable properties'), do hosts read it? (3) Do hosts adjust pricing in response to constraint feedback? (4) Measure emotional response through think-aloud protocol: Do hosts say 'This is helpful' or 'This is annoying'? (5) Post-task survey: 'Did the pricing guidance feel like expert coaching or system criticism?' Hypothesis: 70%+ perceive it as coaching. Net Promoter Score: 'How likely are you to recommend Split Lease based on this pricing experience?' Hypothesis: 8+ out of 10.",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 18:28-18:54",
          "type": "host_call",
          "quote": "Robert: 'Please start with rooms or studios or one bedrooms. Try to go on the lower cost... pricing things out of the market would be no interest to anybody.'",
          "insight": "Robert's honest constraint advice built trust—Jessica's tone shifted from skeptical to warm after this exchange. Test validates whether visual systematization (slider zones, guidance cards) preserves this trust-building effect or loses the human warmth that made it work."
        },
        {
          "source": "cialdini-authority-liking-scarcity.txt, Vincent the waiter example",
          "type": "book",
          "quote": "Vincent built trust by recommending cheaper dishes, arguing against the restaurant's short-term interests but maximizing customer satisfaction.",
          "insight": "Cialdini: 'seeming to argue against own interests' is a powerful liking trigger. Test validates whether constraint feedback ('We recommend lower price even though it means less revenue for us') triggers the same liking effect when systematized through UI."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "15 hosts (8 brokers, 7 individual landlords) in moderated usability sessions. Task-based protocol: 'Create a listing for a Manhattan studio. Set pricing that you think will convert quickly.' Observe interaction with pricing slider and constraint feedback. Use think-aloud: 'Tell me what you're thinking as you set the price.' Code verbal responses as positive ('This is helpful,' 'Good to know'), neutral ('OK, I see that'), or negative ('This is annoying,' 'I know what I'm doing'). Post-task interview: 'How did the pricing guidance make you feel? Supported or second-guessed?' 'Would you adjust your price based on this guidance?' Compare broker vs. landlord responses to identify segment differences.",
      "success_criteria": "70%+ of hosts perceive guidance as 'expert coaching' not 'system criticism.' 60%+ adjust pricing in response to yellow/red zone warnings. Average NPS 8+ out of 10. Qualitative: Hosts say 'I appreciate the honest feedback' or 'This prevents me from pricing too high.' No hosts say 'This feels patronizing' or 'The system is telling me I'm wrong.'",
      "failure_meaning": "If hosts perceive guidance as patronizing, framing/tone is wrong. Possible fixes: (1) Soften language from 'This price rarely converts' to 'Based on market data, reducing price by 15% typically improves conversion speed 3x.' (2) Add dismissibility: 'I understand my market, don't show this again' option for experts like brokers. (3) Delay guidance until host explicitly requests it ('Need pricing help?'). If hosts ignore guidance entirely, salience is the problem—guidance isn't visually prominent enough or doesn't appear at the right moment. If failure is broker-specific but landlords respond well, segment the pattern: verbose guidance for landlords, minimal guidance for brokers.",
      "implementation_hint": "Session recordings with hotjar or fullstory capture: Do hosts pause when guidance card appears? Do they read it (cursor hovers, scrolls) or dismiss immediately? Track: % who see guidance (slider enters yellow/red zone) vs. % who adjust price after seeing it. Correlation = guidance effectiveness. If hosts see it but don't adjust, either they don't trust the data or they're intentionally overriding (which is acceptable—we want informed decisions, not forced compliance)."
    },
    {
      "id": "tests-004",
      "type": "validation_strategy",
      "title": "Scarcity Proof Progression Skepticism-to-Enthusiasm Validation",
      "validates_element": "works-004",
      "journey_phases": ["evaluation", "proposal_mgmt"],
      "problem": "Robert claimed 'We have too much demand and not enough inventory,' but Jessica responded skeptically, committing only to send 'a couple' of test listings. Cialdini explains that 'newly experienced scarcity' (seeing proposals arrive for your listings) is more persuasive than 'described scarcity' (being told demand exists). Without validation, we don't know if progressive demand proof (waitlist snapshot within 24 hours, engagement metrics within 48 hours, proposal within 7 days) actually converts skeptical testing into enthusiastic commitment.",
      "solution": "Longitudinal cohort study tracking 30 professional hosts who start with test listings (1-3 listings in first week). Track progression: Day 1: Waitlist snapshot sent ('Here are 5 active guest searches matching your inventory'). Measure: Open rate, click-through rate, reply rate ('This is encouraging' vs. no response). Days 2-3: Listing goes live, engagement metrics email sent ('Your listing viewed by 18 guests, saved by 3'). Measure: Dashboard login frequency (hypothesis: hosts check dashboard more frequently after seeing metrics). Days 4-7: First proposal arrives. Measure: Time to proposal, host response time to proposal, acceptance rate. Day 8+: Track listing expansion behavior—do hosts send more listings after validating demand? Measure: % who send 4+ more listings within 14 days of first proposal. Hypothesis: 60%+. Interview sample of 10 hosts post-first-proposal: 'What convinced you to send more listings?' Hypothesis: Majority cite proof of demand (proposals, engagement metrics) not verbal claims.",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 14:41-14:49",
          "type": "host_call",
          "quote": "Jessica: 'I'll see if I can get a couple [of listings].'",
          "insight": "Jessica commits cautiously ('a couple,' not 'my full portfolio'), testing whether demand claims are real. Test validates whether rapid proof (within 7 days) converts skeptical testing into scaled commitment or whether test hosts remain cautious regardless."
        },
        {
          "source": "cialdini-authority-liking-scarcity.txt, line 520",
          "type": "book",
          "quote": "The idea that newly experienced scarcity is the more powerful kind applies to situations well beyond the bounds of the cookie study.",
          "insight": "Cialdini: experiencing scarcity directly is more persuasive than being told about it. Test validates timing: Does proof within 7 days (rapid newly-experienced scarcity) convert skepticism faster than proof after 14+ days (slower validation)?"
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "30 professional hosts tracked from evaluation call through first 30 days. Segment into three groups based on time-to-first-proposal: Fast (0-7 days), Medium (8-14 days), Slow (15-30 days). Measure listing expansion behavior by segment: Do Fast hosts send more listings than Medium/Slow? Track engagement with proof touchpoints: waitlist snapshot open rate, engagement metrics dashboard views, proposal response time. Correlate proof timing with expansion: Does earlier proof lead to faster expansion? Interview 10 hosts (3 Fast, 4 Medium, 3 Slow): 'What made you decide to send more listings (or not)?' Code responses: demand proof, economics, ease of process, other.",
      "success_criteria": "Fast cohort (proposal within 7 days) shows: (1) 60%+ send 4+ more listings within 14 days of first proposal vs. 30% Medium/Slow. (2) Average 2x listing count after 30 days vs. baseline at day 7. (3) Interview quotes: 'The proposals came faster than expected' or 'Seeing the engagement metrics convinced me demand is real.' Proof touchpoints engagement: 70%+ open waitlist snapshot, 80%+ view engagement metrics in dashboard within 48 hours of email.",
      "failure_meaning": "If Fast cohort doesn't expand listings significantly more than Medium/Slow, early proof doesn't drive expansion. Alternative explanations: (1) Hosts are inventory-constrained—they want to send more but don't have suitable properties. (2) First proposal economics were disappointing (low markup accepted, guest was difficult). (3) Hosts are inherently cautious regardless of proof (professional skepticism). (4) Proof timing is less important than proof quality—one strong proposal beats three weak ones. If failure is inventory-constraint, address through sourcing support: 'Need help finding more properties? Here's how to identify suitable inventory.' If failure is economics, address pricing guidance during first listing creation.",
      "implementation_hint": "Dashboard analytics: Track daily logins by cohort—do Fast hosts log in more frequently after first proposal (checking for more proposals)? Track listing count over time: Does expansion happen gradually (1-2 per week) or in bursts (5+ after first proposal)? Burst pattern suggests proof-driven enthusiasm. Gradual suggests operational constraints. Survey non-expanders at day 30: 'You received a proposal but didn't send more listings. Why?' Options: Don't have more inventory, Waiting to see how first lease goes, Too busy, Economics weren't attractive, Other. Identify primary dropout reason."
    },
    {
      "id": "tests-005",
      "type": "validation_strategy",
      "title": "Communication Channel Preference Routing Effectiveness Test",
      "validates_element": "works-005",
      "journey_phases": ["onboarding", "proposal_mgmt", "active_lease"],
      "problem": "Jessica explicitly stated 'Text is preferred because you can imagine how many emails I get.' If Split Lease sends time-sensitive notifications (proposals, guest questions) via email when host prefers text, host may miss them, causing delayed responses or lost conversions. Without validation, we don't know if honoring channel preferences actually improves response rates and reduces missed opportunities.",
      "solution": "Before/after analysis with two measurement periods. Period 1 (baseline): All time-sensitive notifications sent via email regardless of preference. Measure: Average proposal response time, % of proposals that receive no response within 48 hours (missed opportunities), host complaints about missed notifications. Period 2 (treatment): Capture channel preference during onboarding, route time-sensitive notifications via preferred channel. Measure same metrics. Compare: Does treatment reduce response time and missed opportunities? Segment analysis: Compare text-preference hosts vs. email-preference hosts. Hypothesis: Text-preference hosts respond 50%+ faster when notifications route via text. Survey hosts post-treatment: 'Do you feel Split Lease communicates effectively with you?' Yes/No. Hypothesis: 85%+ Yes vs. 60% baseline.",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 18:07-18:14",
          "type": "host_call",
          "quote": "Jessica: 'Text is preferred because you can imagine how many emails I get.'",
          "insight": "Jessica's explicit preference with rationale (email overload) suggests routing matters for busy professionals. Test validates whether honoring this preference reduces missed notifications and improves responsiveness."
        },
        {
          "source": "cialdini-authority-liking-scarcity.txt, similarity principle",
          "type": "book",
          "quote": "We like people who are similar to us. Consequently, those who wish to be liked can accomplish that purpose by appearing similar to us in any of a wide variety of ways.",
          "insight": "Honoring communication preference is a form of similarity/liking building. Test validates whether this adaptation improves not just response metrics but also host satisfaction and relationship quality."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Before/after analysis with 100 hosts per period (50 text-preference, 50 email-preference identified post-hoc through engagement patterns in baseline period). Baseline period: 30 days, all notifications via email. Treatment period: 30 days, notifications routed by captured preference. Track: (1) Proposal notification to host response time (median, 75th percentile). (2) % of proposals with no response within 48 hours. (3) % of guest questions with no response within 24 hours. (4) Host support tickets related to missed notifications. Compare by segment: text-preference hosts in treatment vs. baseline should show largest improvement. Survey 40 hosts post-treatment (20 per channel): 'How satisfied are you with Split Lease's communication?' 1-7 scale.",
      "success_criteria": "Treatment period shows: (1) 30-50% reduction in median proposal response time. (2) 40%+ reduction in proposals with no response within 48 hours. (3) 50%+ reduction in communication-related support tickets. (4) Text-preference hosts show 2x larger improvement than email-preference hosts (validates that routing matters, not just general improvement). (5) Communication satisfaction rating averages 6+ out of 7 vs. 4-5 baseline.",
      "failure_meaning": "If treatment shows no improvement, channel routing isn't the bottleneck. Alternative explanations: (1) Hosts see notifications but choose not to respond (disinterest in proposals, not notification delivery). (2) Notification content is unclear, regardless of channel (hosts don't understand what action is required). (3) Hosts prefer to batch review proposals on their schedule, not respond immediately (notification urgency is misaligned with host workflow). If failure, investigate: Are text-preference hosts checking dashboard directly without waiting for notifications? If yes, notification channel is irrelevant—focus on making dashboard more engaging. Are hosts reading notifications but not clicking through? If yes, notification content/CTA is the problem, not routing.",
      "implementation_hint": "Track notification delivery success rate by channel: % of text notifications delivered vs. % of emails delivered and opened. If text delivery rate is <95%, carrier blocking is a problem. Track click-through rate from notification to dashboard by channel: Does text achieve higher CTR than email? If yes, validates channel matters. If no, suggests content is more important than channel. Survey hosts who switched preferences mid-journey: 'Why did you change?' Insights on what drives preference changes."
    },
    {
      "id": "tests-006",
      "type": "validation_strategy",
      "title": "Authority Credential Visual Hierarchy Eye-Tracking Validation",
      "validates_element": "looks-001",
      "journey_phases": ["discovery", "evaluation"],
      "problem": "Authority badge hierarchy is designed to communicate credibility in first 3 seconds through visual hierarchy: licensed platform badge (serif typography, accent color), quantified metrics ('150+ properties'), social proof (partner logos). Without validation, we don't know if hosts actually see and process these credentials, or if visual hierarchy is wrong (elements are present but not salient).",
      "solution": "Eye-tracking study with 20 professional hosts viewing broker partnership page. Task: 'You're evaluating Split Lease as a potential partner. Review this page and decide if you'd schedule a call.' Track: (1) First fixation—where do eyes land first? Hypothesis: Authority badge (top-center) captures first attention. (2) Total fixation time on credentials zone (top 300px) vs. business model zone (mid-page) vs. CTA zone (bottom). Hypothesis: 40%+ of time on credentials. (3) Scan path—do eyes move from badge → metrics → social proof (intended hierarchy) or skip around? (4) Do hosts click partner logos or testimonials (engagement signal)? Post-task interview: 'What information helped you decide whether to schedule a call?' If hosts don't mention credentials, visual hierarchy failed.",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 01:16-01:51",
          "type": "host_call",
          "quote": "Jessica: 'I did look up split lease yesterday.' Then: 'Do you work with management companies?'",
          "insight": "Jessica researched Split Lease website but still opened defensively, suggesting discovery page didn't effectively communicate credentials. Test validates whether visual hierarchy (making credentials more prominent) solves this or if different credentials are needed."
        },
        {
          "source": "looks-elements.json, looks-001",
          "type": "layer_3",
          "quote": "Authority symbols are dominant visual weight in discovery/evaluation phases. Use 3-tier typographic scale: Tier 1 (authority badges): Instrument Serif 24px bold, accent-bright.",
          "insight": "Visual design hypothesis: Large typography + high-contrast color makes credentials salient. Test validates whether this actually captures attention or if hosts scan past to business model content."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "20 professional hosts in lab with eye-tracking equipment (Tobii or similar). Present broker partnership page for 60 seconds. Track gaze patterns: heatmaps, fixation sequences, time-to-first-fixation on authority zone. Task: 'Review this page and think aloud about your impressions.' After 60 seconds, hide page and ask: 'What do you remember about Split Lease?' Code recall: Do they remember specific credentials (license, property count, partner names)? Compare two page variants (A/B within-subjects): Variant A = current design, Variant B = credentials moved to right sidebar instead of hero section. Does placement affect fixation patterns? Post-task survey: 'What information was most important in evaluating Split Lease's credibility?'",
      "success_criteria": "70%+ of hosts fixate on authority badge within first 3 seconds. Average 40%+ of viewing time spent on credentials zone. 80%+ follow intended scan path: badge → metrics → social proof → business model. 60%+ recall at least one specific credential (license, property count, partner name) when page is hidden. Post-task: 70%+ say credentials (licenses, partners, metrics) were 'very important' in assessing credibility.",
      "failure_meaning": "If hosts don't fixate on credentials or can't recall them, visual hierarchy is wrong. Possible fixes: (1) Credentials aren't large enough or high-contrast enough (increase typography scale, use bolder color). (2) Credentials are visually present but semantically unclear (hosts don't understand what 'Licensed real estate platform' means or why it matters). (3) Hosts skip credentials and go straight to business model (suggests credentials are less important than hypothesized—business model clarity is what drives trust, not licensing badges). If failure is placement-related (sidebar performs worse than hero), validates hero positioning. If failure is universal, reconsider what credentials matter to professional hosts—maybe testimonials from other brokers are more persuasive than licenses.",
      "implementation_hint": "Use scroll heatmaps on live partnership page to validate eye-tracking findings at scale. Do hosts scroll past credentials without pausing? If yes, they're not engaged. Use session recording to see: Do hosts click partner logos (curiosity signal)? Do they right-click to verify links (trust-but-verify behavior)? If hosts are actively verifying, credentials matter but need to be verifiable (link to license lookup, link to partner websites, link to case studies). Implement: 'Learn more about our partners' link that opens modal with detailed social proof."
    },
    {
      "id": "tests-007",
      "type": "validation_strategy",
      "title": "ROI Calculator Interaction Completion and Confidence Validation",
      "validates_element": "looks-002",
      "journey_phases": ["evaluation", "pricing"],
      "problem": "Economic transparency dashboard with interactive ROI calculator is designed to reduce cognitive load and build pricing confidence through visual comparison cards, real-time value updates with spring animations, and data-anchored recommendations. Without validation, we don't know if hosts actually use the calculator, complete the interaction, or gain confidence from it, or if UI complexity creates abandonment.",
      "solution": "Instrumented calculator on evaluation page tracking: (1) % of hosts who land on evaluation page and interact with calculator (engagement rate). (2) % who adjust inputs (base rent, lease duration, markup) vs. view default scenario only. (3) % who complete full interaction (adjust all inputs, view results) vs. abandon mid-interaction. (4) Input patterns: What rent ranges do hosts enter? What markup percentages? Outlier detection: Are hosts entering unrealistic values (testing boundaries)? (5) Time spent on calculator: <30 seconds (glance), 30-120 seconds (exploration), 120+ seconds (deep modeling). Hypothesis: 70%+ spend 30-120 seconds. (6) Correlation: Do hosts who use calculator proceed to scheduling call at higher rate than non-users? Survey calculator users: 'Did the calculator help you understand the economics?' Yes/No. 'How confident do you feel about pricing?' 1-7 scale.",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 06:29-12:52",
          "type": "host_call",
          "quote": "Jessica spent 6+ minutes mentally modeling economics through Q&A with Robert, asking 'What's the price range?' and 'How does the markup work?'",
          "insight": "Jessica needed to externalize economic modeling but had no tool, so she used Robert as calculator. Test validates whether self-service calculator provides same clarity without requiring 6+ minutes of call time."
        },
        {
          "source": "behaves-elements.json, behaves-002",
          "type": "layer_4",
          "quote": "Real-time calculator feedback loop: as host types new base rent value, calculator updates in real-time with debouncing (300ms). Value count-up animation (spring easing) creates visual drama.",
          "insight": "Interaction design hypothesis: Real-time updates + smooth animations make calculator engaging and satisfying. Test validates whether this creates perceived ease of use or whether hosts find animations distracting/slow."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Track 200 hosts who visit evaluation page with calculator. Instrument calculator with event logging: calculator_opened, input_changed (which field), calculation_completed, calculator_closed. Track session duration, inputs entered, results viewed. Segment analysis: Compare hosts who use calculator vs. don't use calculator on: (1) Call scheduling rate. (2) First listing creation time (post-call). (3) Pricing accuracy (listings priced within recommended range). Survey random sample of 40 calculator users post-call: 'How helpful was the ROI calculator?' 1-7 scale. 'What would you change about it?' Open-ended.",
      "success_criteria": "70%+ of evaluation page visitors interact with calculator (click or adjust inputs). 80%+ of those who start complete full interaction (enter base rent, lease duration, view results). Average time spent 30-120 seconds (exploratory but not stuck). Calculator users proceed to call scheduling at 1.5x rate of non-users. Calculator users create first listing 30%+ faster than non-users. Post-survey: 85%+ rate calculator 6+ out of 7 helpful. Open-ended feedback: 'Easy to use,' 'Helped me understand ROI,' 'Answered my questions.'",
      "failure_meaning": "If engagement rate is low (<50%), calculator isn't discoverable or value isn't clear. Fixes: (1) Add explainer headline above calculator: 'See Your Potential Earnings in 2 Minutes.' (2) Add inline help: '?' icons explaining each input. (3) Use video demo showing calculator in action. If abandonment is high (users start but don't complete), interaction is too complex or slow. Fixes: (1) Simplify inputs—remove lease duration if most hosts use 6 months, pre-set that as default. (2) Speed up animations (300ms debounce may feel sluggish). (3) Add progress indicator: 'Step 1 of 3: Enter base rent.' If completion is high but satisfaction is low, results aren't useful. Fixes: (1) Show more granular breakdown (not just total commission, but monthly cash flow). (2) Add sensitivity analysis: 'If you reduce price by 10%, commission drops to $X but conversion speeds up by 3 days.' (3) Add comparison to other hosts: 'Hosts with similar properties typically markup 15-25%.'",
      "implementation_hint": "Use session replay to watch hosts interact with calculator. Are they confused by any input? Do they try to interact with non-interactive elements (clicking static text expecting it to do something)? Do they leave calculator and return multiple times (exploration pattern)? Do they screenshot results (high-value signal)? Track mobile vs. desktop: Does calculator work well on mobile or do small screens create usability issues?"
    },
    {
      "id": "tests-008",
      "type": "validation_strategy",
      "title": "Constraint Feedback Tone Perception A/B Test",
      "validates_element": "behaves-003",
      "journey_phases": ["listing_creation", "active_lease"],
      "problem": "Constraint feedback anticipatory guidance (pricing slider with color zones, guidance card appearing as host approaches yellow/red zones) is designed to provide honest recommendations that prevent pricing errors. Robert's verbal version built trust with Jessica, but without validation we don't know if UI systematization preserves collaborative tone or feels patronizing, especially to professional hosts who have superior market knowledge.",
      "solution": "A/B test two constraint feedback tones with 80 hosts (40 per variant) during listing creation. Both variants show same pricing slider with zones, but guidance card language differs. Variant A (collaborative): 'Based on what's converting right now, here's an adjustment that would help this listing perform better: Reducing from $150 to $125/day would cut conversion time from 30 days to 10 days (based on 150 comparable listings).' Variant B (assertive): 'This price is 20% above comparable properties and rarely converts. Reduce to $125/day to improve visibility.' Measure: (1) % of hosts who adjust pricing after seeing guidance. (2) % who dismiss guidance without adjusting. (3) Task completion rate (publish listing). (4) Post-task survey: 'How did the pricing guidance make you feel?' Options: Supported, Informed, Uncertain, Criticized, Annoyed. (5) NPS: 'How likely are you to recommend Split Lease based on this experience?'",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 18:28-18:54",
          "type": "host_call",
          "quote": "Robert: 'Pricing things out of the market would be no interest to anybody.' Jessica responded warmly: 'That sounds good to me. Thank you so much, Robert.'",
          "insight": "Robert's constraint advice was honest but collaborative ('would be no interest to anybody,' not 'you're wrong'). Test validates which tone—collaborative vs. assertive—achieves same trust-building effect in UI."
        },
        {
          "source": "feels-elements.json, feels-001",
          "type": "layer_5",
          "quote": "Copy guidelines: Frame constraints as collaborative optimization: 'Based on what's converting right now, here's an adjustment that would help this listing perform better.' NOT: 'This price is wrong.'",
          "insight": "Layer 5 recommends collaborative framing to avoid triggering defensiveness. Test validates whether hosts actually perceive collaborative tone as different from assertive tone, or if any constraint feedback feels critical."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "80 hosts creating their first listing. Randomly assigned to Variant A (collaborative) or B (assertive). Task: 'Create a listing for a [property type] and set pricing.' Track interaction: slider position when guidance card appears, time spent reading guidance, whether price is adjusted after viewing guidance. Post-task survey: Emotional response (Supported/Informed/Criticized/Annoyed), NPS, open-ended: 'How did you feel about the pricing guidance?' Segment analysis: Compare brokers vs. individual landlords—do professionals respond differently to constraint feedback than non-professionals?",
      "success_criteria": "Variant A (collaborative) shows: (1) Higher adjustment rate (70%+ adjust price vs. 50% Variant B). (2) Lower dismissal rate without adjustment (20% vs. 40% Variant B). (3) Higher 'Supported'/'Informed' emotional response (80%+ vs. 60% Variant B). (4) Higher NPS (8+ vs. 6 Variant B). (5) Open-ended: Variant A feedback includes 'helpful,' 'not pushy,' 'expert advice.' Variant B includes 'felt criticized,' 'too direct,' 'I know my market better.' Segment finding: If brokers respond equally well to both variants, tone matters less for professionals (they focus on data, not delivery).",
      "failure_meaning": "If both variants perform equally (no significant difference in adjustment rate or emotional response), tone doesn't matter—hosts either trust the data or don't, regardless of phrasing. Alternative: Focus on data credibility ('based on 150 comparable listings') rather than tone refinement. If both variants underperform (low adjustment rate, high annoyance), constraint feedback itself is unwanted—hosts prefer full autonomy even if it leads to pricing errors. Alternative: Make guidance opt-in: 'Need pricing help?' button that reveals guidance only when requested. If brokers respond worse to both variants than landlords, professional expertise creates resistance to any guidance—consider suppressing constraint feedback for verified broker accounts or using softer framing: 'FYI: Comparable properties at $125/day convert in 10 days vs. 30 days at $150.'",
      "implementation_hint": "Track: Do hosts who dismiss guidance (don't adjust price) return to adjust later (suggesting they thought about it) or publish at original price (suggesting they rejected guidance)? If they return to adjust, dismissal is part of exploration process, not true rejection. If they publish at original price, track conversion outcome: Do their listings convert at predicted timeline (validates guidance accuracy) or convert faster (invalidates guidance, hosts were right)? Use this data to refine guidance thresholds."
    },
    {
      "id": "tests-009",
      "type": "validation_strategy",
      "title": "Scarcity Proof Engagement Metrics Animation Impact Test",
      "validates_element": "looks-004",
      "journey_phases": ["evaluation", "proposal_mgmt"],
      "problem": "Scarcity proof visualization uses real-time engagement metrics (views, saves, matched searches) with spring animation count-up to create visual drama that emphasizes growing demand. Design hypothesis: animated metrics feel more dynamic and exciting than static numbers, validating scarcity through movement. Without testing, we don't know if animations enhance perceived demand or feel gimmicky/slow.",
      "solution": "A/B test two dashboard variants for 60 hosts receiving their first engagement metrics. Variant A (animated): Metrics count up from 0 to actual value with spring easing (800ms per metric, 200ms stagger). Variant B (static): Metrics display final value immediately, no animation. Measure: (1) Time spent viewing metrics section (dwell time). Hypothesis: Variant A engages attention longer. (2) Click-through to listing detail page (curiosity signal). (3) Return dashboard visits within 24 hours (re-engagement). Hypothesis: Variant A prompts more checking behavior ('Did views increase?'). (4) Post-viewing survey: 'How confident do you feel about demand for your listing?' 1-7 scale. Hypothesis: Variant A scores higher. (5) Qualitative: 'How did you feel when you saw your listing metrics?' Options: Excited, Reassured, Neutral, Skeptical.",
      "evidence": [
        {
          "source": "cialdini-authority-liking-scarcity.txt, line 520",
          "type": "book",
          "quote": "The idea that newly experienced scarcity is the more powerful kind applies to situations well beyond the bounds of the cookie study.",
          "insight": "Cialdini: experiencing scarcity directly is most persuasive. Test validates whether animated metrics create stronger sense of 'experiencing' demand through visual motion vs. static numbers."
        },
        {
          "source": "looks-elements.json, looks-004",
          "type": "layer_3",
          "quote": "Use spring animation to count up from 0 to actual number, creating visual drama that emphasizes growing demand. Metrics update every 60 seconds with smooth number transitions.",
          "insight": "Design hypothesis: animation creates micro-celebration ('Look, 18 people viewed your listing!') that static numbers don't. Test validates whether this emotional lift translates to increased confidence and engagement."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "60 hosts randomly assigned to Variant A (animated) or B (static) upon first engagement metrics availability (48 hours after listing goes live). Track using analytics: time on dashboard page, interactions with metrics section (clicks, hovers), return visits. Post-session survey (sent 1 hour after first metrics view): 'How confident are you that your listing will receive proposals?' 1-7 scale. 'How did the engagement metrics make you feel?' Open-ended. Compare Variant A vs. B on all metrics. Hypothesis: Variant A shows 20-30% higher dwell time, 15-20% higher confidence rating, more 'Excited'/'Reassured' emotional responses.",
      "success_criteria": "Variant A (animated) shows: (1) 20-30% higher dwell time on metrics section (hosts watch animation complete vs. glance at static numbers). (2) 15-20% higher click-through to listing detail. (3) 25% more return visits within 24 hours (checking for updated metrics). (4) Confidence rating averages 6+ vs. 5 for Variant B. (5) Emotional response: 70%+ 'Excited'/'Reassured' vs. 50% Variant B. Open-ended: Variant A feedback includes 'seeing the views count up was cool,' 'felt real-time,' 'made me excited.' Variant B neutral or no comments about metrics presentation.",
      "failure_meaning": "If Variant A shows no improvement or performs worse (lower dwell time, lower confidence), animations are distracting or feel manipulative. Hosts may perceive count-up as 'trying too hard' to impress them. Alternative: Static numbers with subtle visual emphasis (bold typography, accent color, icon) create same impact without motion. If failure is segment-specific (e.g., brokers prefer static, landlords prefer animated), personalize: sophisticated users get minimal UI, casual users get celebratory UI. If both variants show low engagement (low dwell time, low confidence regardless of variant), engagement metrics themselves aren't meaningful to hosts—they want proposals, not views. Alternative: De-emphasize views/saves, emphasize matched searches ('5 active guests searching for your property type') which is closer to actual conversion.",
      "implementation_hint": "Track: Do hosts pause to watch full animation (800ms + stagger = ~1 second total) or leave page mid-animation (suggests animation is too slow)? If hosts leave mid-animation, reduce duration to 400ms. Do hosts hover over metrics during animation (curiosity about what's animating) or ignore animation? If ignore, animation isn't adding value. Session replay: Do hosts smile or lean in during animation (positive engagement signals captured on camera if usability test)? Track correlation between seeing metrics and sending more listings—do hosts who see strong metrics (20+ views) send additional listings faster than hosts with weak metrics (5 views)? If yes, metrics (not animation) drive behavior. If no, neither metrics nor animation matter—something else drives listing expansion."
    },
    {
      "id": "tests-010",
      "type": "validation_strategy",
      "title": "Professional Skepticism Emotional Arc Journey Validation",
      "validates_element": "feels-001",
      "journey_phases": ["discovery", "evaluation", "onboarding"],
      "problem": "Jessica's emotional arc—professional skepticism (defensive, interrogating) to collaborative trust (warm, appreciative)—was driven by Robert's expertise demonstration and honest guidance. Design hypothesis: systematized patterns (authority credentials, honest constraint feedback, transparent economics) can replicate this emotional transition at scale. Without validation, we don't know if the arc is achievable through product alone or if human agent skill is irreplaceable.",
      "solution": "Journey-level qualitative study with 15 professional hosts tracked from discovery through onboarding. Method: Semi-structured interviews at three touchpoints: (1) Post-discovery (after viewing partnership page): 'What were your initial impressions of Split Lease? What concerns did you have?' Code emotional state: skeptical, curious, cautious, confident. (2) Post-evaluation call: 'How do you feel about partnering with Split Lease now? What changed from your initial impression?' Code emotional shift: more skeptical, no change, more confident, committed. Probe: 'What specific information or interaction built your trust?' (3) Post-first-listing creation: 'How do you feel now that you've created your first listing?' Code: anxious about conversion, confident it will work, neutral/waiting. Track emotional arc across 3 touchpoints for each host. Identify patterns: Do hosts who start skeptical become collaborative by touchpoint 3? What elements drive the shift? Compare to Jessica's arc: discovery skepticism → evaluation respect → onboarding collaboration.",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 01:01-03:00 vs. 18:54-19:03",
          "type": "host_call",
          "quote": "Jessica opened defensive ('Do you work with management companies?') and ended warm ('That sounds good to me. Thank you so much, Robert.')—a complete emotional reversal in 19 minutes.",
          "insight": "Jessica's arc proves skepticism-to-trust transition is achievable. Test validates whether product systematization (credentials, calculator, constraint feedback) drives same arc or if Robert's specific rapport-building was unique/unreplicable."
        },
        {
          "source": "feels-elements.json, feels-001",
          "type": "layer_5",
          "quote": "Target emotion: respect. Emotion rationale: Hosts transition from skepticism to commitment through respect for Split Lease's expertise and honesty, not through excitement or urgency.",
          "insight": "Design targets respect (cognitive trust) not excitement (emotional persuasion). Test validates whether respect-building elements (authority, honest guidance, transparency) actually generate respect emotional response."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "15 professional hosts (brokers, property managers) recruited for journey study. Interview at 3 touchpoints: (1) After viewing partnership page (discovery), (2) After evaluation call (evaluation), (3) After creating first listing (onboarding). Each interview 15-20 minutes, semi-structured with emotional state questions. Code transcripts for emotional keywords: skeptical, cautious, uncertain, curious (negative to neutral) vs. confident, reassured, optimistic, committed (positive). Map each host's arc: What emotions at each touchpoint? What caused shifts? Compare to Jessica's arc. Identify hosts who don't achieve collaborative trust by touchpoint 3—why? What elements didn't work for them?",
      "success_criteria": "70%+ of hosts move from skeptical/cautious (touchpoint 1) to confident/committed (touchpoint 3). Identified drivers of emotional shift: 60%+ mention authority credentials as trust-builder at touchpoint 1, 70%+ mention economic transparency (calculator or call discussion) as confidence-builder at touchpoint 2, 80%+ mention honest guidance (constraint feedback or Robert's advice) as relationship-builder. Hosts who achieve collaborative trust show same behavioral outcomes as Jessica: send listings within 7 days, respond positively to proposal notifications, expand listings after validation.",
      "failure_meaning": "If <50% of hosts achieve collaborative trust arc, product alone doesn't drive emotional transition—human agent skill is critical. Possible reasons: (1) Authority credentials are necessary but not sufficient—hosts need human reassurance even when credentials are visible. (2) Economic transparency doesn't build confidence without agent interpretation—calculator is useful but doesn't replace conversation. (3) Constraint feedback feels different from agent advice—hosts trust humans to give honest guidance but not systems. Alternative: Hybrid model—product provides information, human agent provides interpretation and reassurance. If failure is segment-specific (brokers don't achieve arc but landlords do), professional hosts require different approach: lighter touch, more peer validation (testimonials from other brokers), less explanation (assume sophisticated).",
      "implementation_hint": "Record evaluation calls and code agent behavior: Which rapport-building tactics does agent use (empathy, expertise demonstration, autonomy language, honest constraints)? Correlate agent tactics with host emotional response post-call. Identify: Which agent behaviors are replicable through product (transparency, data-sharing) vs. inherently human (warmth, empathy, adaptive explanation)? Use findings to decide: Can we reduce agent involvement by improving product? Or should we invest in agent training to replicate Robert's effectiveness?"
    },
    {
      "id": "tests-011",
      "type": "validation_strategy",
      "title": "Journey-Level Coherence: Phase Transition Dropout Analysis",
      "validates_element": "coherence-report.json (overall journey coherence)",
      "journey_phases": ["all"],
      "problem": "Coherence report identifies phase transition friction: discovery → evaluation (authority trust gate), evaluation → onboarding (economic clarity + broker approval gate), onboarding → listing_creation (effort vs. reward uncertainty), listing_creation → proposal_mgmt (scarcity validation). Each transition is an exit risk. Without validation, we don't know which transitions cause most dropout and whether elements designed to smooth transitions (authority badges, ROI calculator, scarcity proof) actually reduce dropout.",
      "solution": "Funnel analysis tracking 200 professional hosts from discovery through retention. Measure conversion rate at each phase transition: discovery → evaluation (% who schedule call after viewing partnership page), evaluation → onboarding (% who complete signup after call), onboarding → listing_creation (% who create first listing within 7 days), listing_creation → proposal_mgmt (% whose listings receive proposals within 14 days), proposal_mgmt → retention (% who send additional listings after first proposal). Identify highest-dropout transitions. For each transition, survey dropouts (exit survey): 'Why didn't you proceed?' Options: Not convinced of legitimacy, Economics unclear, Too much effort, Broker didn't approve, No time, Other. Compare dropout reasons to elements designed to address them: authority dropout → works-001, economic dropout → works-002, effort dropout → pricing tools, broker approval → no current element (gap).",
      "evidence": [
        {
          "source": "coherence-report.json, transition friction analysis",
          "type": "layer_6",
          "quote": "Discovery → Evaluation: Authority trust gate. Evaluation → Onboarding: Economic clarity + broker approval. Onboarding → Listing_creation: Effort vs. reward. Listing_creation → Proposal_mgmt: Scarcity validation.",
          "insight": "Coherence report identifies friction points but doesn't quantify dropout rates. Test measures actual dropout at each gate, identifies which is most critical to address first."
        },
        {
          "source": "jessica-filomeno-call.txt, 14:27",
          "type": "host_call",
          "quote": "Jessica: 'I have to run it by my broker, but I am interested.'",
          "insight": "Broker approval is an identified exit gate. Test quantifies: What % of brokers actually drop out due to broker disapproval? Is this a major dropout driver or rare edge case? If major, requires new element to address (broker firm outreach, pre-approved firm list)."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "200 professional hosts tracked through journey funnel. Use analytics to measure conversion rate at each transition. Calculate: (1) Discovery → Evaluation conversion (partnership page view → call scheduled). (2) Evaluation → Onboarding (call completed → signup completed). (3) Onboarding → Listing_creation (signup → first listing published). (4) Listing_creation → Proposal_mgmt (listing published → first proposal received). (5) Proposal_mgmt → Retention (first proposal → 2nd listing published). Identify lowest conversion rate (highest dropout). For that transition, conduct exit surveys with 40 dropouts: 'You [viewed partnership page/completed call/signed up] but didn't [schedule call/complete signup/create listing]. Why?' Open-ended + multiple choice options based on hypothesized friction points. Compare dropout reasons to existing elements designed to address them. Identify gaps.",
      "success_criteria": "Each transition achieves target conversion rate: (1) Discovery → Evaluation: 30%+ (high bar, cold traffic). (2) Evaluation → Onboarding: 60%+ (call engagement should be high-intent). (3) Onboarding → Listing_creation: 70%+ (signup signals commitment). (4) Listing_creation → Proposal_mgmt: 80%+ (listing quality permitting). (5) Proposal_mgmt → Retention: 60%+ (first proposal validates model). Exit survey validation: Dropout reasons align with hypothesized friction (authority, economics, effort, approval). <10% cite 'Other' unexpected reasons. For top dropout transition, 70%+ of exit reasons are addressable by existing elements (suggests implementation, not design, is issue) or identify new element needed (e.g., broker firm outreach for approval gate).",
      "failure_meaning": "If dropout is concentrated at one transition (e.g., Evaluation → Onboarding), that gate is the critical constraint—address it first. If dropdown reasons don't align with hypothesized friction (e.g., dropouts cite 'Too complicated' but economics elements tested well), there's a gap between element validation and actual journey experience—implementation may be poor, or elements aren't discoverable. If dropout is evenly distributed (no single bad transition), journey is leaky overall—requires holistic improvement, not targeted fixes. If 'Other' dropout reasons dominate (>30%), current friction hypothesis is wrong—conduct deep-dive qualitative interviews to understand real barriers.",
      "implementation_hint": "Use cohort comparison: Hosts exposed to full element suite (authority badges, ROI calculator, constraint feedback, scarcity proof) vs. control cohort without these elements. Does element suite reduce dropout at targeted transitions? If yes, validates elements. If no, elements aren't effective or aren't being used. Track: For each transition, what % of dropouts never engaged with relevant elements (e.g., economic dropout who never opened calculator)? If high, discoverability is the issue—make elements more prominent, add prompts. Track: Time-to-dropout—do hosts drop out immediately (didn't engage) or after exploration (engaged but unconvinced)? Immediate dropout suggests positioning/value prop issue. Delayed dropout suggests elements were tried but didn't deliver."
    },
    {
      "id": "tests-012",
      "type": "validation_strategy",
      "title": "Broker Approval Gate B2B Partnership Model Validation",
      "validates_element": "works-001 + coherence gap (broker approval not directly addressed)",
      "journey_phases": ["evaluation", "onboarding"],
      "problem": "Jessica stated 'I have to run it by my broker' (line 14:27), revealing broker approval as a structural gate for professional hosts. Current elements address individual broker trust (authority credentials, economics, confidentiality) but don't address firm-level approval process. Coherence report identifies this as gap: 'Broker approval gate: Jessica's commitment is contingent on her broker's approval. If approval is denied, she exits regardless of personal interest.' Without validation, we don't know: (1) What % of brokers require firm approval? (2) What % get approval vs. denial? (3) What factors drive approval decisions? (4) Can Split Lease influence this gate?",
      "solution": "Mixed-methods study with two phases. Phase 1 (quantitative): Survey 60 brokers who completed evaluation calls. Ask: (1) 'Do you need your broker firm's approval to partner with Split Lease?' Yes/No. (2) If yes: 'Have you requested approval?' Yes/Waiting/No. (3) If requested: 'What was the outcome?' Approved/Denied/Pending. (4) If denied: 'Why was approval denied?' Options: Risk concerns, Competes with firm policy, Unclear economics, Requires legal review, Other. Calculate: % of brokers requiring approval, % approval rate, top denial reasons. Phase 2 (qualitative): Interview 10 broker firms (operations managers or compliance officers). Ask: (1) 'What criteria do you use to approve external partnerships for your agents?' (2) 'What information would you need to approve Split Lease partnership?' (3) 'What concerns would cause you to deny approval?' Synthesize: What B2B elements are needed to smooth firm approval (firm-level agreements, compliance documentation, case studies from peer firms)?",
      "evidence": [
        {
          "source": "jessica-filomeno-call.txt, 14:27",
          "type": "host_call",
          "quote": "Jessica: 'I have to run it by my broker, but I am interested.'",
          "insight": "Jessica is not the final decision-maker. Broker approval is an external dependency that current elements don't address. Test quantifies prevalence and identifies what's needed to influence this gate."
        },
        {
          "source": "coherence-report.json, cross-phase pattern",
          "type": "layer_6",
          "quote": "The broker approval gate as a structural dropout risk: Brokers are agents, not principals. They operate under regulatory oversight and firm policies. Any partnership that bypasses broker approval is DOA.",
          "insight": "Coherence report identifies this as structural, not individual-level issue. Test validates severity and identifies solution path (B2B sales approach, not B2C)."
        }
      ],
      "priority": "high",
      "validation_method": "manual_review",
      "test_description": "Phase 1: Survey 60 brokers (those who completed evaluation calls in past 90 days) via email. 5-minute survey with quantitative questions. Track response rate, calculate approval prevalence and rate. Phase 2: Recruit 10 broker firms for 30-minute interviews (compensate with $100 gift card). Semi-structured interview protocol. Record, transcribe, code for approval criteria themes. Synthesize findings into: (1) Approval criteria checklist. (2) Firm-level collateral needs (compliance docs, insurance proof, case studies). (3) Recommended B2B sales process (individual broker intro → firm-level presentation → approval).",
      "success_criteria": "Phase 1: >70% of brokers require firm approval (validates structural nature). Approval rate >60% (suggests gate is passable with current info). Top denial reasons are addressable (e.g., 'Unclear economics' → provide firm-level ROI deck; 'Risk concerns' → provide insurance/legal docs). Phase 2: Firm interviews identify 3-5 consistent approval criteria. 80%+ of criteria are addressable through new collateral (not insurmountable barriers like regulatory prohibition). Firms express willingness to approve if criteria met: 'If you provide [X, Y, Z], we'd be open to agents using this.' Deliverable: B2B partnership playbook with firm-level sales process and collateral templates.",
      "failure_meaning": "If approval rate is low (<40%) and denial reasons are structural (e.g., 'Violates firm policy,' 'Regulatory concerns,' 'Not interested in split-lease model'), broker approval gate is a hard barrier that collateral can't solve. Alternative: (1) Target independent brokers who don't require firm approval (change acquisition strategy). (2) Negotiate firm-level partnerships where Split Lease becomes approved vendor (changes business model to B2B2C). (3) Accept that broker channel has structural dropout and focus on direct landlord acquisition. If firm interviews reveal approval criteria that Split Lease can't meet (e.g., 'Must be publicly traded company,' 'Requires $5M insurance policy'), re-evaluate broker segment viability—cost to meet criteria may exceed segment value.",
      "implementation_hint": "Track in CRM: For each broker lead, capture 'Firm approval required?' field. For those requiring approval, track approval status over time. Measure: How long does approval take (days from request to decision)? What % get stuck in 'Pending' (suggests firm isn't prioritizing decision)? If pending rate is high, provide broker with firm presentation deck: 'Here's a one-pager to share with your broker about Split Lease. It answers common compliance questions.' Track: Does provision of firm deck increase approval rate? If yes, validates B2B collateral need. If no, approval isn't about information—it's about firm priorities or politics."
    }
  ]
}
