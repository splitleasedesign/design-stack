{
  "lens": {
    "guest_call": "Theodore Matthews.txt",
    "book_extract": "articulatingdesign-severity-framework.txt"
  },
  "elements": [
    {
      "id": "tests-001",
      "type": "validation_strategy",
      "title": "Self-Articulating UI Comprehension Test",
      "validates_element": "works-001",
      "journey_phases": ["discovery", "listing_evaluation", "proposal_creation"],
      "problem": "If the self-articulating UI fails, guests will still need agent calls to understand basic platform mechanics, maintaining high CAC and limiting scale.",
      "solution": "Run a task-based usability test where participants must answer 4 questions using only the UI (no agent): What is SplitLease? How does the 24-hour guarantee work? What does submitting a proposal do? What is the 40% discount tier? Score comprehension accuracy.",
      "evidence": [
        {
          "source": "Theodore Matthews.txt, entire call",
          "type": "host_call",
          "quote": "Theodore needed 14 minutes of agent explanation for concepts the UI should communicate.",
          "insight": "If the redesigned UI can answer these questions without an agent, the articulation pattern is working."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 8 participants unfamiliar with SplitLease. Show them the redesigned listing page with articulation microcopy. Ask them to explain in their own words: (1) What SplitLease is, (2) how the 24-hour guarantee works, (3) what happens when they submit a proposal, (4) what the discount tier offers. Score each answer 0-2 for accuracy.",
      "success_criteria": "Average comprehension score of 6/8 or higher (75%). All 4 concepts must average above 1.0/2.0 individually.",
      "failure_meaning": "If comprehension is below 6/8, the articulation microcopy is either too subtle (not read) or too complex (not understood). Need to test visibility (is it seen?) separately from clarity (is it understood?).",
      "implementation_hint": "Can be run as an unmoderated UserTesting.com study with screen recordings. Key metric is the guest's verbal explanation accuracy, not task completion time."
    },
    {
      "id": "tests-002",
      "type": "validation_strategy",
      "title": "Reframing Effectiveness A/B Test",
      "validates_element": "works-002",
      "journey_phases": ["listing_evaluation", "acceptance"],
      "problem": "If reframing comparisons don't work as well in UI form as they did verbally with Theodore, we could be adding complexity without benefit.",
      "solution": "A/B test listing pages with and without the reframing comparison card. Variant A: standard listing page with informational text about the 24-hour guarantee. Variant B: listing page with the reframing comparison ('Like Airbnb, but with 24 hours to decide'). Measure conversion from listing_evaluation to proposal_creation.",
      "evidence": [
        {
          "source": "Theodore Matthews.txt, 04:36-04:50",
          "type": "host_call",
          "quote": "That's true. That's fair. That's a good point.",
          "insight": "The verbal reframe had instant impact. The A/B test validates whether the same technique works visually."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "Split traffic 50/50 on listing pages. Control: 'Your payment is held for 24 hours. If the property doesn't match the listing, you get a full refund.' Treatment: 'Think Airbnb — you don't tour the apartment first. But unlike Airbnb, you get 24 hours and a full refund if anything is off.' Measure proposal submission rate.",
      "success_criteria": "Treatment shows statistically significant (p<0.05) increase in proposal submission rate, minimum 10% lift.",
      "failure_meaning": "If no lift, the reframing technique may require more context than a single comparison line — might need the full comparison card (looks-004) rather than inline microcopy.",
      "implementation_hint": "Implement via feature flag on the listing page protection section. Track event: proposal_submitted with variant label. Run for minimum 2 weeks or 200 conversions per variant."
    },
    {
      "id": "tests-003",
      "type": "validation_strategy",
      "title": "Matching Progress Visibility Retention Test",
      "validates_element": "works-003",
      "journey_phases": ["negotiation"],
      "problem": "If the matching progress tracker doesn't reduce post-bid dropout, we're adding UI complexity without addressing the core retention issue.",
      "solution": "Compare post-bid retention (app opens/email opens at Day 7 and Day 14) for guests with the progress tracker vs. the current experience (no tracker). Secondary metric: agent call volume for 'what's happening with my bid?' questions.",
      "evidence": [
        {
          "source": "Theodore Matthews.txt, 08:40",
          "type": "host_call",
          "quote": "Give us two to three weeks to complete the process.",
          "insight": "The 2-3 week wait is the highest-risk dropout period. If the tracker keeps guests engaged, it's working."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Track two cohorts: guests who submitted proposals with the matching tracker visible vs. historical cohort without it. Measure: (1) App opens during matching period, (2) Email open rates for matching updates, (3) Agent call volume for status inquiries, (4) Conversion from proposal → acceptance.",
      "success_criteria": "Tracker cohort shows 20% higher 14-day retention (app opens) and 30% reduction in agent status-inquiry calls.",
      "failure_meaning": "If retention doesn't improve, the matching progress tracker may not be providing enough value — guests may need CONTENT during the wait (listings, neighborhood info) rather than just progress updates.",
      "implementation_hint": "Instrument with analytics events: matching_tracker_viewed, matching_update_received, agent_call_status_inquiry. Compare weekly cohorts. Minimum 50 proposals per cohort."
    },
    {
      "id": "tests-004",
      "type": "validation_strategy",
      "title": "Savings Counter Cleaning Compliance Test",
      "validates_element": "works-004",
      "journey_phases": ["active_lease"],
      "problem": "If the savings counter doesn't increase cleaning photo submission rates, the value articulation approach isn't strong enough to drive behavioral compliance.",
      "solution": "Compare cleaning photo submission rates for discount-tier guests with the savings counter visible vs. the current experience. Track both submission rate and the emotional quality of the interaction (via optional 1-tap feedback: 'Was this worth it? Yes/No').",
      "evidence": [
        {
          "source": "Theodore Matthews.txt, 12:43-12:55",
          "type": "host_call",
          "quote": "For less flexibility and willing to do some chores, you get a 40% discount.",
          "insight": "The discount motivates signup but may not sustain compliance. The savings counter must maintain motivation over 16+ weeks."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "Split discount-tier guests into control (current cleaning flow) and treatment (cleaning flow + savings counter animation). Track: (1) Weekly cleaning submission rate, (2) Submission speed (seconds from prompt to submission), (3) Streak maintenance (consecutive weeks with submissions), (4) Optional 1-tap feedback score.",
      "success_criteria": "Treatment group shows 15% higher weekly submission rate and 20% longer average streak. 1-tap feedback 'worth it' rate above 80%.",
      "failure_meaning": "If submission rate doesn't improve, the dollar value framing may not be sufficient — may need to add social comparison ('Most guests in your area submit Space Care within 10 minutes of checkout').",
      "implementation_hint": "Track events: space_care_prompt_shown, space_care_submitted, savings_counter_viewed, savings_counter_milestone_reached. A/B split at the user level, not session level."
    },
    {
      "id": "tests-005",
      "type": "validation_strategy",
      "title": "Protection Model Comprehension Test",
      "validates_element": "communicates-001",
      "journey_phases": ["listing_evaluation", "acceptance"],
      "problem": "If the progressive protection reveal doesn't resolve the 'signing before seeing' anxiety, guests will still need agent calls to feel safe committing.",
      "solution": "Post-acceptance survey asking: 'Before you submitted your proposal, how well did you understand the 24-hour guarantee? (1-5 scale)' and 'Did the protection information on the listing page help you feel safe committing? (Yes/Somewhat/No)'. Compare responses with the pre-redesign baseline.",
      "evidence": [
        {
          "source": "Theodore Matthews.txt, 04:07-04:50",
          "type": "host_call",
          "quote": "At one point we'll actually get to see the apartment before I signed the lease... That's true. That's fair.",
          "insight": "Theodore's objection was completely resolved by the agent's explanation. The protection reveal must achieve the same resolution without an agent."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Post-proposal survey (in-app, 2 questions max). Question 1: 'How confident are you that your money is protected?' (1-5 scale). Question 2: 'What would you do if the apartment didn't match the listing?' (free text). Score free text for awareness of 24-hour guarantee and refund option.",
      "success_criteria": "Average confidence score of 4.0/5.0 or higher. 80% of free-text responses mention the 24-hour guarantee or refund option unprompted.",
      "failure_meaning": "If confidence is below 4.0, the protection information is either not visible enough or not persuasive enough. Test visibility first (do they see it?) before redesigning content.",
      "implementation_hint": "Trigger survey 30 seconds after proposal submission. Use in-app modal with 2-question form. Store responses with user journey phase metadata. Run for 100 responses minimum."
    },
    {
      "id": "tests-006",
      "type": "validation_strategy",
      "title": "Journey-Level Confusion-to-Clarity Arc Validation",
      "validates_element": "feels-003",
      "journey_phases": ["discovery", "search", "listing_evaluation", "proposal_creation"],
      "problem": "If the sequential question-resolution arc doesn't work, the guest may still feel confused at proposal submission, leading to abandonment at the final step.",
      "solution": "Instrument the listing page with scroll-depth and time-on-section analytics. Track whether guests who engage with each section sequentially (discovery → match → price → protection → propose) have higher conversion than those who skip sections. Validate that the confusion-to-clarity arc produces measurable confidence increase.",
      "evidence": [
        {
          "source": "Theodore Matthews.txt, 00:24 to 14:09",
          "type": "host_call",
          "quote": "From 'Now I am confused' to 'I really appreciate our chat.'",
          "insight": "The emotional arc happened because questions were resolved sequentially. The page layout must replicate this sequential resolution."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Track section engagement order on listing pages: (1) Which sections are viewed (scroll depth per section), (2) Time spent on each section, (3) Whether engagement is sequential or random. Correlate with proposal submission rate. Hypothesis: guests who engage sections in the designed order (match → price → protection → propose) convert at 2x the rate of random engagers.",
      "success_criteria": "Sequential engagers convert at 1.5x or higher rate than random engagers. Average time to proposal submission decreases by 20%.",
      "failure_meaning": "If sequential engagement doesn't correlate with higher conversion, the section order may not match the guest's natural question sequence. Need qualitative research to discover the actual question order.",
      "implementation_hint": "Implement intersection observer on each listing page section. Log: section_id, timestamp_entered, timestamp_exited, scroll_position. Build engagement sequence per session. Compare conversion rates by sequence type. Minimum 500 sessions."
    }
  ]
}
