{
  "lens": {
    "host_call": "kelly-call.txt",
    "book_extract": "playbigger-category-design.txt"
  },
  "elements": [
    {
      "id": "tests-0904-001",
      "type": "validation_strategy",
      "title": "Discovery-to-Evaluation Conversion After Problem-First Sequencing",
      "validates_element": "ds-ui-0904-001",
      "journey_phases": ["discovery", "evaluation"],
      "problem": "The problem-first discovery sequence (ds-ui-0904-001) restructures outreach to lead with risk awareness before mentioning any guest or platform feature. If this sequence fails, discovery conversion will remain near zero for direct-work-preference hosts like Kelly.",
      "solution": "A/B test the problem-first call script against the current solution-first script with hosts who express a direct-work preference during discovery outreach.",
      "evidence": [
        {
          "source": "kelly-call.txt, 02:09",
          "type": "host_call",
          "quote": "I'd prefer to just kind of work with individuals directly... I wanted someone who I can get a little bit more information as far as their nursing and just being more confident in who they were.",
          "insight": "Kelly rejected at discovery because no problem was surfaced. The test must measure whether problem-first sequencing prevents this specific rejection pattern."
        },
        {
          "source": "playbigger-category-design.txt, Ch 2",
          "type": "book",
          "quote": "Winning companies today market the problem, not just the solution.",
          "insight": "Play Bigger's thesis is that problem marketing creates the category. The test validates whether this thesis holds for Split Lease's host discovery calls."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "Split discovery outreach calls into two groups: Group A uses the current script (lead with guest match: 'I'm working with Landon who needs a room'). Group B uses the problem-first script (lead with risk question: 'Have you ever had a tenant stop paying or leave early? What happened?' followed by category frame, then guest proof). Minimum sample size: 50 calls per group targeting hosts who have previously listed on nursing/travel nurse sites.",
      "success_criteria": "Group B achieves a 30% or higher discovery-to-evaluation conversion rate among hosts who express a direct-work preference, compared to the current near-zero baseline in Group A. Evaluation is defined as the host agreeing to hear more about the category or requesting additional information about the guarantee.",
      "failure_meaning": "If Group B does not outperform Group A, the problem-first principle is correct but the specific wording or sequencing is wrong. Iterate on the risk question (try different risk scenarios: non-payment, property damage, early departure) and the timing of the category reveal. Failure does NOT invalidate the problem-first principle -- it invalidates the specific implementation.",
      "implementation_hint": "Train 3-5 agents on the problem-first script. Record all calls (with consent). Score each call on: (1) whether the risk question was asked before any guest mention, (2) whether the host acknowledged the risk, (3) whether the host asked a follow-up question about the guarantee, (4) whether the host agreed to hear more. Compare scores across groups."
    },
    {
      "id": "tests-0904-002",
      "type": "validation_strategy",
      "title": "Category Recall Test After Vocabulary Replacement",
      "validates_element": "ds-ui-0904-003",
      "journey_phases": ["discovery", "evaluation"],
      "problem": "The category-defining vocabulary (ds-ui-0904-003) replaces intermediation terms ('matchmaker,' 'guest/host') with outcome terms ('guaranteed-income hosting,' 'verified occupant'). If the vocabulary change does not alter host recall, the replacement is cosmetic, not category-defining.",
      "solution": "Measure category recall accuracy in hosts contacted with old vs. new vocabulary at 7 days and 30 days post-call.",
      "evidence": [
        {
          "source": "kelly-call.txt, 01:14",
          "type": "host_call",
          "quote": "I work basically as a matchmaker between like these types of guests and hosts.",
          "insight": "Bryant's vocabulary set Kelly's mental category as 'intermediary.' The test measures whether new vocabulary sets the mental category as 'guaranteed income.'"
        },
        {
          "source": "playbigger-category-design.txt, Ch 1 (Jawbone)",
          "type": "book",
          "quote": "Three times Jawbone came up with cool innovations that created a brand-new product category. But three times, Jawbone failed to develop and dominate the category it created.",
          "insight": "Jawbone's vocabulary failure (perceived as 'just another wireless headset') is the validation target. If hosts recall 'matchmaker' instead of 'guaranteed income,' the vocabulary has failed."
        }
      ],
      "priority": "high",
      "validation_method": "manual_review",
      "test_description": "Contact 20 hosts using the old vocabulary ('matchmaker between guests and hosts') and 20 hosts using the new vocabulary ('guaranteed-income hosting manager'). At 7 days and 30 days post-call, a different agent contacts each host with a neutral question: 'You spoke with someone from Split Lease recently. What do you remember about what they do?' Record the host's response verbatim. Code each response as 'outcome framing' (guaranteed income, protected hosting, payment guarantee) or 'process framing' (matchmaker, find tenants, connect guests and hosts, marketplace).",
      "success_criteria": "60% or more of hosts contacted with the new vocabulary describe Split Lease in outcome terms at 7 days, and 40% or more at 30 days. Hosts contacted with old vocabulary serve as the control baseline -- expect near-zero outcome framing.",
      "failure_meaning": "If new vocabulary does not improve outcome recall, the specific replacement terms may be too complex or unfamiliar. 'Guaranteed-income hosting' is a 3-word compound noun that may not stick in casual recall. Test shorter alternatives: 'guaranteed hosting,' 'income-protected hosting,' or simply 'guaranteed rent.'",
      "implementation_hint": "The recall test call should be brief (under 60 seconds) and non-promotional. Frame it as a research call: 'We are improving our outreach and want to understand what stuck with you.' Do not prompt with any vocabulary -- let the host recall freely. Record and transcribe all responses for coding."
    },
    {
      "id": "tests-0904-003",
      "type": "validation_strategy",
      "title": "Risk-Contrast Card Engagement and Follow-Up Rate",
      "validates_element": "ds-ui-0904-008",
      "journey_phases": ["discovery", "evaluation"],
      "problem": "The risk-contrast comparison card (ds-ui-0904-008) is designed to surface risk dimensions that direct-hosting hosts have never considered. If hosts do not engage with the card or do not ask follow-up questions after viewing it, the card is not displacing their incumbent anchor.",
      "solution": "Deploy the risk-contrast card as a digital touchpoint (email or landing page) for hosts who expressed direct-work preference and measure engagement and follow-up rates.",
      "evidence": [
        {
          "source": "kelly-call.txt, 02:09",
          "type": "host_call",
          "quote": "I'd prefer to just kind of work with individuals directly.",
          "insight": "Kelly evaluated her approach on one dimension only (trust through vetting). The card surfaces four dimensions. The test measures whether multi-dimensional exposure changes host behavior."
        },
        {
          "source": "playbigger-category-design.txt, Ch 2 (Anchoring Effect)",
          "type": "book",
          "quote": "In category dynamics, an early company that solves the problem will win a powerful place in customers' minds. It becomes an anchor.",
          "insight": "The card must displace the direct-hosting anchor by introducing new dimensions. Success = host asks about a dimension they had not previously considered."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Send the risk-contrast card to 100 hosts who previously declined or expressed direct-work preference. Deliver via email with the card embedded as HTML. Track: (1) email open rate, (2) time spent on card (heatmap or scroll depth if on a landing page), (3) which rows were hovered/tapped (on the landing page version), (4) whether the host replied to the email or called the agent within 7 days of viewing. Control group: 100 similar hosts who receive a standard follow-up email with no risk-contrast card.",
      "success_criteria": "40% or more of hosts who view the risk-contrast card ask a follow-up question about guarantees or protection (via reply, call, or link tap). This compares against a baseline of near-zero re-engagement from standard follow-up emails. Secondary: hosts engage with 3 or more of the 4 risk rows, indicating multi-dimensional processing.",
      "failure_meaning": "If the card does not generate follow-up questions, the risk dimensions may be too abstract. Replace generic dimensions ('If tenant stops paying') with personalized ones that reference the host's specific market ('If a travel nurse in Troy stops paying in month two'). Alternatively, the card may need to be delivered via SMS link rather than email to match the channel hosts are accustomed to.",
      "implementation_hint": "Build two versions of the landing page: one with hover/tap analytics on each row (for desktop/tablet), one with tap-to-expand for mobile. Use the L4 row-level emphasis interaction pattern (behaves-0904-002) on the landing page version. Track which risk dimension generates the most engagement -- this reveals which risk resonates most with direct-work-preference hosts."
    },
    {
      "id": "tests-0904-004",
      "type": "validation_strategy",
      "title": "Vetting Evidence Trail Conversion for Control-Oriented Hosts",
      "validates_element": "ds-ui-0904-009",
      "journey_phases": ["evaluation", "proposal_mgmt"],
      "problem": "The vetting evidence trail (ds-ui-0904-009) shows expandable verification cards with source attribution and dates. If control-oriented hosts do not engage with the evidence or do not convert after viewing it, the transparency approach is insufficient to overcome Choice Supportive Bias toward direct vetting.",
      "solution": "Present the vetting evidence trail to hosts who have previously declined due to vetting/control concerns and measure whether it changes their disposition.",
      "evidence": [
        {
          "source": "kelly-call.txt, 02:09",
          "type": "host_call",
          "quote": "I wanted someone who I can get a little bit more information as far as their nursing and just being more confident in who they were.",
          "insight": "Kelly's specific information needs (nursing credentials, personal identity) are addressed by the Employment and Identity cards. The test measures whether seeing this evidence changes disposition."
        },
        {
          "source": "playbigger-category-design.txt, Ch 2 (Choice Supportive Bias)",
          "type": "book",
          "quote": "Once you've committed to a product or service in a new category, you're likely to give positive qualities to an option you've chosen just because you've chosen it.",
          "insight": "The evidence trail must be undeniably more thorough than personal vetting to overcome Choice Supportive Bias. The test measures whether 'undeniable' is achieved."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 10 hosts who host travel nurses and currently use nursing sites (Furnished Finder, etc.) for direct tenant finding. Show each host a mock vetting evidence trail for a fictional guest, including: (1) Employment verification with employer name and license check, (2) Identity verification with government ID match, (3) Payment history across 3 prior leases with zero missed payments, (4) Background check with clear result. After viewing, ask: 'How does this compare to the information you currently get through nursing sites? Would you feel more or less confident about this guest than a guest you found directly?' Record responses. Then ask: 'If this level of verification was available before you saw any guest, would you consider using a platform that provided it?'",
      "success_criteria": "25% or more of tested hosts say the vetting trail provides MORE information than they can obtain through direct vetting, AND express willingness to consider platform-mediated hosting if this evidence level is standard. The specific conversion signal is the host saying something equivalent to 'I get more here than I could get on my own.'",
      "failure_meaning": "If hosts still prefer direct vetting after seeing the trail, the evidence may be insufficient in a specific dimension. Check which cards hosts engage with most (Employment, Identity, Payment, Background) and which they dismiss. If hosts say 'I would still want to talk to them myself,' the trail is not replacing direct vetting but could be positioned as a complement to it -- see the Dual Verification Transparency element from the Dimitri run.",
      "implementation_hint": "Use the L4 progressive disclosure pattern (behaves-0904-003): all cards collapsed by default, click-to-expand. Track which cards are expanded first, which are expanded at all, and how long hosts spend on expanded detail sections. If Employment is consistently expanded first and for the longest duration, it validates Kelly's specific concern about professional credentials."
    },
    {
      "id": "tests-0904-005",
      "type": "validation_strategy",
      "title": "Post-Rejection Seed Card Category Recall Displacement",
      "validates_element": "ds-ui-0904-012",
      "journey_phases": ["discovery", "retention"],
      "problem": "The post-rejection category seed card (ds-ui-0904-012) is designed to replace the incorrect category label ('matchmaker') with the correct one ('guaranteed-income hosting') in the declined host's memory. If the seed card does not displace the original anchor, the re-engagement effort will face the same category-level rejection on future contact.",
      "solution": "Send the seed card to declined hosts and measure category recall displacement at 3 and 6 months.",
      "evidence": [
        {
          "source": "kelly-call.txt, 01:14",
          "type": "host_call",
          "quote": "I work basically as a matchmaker between like these types of guests and hosts.",
          "insight": "This is the anchor to displace. The seed card's headline ('Guaranteed-Income Hosting') must replace 'matchmaker' in Kelly's memory."
        },
        {
          "source": "playbigger-category-design.txt, Ch 2 (Anchoring Effect)",
          "type": "book",
          "quote": "The first offer put on the table in a negotiation has a powerful impact on any other offer that comes after.",
          "insight": "The seed card is a counter-anchor. The test measures whether a single visual artifact can displace a verbal anchor set during a live conversation."
        }
      ],
      "priority": "medium",
      "validation_method": "manual_review",
      "test_description": "For all hosts who decline at discovery over a 3-month period, send the seed card (via SMS at T+25 minutes per the L4 delivery sequence). At 3 months and 6 months, have an agent make a brief research call: 'You spoke with us a while back. What do you remember about what Split Lease does?' Code responses as 'old category' (matchmaker, connects people, finds tenants) or 'new category' (guaranteed income, payment protection, verified hosting). Compare recall between hosts who received the seed card and those who did not (historical baseline).",
      "success_criteria": "50% or more of hosts who received the seed card describe Split Lease in new-category terms at 3 months. 30% or more maintain new-category recall at 6 months. Historical baseline (hosts who declined without receiving the seed card) is expected to show near-zero new-category recall.",
      "failure_meaning": "If the seed card does not displace the anchor, it may be: (1) not visually prominent enough (the category name is not the overwhelming focal point), (2) delivered at the wrong time (25 minutes may be too soon or too late), (3) competing with a stronger verbal anchor that a single visual artifact cannot overcome. Consider testing a more aggressive counter-anchor: a 30-second video from the agent explaining guaranteed-income hosting, delivered via the same SMS channel.",
      "implementation_hint": "Track seed card delivery, SMS link tap rate, and page dwell time. If tap rate is below 20%, the SMS copy may need iteration. If tap rate is above 20% but recall displacement is low, the card's visual design may not be creating sufficient anchor strength -- test a bolder headline treatment (larger font, different color, animation on page load)."
    },
    {
      "id": "tests-0904-006",
      "type": "validation_strategy",
      "title": "Local Social Proof Notification Re-engagement Rate",
      "validates_element": "ds-ui-0904-011",
      "journey_phases": ["retention", "discovery"],
      "problem": "The local social proof notification (ds-ui-0904-011) relies on Play Bigger's Groupthink Bias to trigger re-engagement in pre-category hosts. If the notification does not generate re-engagement (link taps, agent calls, referrals), the local social proof mechanism is either ineffective or the notifications are being perceived as marketing.",
      "solution": "Deploy milestone-triggered notifications to pre-category hosts in micro-markets where at least 2 hosts have joined, and measure re-engagement rates.",
      "evidence": [
        {
          "source": "kelly-call.txt, 04:12-04:49",
          "type": "host_call",
          "quote": "If anything changes or I have an associate that has a property or unit that would work, I will definitely give you... I'm right outside, I'm sort of in like Metro Detroit right outside Detroit City.",
          "insight": "Kelly left the door open and identified her geographic market. The notification leverages both: geographic identity and non-zero openness."
        },
        {
          "source": "playbigger-category-design.txt, Ch 2 (Groupthink Bias)",
          "type": "book",
          "quote": "Groupthink Bias describes a tendency to believe things because other people do.",
          "insight": "The notification's core mechanism is Groupthink Bias. The test validates whether this mechanism actually changes behavior in pre-category hosts."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Identify 5 micro-markets where at least 2 hosts have joined. In each micro-market, identify pre-category hosts who declined at discovery. Send the local social proof notification (per L4 milestone-triggered delivery) via SMS. Track: (1) SMS delivery rate, (2) link tap rate, (3) page dwell time, (4) whether the host contacts the agent within 14 days, (5) whether the host makes a referral. Run for 6 months across all available micro-markets.",
      "success_criteria": "10-15% of pre-category hosts who receive a local social proof notification re-engage within 6 months (defined as: tapping the SMS link AND either contacting the agent, visiting the platform, or making a referral). Secondary: referral rate of 5% or higher (pre-category hosts referring their associates).",
      "failure_meaning": "If re-engagement is below 5%, the notifications may be: (1) perceived as marketing despite the informational framing, (2) arriving in micro-markets where the host count is too low to trigger Groupthink Bias (2 hosts may not be enough -- test with thresholds of 3, 5, and 10), (3) failing because the category itself has not been established (social proof requires at least some category awareness to be effective). Consider supplementing with outcome data (total guaranteed income paid in the micro-market) to make the social proof more concrete.",
      "implementation_hint": "Start with the micro-market that has the highest host density. Measure at each milestone (2, 3, 5 hosts) to identify the minimum viable social proof threshold. If re-engagement spikes at 5 hosts but not at 2, the Groupthink Bias threshold for Split Lease's market is 5 local hosts, not 2."
    },
    {
      "id": "tests-0904-007",
      "type": "validation_strategy",
      "title": "Problem-Absorption Gate Dwell Time Calibration",
      "validates_element": "behaves-0904-001",
      "journey_phases": ["discovery"],
      "problem": "The progressive disclosure gate (behaves-0904-001) enforces a 4-second dwell on the Problem tier before revealing the Category tier. This timing is a heuristic. Too long causes bounce; too short fails to activate problem recognition.",
      "solution": "A/B test different dwell thresholds to find the optimal duration that maximizes problem recognition without increasing bounce rate.",
      "evidence": [
        {
          "source": "kelly-call.txt, 00:35-02:09",
          "type": "host_call",
          "quote": "Bryant delivers solution-data for 94 seconds before Kelly rejects. No problem was surfaced during those 94 seconds.",
          "insight": "The call had zero problem-dwell time. The gate introduces forced problem-dwell. The question is: how much dwell is enough?"
        },
        {
          "source": "playbigger-category-design.txt, Ch 2 (Anchoring Effect)",
          "type": "book",
          "quote": "The first offer put on the table in a negotiation has a powerful impact on any other offer that comes after.",
          "insight": "The problem must be the first 'offer on the table.' Dwell time ensures the problem registers as the anchor before the solution appears."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "Deploy the three-tier discovery outreach page (Problem > Category > Proof) with the progressive disclosure gate. Test three dwell thresholds: 2 seconds (Group A), 4 seconds (Group B), and 6 seconds (Group C). Minimum 200 visitors per group from cold outreach campaigns targeting host prospects. Measure: (1) bounce rate per group, (2) scroll depth past Problem tier, (3) time spent on Category tier after reveal, (4) click-through to any CTA or contact action, (5) whether the host contacts the agent within 7 days.",
      "success_criteria": "The optimal dwell time is the threshold that produces the lowest bounce rate AND the highest downstream engagement (Category tier dwell + CTA click-through). If 4 seconds has 15% bounce and 25% CTA click, but 2 seconds has 10% bounce and 20% CTA click, 4 seconds is optimal because the engagement lift outweighs the bounce increase. If 6 seconds has 25% bounce, it is too long regardless of engagement.",
      "failure_meaning": "If all thresholds produce high bounce rates (>30%), the gate mechanism itself may be too aggressive for cold-traffic contexts. Consider replacing the blur/collapse gate with a softer approach: the Category and Proof tiers are visible but visually de-emphasized (low opacity, not blurred) until the Problem tier dwell threshold is met. This preserves the visual sequencing without the 'broken page' impression.",
      "implementation_hint": "Implement the gate with a URL parameter that sets the dwell threshold (?dwell=2000, ?dwell=4000, ?dwell=6000). This allows rapid iteration without redeploying. Add analytics events for: gate_start, gate_opened (with timestamp), category_revealed, proof_revealed, and any click/tap events."
    },
    {
      "id": "tests-0904-008",
      "type": "validation_strategy",
      "title": "Respectful Departure Warm Recall Test",
      "validates_element": "feels-0904-004",
      "journey_phases": ["discovery", "retention"],
      "problem": "The respectful departure element (feels-0904-004) prescribes a specific emotional sequence after rejection: validate, plant category seed, close on personal connection. If this sequence does not produce warm recall in the declined host, re-engagement efforts will face the same rejection on future contact.",
      "solution": "Compare recall sentiment in hosts who experienced a respectful departure vs. hosts who experienced the current unstructured departure.",
      "evidence": [
        {
          "source": "kelly-call.txt, 03:02-04:49",
          "type": "host_call",
          "quote": "Bryant: 'I hear you for that. And I definitely understand where you're coming from.' Then continues pitching Landon. Later, the Michigan connection creates warmth.",
          "insight": "The current departure is mixed: validation followed by continued pitching followed by geographic warmth. The test measures whether a structured departure (validate -> seed -> warm close) produces more uniformly warm recall."
        },
        {
          "source": "playbigger-category-design.txt, Ch 2 (6-10 Law)",
          "type": "book",
          "quote": "It always takes time to define a category, develop it, and change the way we see a problem and its solution. And that time is measured in years.",
          "insight": "The 6-10 law means re-engagement will occur months later. The host's emotional memory at that point determines whether they are open or closed. Warm recall = open. Cold recall = closed."
        }
      ],
      "priority": "medium",
      "validation_method": "manual_review",
      "test_description": "Train agents on the respectful departure sequence: (1) validate with zero resistance ('That makes complete sense'), (2) plant category seed in one sentence ('One thing worth knowing -- we guarantee income even if a tenant stops paying'), (3) close on personal connection ('Great chatting with a fellow Troy person'). For 30 declined hosts, use the structured departure. For 30 declined hosts (historical baseline), use the current unstructured approach. At 30 days, have a neutral party call each host: 'You spoke with Split Lease about a month ago. I am calling to ask -- if we ever had something relevant for your area, would you be open to hearing about it?' Record: (1) the host's immediate response (positive, neutral, negative), (2) the host's recall of the prior interaction (warm, neutral, cold), (3) whether the host volunteers any referral information.",
      "success_criteria": "70% or more of hosts who experienced the structured departure respond positively or neutrally to the 30-day follow-up (vs. expected 40-50% for unstructured). 30% or more describe their recall of the prior interaction as warm. 15% or more volunteer referral information without prompting.",
      "failure_meaning": "If structured departure does not improve recall sentiment, the emotional sequence may be correct but the delivery may require more natural execution. The 'plant category seed' step in particular could feel scripted if agents deliver it mechanically. Refine training to emphasize natural delivery: the seed should feel like a genuine aside, not a script element. Alternatively, the 30-day follow-up interval may be too short or too long for the recall effect to be measurable.",
      "implementation_hint": "Record all departure sequences (with consent) for quality review. Score each on adherence to the three-step sequence and naturalness of delivery. Exclude calls where the agent deviated significantly from the structured departure. The test is validating the sequence design, not agent execution -- filter for clean implementations."
    },
    {
      "id": "tests-0904-009",
      "type": "validation_strategy",
      "title": "Vocabulary Enforcement Agent Compliance and Outcome Correlation",
      "validates_element": "behaves-0904-006",
      "journey_phases": ["discovery", "evaluation"],
      "problem": "The vocabulary enforcement state machine (behaves-0904-006) provides real-time guidance to agents during calls. If agents do not use the category-defining vocabulary despite the guidance, the system is ineffective. If agents use it but outcomes do not improve, the vocabulary itself may not be the differentiator.",
      "solution": "Measure agent vocabulary compliance with and without the CRM panel, and correlate vocabulary usage with discovery-to-evaluation conversion.",
      "evidence": [
        {
          "source": "kelly-call.txt, 01:14",
          "type": "host_call",
          "quote": "I work basically as a matchmaker between like these types of guests and hosts.",
          "insight": "Bryant used the old vocabulary naturally because no guidance system existed. The test measures whether a guidance system changes agent behavior."
        },
        {
          "source": "playbigger-category-design.txt, Ch 2 (Magic Triangle)",
          "type": "book",
          "quote": "Category design is the mindful creation and development of a new market category, designed so the category will pull in customers who will then make the company its king.",
          "insight": "Vocabulary is the primary weapon in category creation. The test validates whether systematic vocabulary enforcement at the agent level improves category establishment at the host level."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Deploy the CRM vocabulary panel for half of the agent team (Group A). The other half operates without it (Group B). Run for 8 weeks. For Group A, track: (1) post-call vocabulary reports (old terms used vs. new terms used), (2) pre-call highlight engagement (did the agent view the Category Opener?), (3) discovery-to-evaluation conversion rate. For Group B, manually transcribe and code 20% of calls for vocabulary usage. Compare vocabulary usage rates and conversion rates between groups.",
      "success_criteria": "Group A agents use category-defining vocabulary 70% or more of the time (vs. expected 20-30% for Group B). Group A's discovery-to-evaluation conversion rate is at least 20% higher than Group B's. The correlation between vocabulary compliance and conversion must be statistically significant (p < 0.05).",
      "failure_meaning": "If agents use the vocabulary but conversion does not improve, the vocabulary change is necessary but not sufficient -- other call dynamics (tone, timing, rapport) may matter more. If agents do not use the vocabulary despite the panel, the panel's UX may need redesign (too subtle, too distant from the agent's primary workflow, or the pulses are ignored). Consider a more prominent intervention: a full-screen vocabulary prompt during the pre-call screen that requires agent acknowledgment before dialing.",
      "implementation_hint": "Start with speech-to-text detection in passive mode (detect but do not pulse) for the first 2 weeks to establish baseline vocabulary usage. Then activate the pulse feature and measure the delta. This isolates the panel's visibility effect from the pulse's correction effect."
    },
    {
      "id": "tests-0904-010",
      "type": "validation_strategy",
      "title": "Vulnerability Activation Without Defensive Backlash",
      "validates_element": "feels-0904-001",
      "journey_phases": ["discovery"],
      "problem": "The vulnerability activation element (feels-0904-001) prescribes surfacing the host's uninsured risk without triggering fear or defensiveness. The boundary between productive vulnerability (host recognizes a gap) and counterproductive fear (host feels attacked) is narrow and host-dependent.",
      "solution": "Test the risk question with control-oriented hosts in moderated interviews and measure emotional response on a vulnerability-vs-defensiveness spectrum.",
      "evidence": [
        {
          "source": "kelly-call.txt, 02:09",
          "type": "host_call",
          "quote": "I'd prefer to just kind of work with individuals directly... being more confident in who they were.",
          "insight": "Kelly's emotional state was settled and confident. The risk question must crack this confidence gently, not shatter it."
        },
        {
          "source": "playbigger-category-design.txt, Ch 1 (Uber)",
          "type": "book",
          "quote": "Uber made all of us aware that we had a taxi problem.",
          "insight": "Uber activated vulnerability (standing in the rain, uncertain if a taxi would come) without activating fear. The test measures whether Split Lease's risk question achieves the same balance."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 15 hosts who currently rent directly to travel nurses or short-term tenants via nursing sites. Conduct 30-minute moderated interviews. In each session: (1) Ask about their current hosting approach and satisfaction level (baseline emotional state). (2) Present the risk question in three variants: Variant A (gentle): 'You manage your properties directly, which gives you real control. One thing I am curious about -- if a tenant stopped paying in month two, what would you do?' Variant B (moderate): 'What happens when things go wrong with a direct tenant? Non-payment, damage, early departure -- who absorbs the cost?' Variant C (strong): 'Direct hosting means you carry all the risk. If a tenant defaults, stops paying, or damages the property, you have no recourse.' (3) After each variant, measure: emotional response (coded as vulnerability, curiosity, defensiveness, or dismissal), whether the host asks a follow-up question, and the host's stated interest in hearing about an alternative.",
      "success_criteria": "The optimal variant produces vulnerability or curiosity in 60% or more of hosts AND defensiveness in fewer than 20%. Variant A is expected to produce the highest curiosity-to-defensiveness ratio. If Variant A produces vulnerability in only 30% of hosts but zero defensiveness, it is preferable to Variant B which might produce vulnerability in 50% but defensiveness in 30%.",
      "failure_meaning": "If all variants produce defensiveness in more than 30% of hosts, the risk question approach itself may be unsuitable for control-oriented hosts. Alternative: instead of asking about risk, show evidence of risk (anonymized case study of a non-paying tenant in the host's market). Showing evidence is less confrontational than asking a question because the host evaluates someone else's situation, not their own.",
      "implementation_hint": "Use a within-subjects design: each host hears all three variants (randomized order) applied to different risk scenarios (non-payment, property damage, early departure). This lets you measure both variant effectiveness and scenario resonance. Video-record sessions (with consent) for emotional response coding by two independent reviewers."
    },
    {
      "id": "tests-0904-011",
      "type": "validation_strategy",
      "title": "Seed Card Delivery Timing Optimization",
      "validates_element": "behaves-0904-004",
      "journey_phases": ["discovery", "retention"],
      "problem": "The post-rejection delivery sequence (behaves-0904-004) sends the seed card at T+25 minutes via SMS. This timing is based on the hypothesis that 0-20 minutes is the 'rejection processing' window (too soon) and 24+ hours is the anchor hardening window (too late). The 25-minute window is untested.",
      "solution": "A/B test different delivery times to find the optimal window for counter-anchor displacement.",
      "evidence": [
        {
          "source": "kelly-call.txt, 04:12-04:59",
          "type": "host_call",
          "quote": "Kelly: 'I have your number, if anything changes.' Bryant: 'Thanks so much.' The call ends warmly.",
          "insight": "The call ended at approximately 04:59. The SMS must arrive after the rejection affect dissipates but before the category anchor hardens."
        },
        {
          "source": "playbigger-category-design.txt, Ch 2 (Anchoring Effect)",
          "type": "book",
          "quote": "The first offer put on the table in a negotiation has a powerful impact on any other offer that comes after.",
          "insight": "Anchor displacement has a temporal window. The test identifies that window empirically."
        }
      ],
      "priority": "low",
      "validation_method": "a_b_test",
      "test_description": "For all hosts who decline at discovery over a 4-month period, randomly assign to three delivery time groups: Group A receives the seed card SMS at T+15 minutes, Group B at T+25 minutes, Group C at T+60 minutes. Track: (1) SMS link tap rate per group, (2) page dwell time per group, (3) category recall at 30-day follow-up (coded as old-category vs. new-category per tests-0904-005 methodology).",
      "success_criteria": "The delivery time that produces the highest combination of SMS link tap rate (>25%) and new-category recall at 30 days (>40%) is optimal. If T+15 and T+25 produce similar tap rates but T+25 produces better recall, the extra 10 minutes allows the rejection affect to dissipate enough for the counter-anchor to land.",
      "failure_meaning": "If no delivery time produces meaningful recall displacement, the SMS channel itself may be the problem. Test email delivery (embedded card, no link required) as an alternative. Alternatively, the seed card content may need to be delivered verbally at the end of the call (before the goodbye) rather than digitally after it. This would mean the 'respectful departure' element (feels-0904-004) should include the category seed as a spoken element, not a post-call artifact.",
      "implementation_hint": "Use the CRM's scheduling system to automate SMS sends at the designated intervals. Track delivery confirmation, tap, and dwell as conversion events. If the CRM supports link shorteners with analytics, use them to avoid third-party tracking dependencies."
    },
    {
      "id": "tests-0904-012",
      "type": "validation_strategy",
      "title": "Category Curiosity Activation Rate in Discovery",
      "validates_element": "feels-0904-003",
      "journey_phases": ["discovery"],
      "problem": "The category curiosity element (feels-0904-003) prescribes using an unfamiliar category name ('guaranteed-income hosting') to trigger curiosity in satisfied hosts. If the name does not trigger curiosity (because it is too complex, too corporate, or not different enough from existing categories), the emotional gateway to category adoption fails to open.",
      "solution": "Test the category name's curiosity-generating power against alternative names and against the current name ('matchmaker') in a rapid concept test.",
      "evidence": [
        {
          "source": "kelly-call.txt, 01:14",
          "type": "host_call",
          "quote": "I work basically as a matchmaker between like these types of guests and hosts.",
          "insight": "'Matchmaker' triggered recognition and classification, not curiosity. The test measures whether 'guaranteed-income hosting' triggers the opposite response."
        },
        {
          "source": "playbigger-category-design.txt, Ch 1",
          "type": "book",
          "quote": "They introduce the world to a new category of product or service. They replace our current point of view on the world with a new point of view.",
          "insight": "A new category name must introduce a new point of view. The test measures whether the name successfully introduces unfamiliarity."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 20 property owners who rent to short-term or mid-term tenants. In rapid concept testing (5 minutes per participant): Present four category names, one at a time, in randomized order: (A) 'We are a matchmaker between guests and hosts.' (B) 'We offer guaranteed-income hosting.' (C) 'We provide protected leasing for property owners.' (D) 'We manage guaranteed rent for your property.' After each name, ask: 'What do you think that means? What questions come to mind?' Code responses as: recognition (host immediately classifies it within an existing category), curiosity (host asks a follow-up question or says 'tell me more'), confusion (host says 'I do not understand what that means'), or dismissal (host expresses disinterest).",
      "success_criteria": "The optimal name produces curiosity in 50% or more of participants AND recognition (immediate classification) in fewer than 20%. 'Guaranteed-income hosting' (Option B) is expected to perform best because it combines a familiar concept (income) with an unfamiliar modifier (guaranteed, in the hosting context). If Option D ('guaranteed rent') outperforms B, adopt it -- shorter names may be more curiosity-inducing because they are faster to process.",
      "failure_meaning": "If all names produce recognition or confusion rather than curiosity, the category naming approach needs fundamental rethinking. Consider testing a question-based category introduction instead of a declarative name: 'What if your rental income was guaranteed regardless of what your tenant does?' Questions are inherently curiosity-generating because they create information gaps.",
      "implementation_hint": "Run this as a rapid remote test (Zoom or phone). Each session takes 5 minutes. Record all sessions. Have two independent coders classify responses. Inter-rater reliability must exceed 80% for the coding to be valid. If coding disagrees, add 'ambiguous' as a fifth category."
    },
    {
      "id": "tests-0904-013",
      "type": "validation_strategy",
      "title": "Journey-Level Validation: Discovery-Through-Retention Emotional Arc for Declined Hosts",
      "validates_element": "all L1-L5 elements",
      "journey_phases": ["discovery", "evaluation", "retention"],
      "problem": "All elements in this run address a single scenario: a host who declines at discovery. The full emotional arc (curiosity -> vulnerability -> relief -> warmth -> belonging) spans months and multiple touchpoints. No individual element test validates whether the end-to-end arc produces eventual conversion.",
      "solution": "Track a cohort of declined hosts through the complete arc over 6 months and measure cumulative conversion.",
      "evidence": [
        {
          "source": "kelly-call.txt, full transcript",
          "type": "host_call",
          "quote": "Kelly declined but left the door open: 'If anything changes or I have an associate that has a property or unit that would work, I will definitely give you.'",
          "insight": "Kelly is the prototypical pre-category host. The journey-level test validates whether the full arc (problem-first call -> seed card -> dormancy -> social proof notification -> re-engagement) produces eventual conversion for hosts like her."
        },
        {
          "source": "playbigger-category-design.txt, Ch 2 (6-10 Law)",
          "type": "book",
          "quote": "It always takes time to define a category, develop it, and change the way we see a problem and its solution. And that time is measured in years.",
          "insight": "The 6-10 law means validation requires longitudinal tracking. A 6-month cohort study is a minimum viable timeline for measuring category adoption among pre-category hosts."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Establish a cohort of 100 hosts who decline at discovery over a 3-month enrollment period. Apply the full arc: (1) problem-first discovery call, (2) respectful departure with category seed, (3) seed card SMS at T+25 minutes, (4) 30-day dormancy, (5) local social proof notifications at geographic milestones, (6) agent-initiated re-engagement when the host taps a notification link. Track each host through all touchpoints over 6 months. Measure: (1) seed card tap rate, (2) social proof notification tap rate, (3) agent re-engagement calls resulting from notification taps, (4) eventual conversion (host agrees to list a property or makes a referral that converts), (5) total referrals generated by pre-category hosts.",
      "success_criteria": "10% or more of the cohort converts (lists a property or generates a converting referral) within 6 months. 20% or more re-engages at least once (taps a notification, calls the agent, or responds to an outreach). The cost per acquisition for re-engaged declined hosts should be lower than the cost of cold-acquiring new hosts, because the relationship and category seed are already established.",
      "failure_meaning": "If conversion is below 5% at 6 months, the full arc may be too passive. Consider adding an active intervention: at Month 3, send a brief case study of a host in the same market who experienced the problem (non-payment, property damage) and used guaranteed-income hosting to resolve it. This moves from social proof (others are doing this) to problem proof (this is what happens without it). If referrals are high but personal conversion is low, pre-category hosts may be better used as referral channels than as direct conversion targets -- adjust the strategy accordingly.",
      "implementation_hint": "Build a dashboard that tracks each cohort member through the arc stages: DECLINED -> SEED_CARD_SENT -> SEED_CARD_VIEWED -> DORMANT -> SOCIAL_PROOF_SENT -> SOCIAL_PROOF_VIEWED -> RE_ENGAGED -> CONVERTED. This pipeline visualization reveals where the arc breaks down. If most hosts stall at DORMANT -> SOCIAL_PROOF_SENT (meaning no milestones are reached in their micro-market), the bottleneck is local adoption density, not the arc design."
    }
  ]
}