{
  "lens": {
    "guest_call": "Susan  Bryant customer call.txt",
    "book_extract": "donnorman-affordances-signifiers-feedback.txt"
  },
  "elements": [
    {
      "id": "tests-001",
      "type": "validation_strategy",
      "title": "System Image Comprehension Test",
      "validates_element": "works-001",
      "journey_phases": ["discovery", "search"],
      "problem": "The platform may still fail to communicate what Split Lease is without human mediation. Susan misidentified it as a real estate brokerage (4:55). If the system image remains incoherent, first-time visitors will bounce before understanding the service.",
      "solution": "Run a five-second usability test with 20 participants who have never heard of Split Lease. Show the landing page for 5 seconds, then ask: 'What does this service do?' Code responses for accuracy against the target conceptual model: part-time housing where you book specific nights.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 4:55",
          "type": "guest_call",
          "quote": "Real estate Agent.",
          "insight": "Susan defaulted to the closest familiar category because the platform had no perceivable signifier for its actual model. The 5-second test directly measures whether the new system image prevents this miscategorization."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 The System Image",
          "type": "book",
          "quote": "When the system image is incoherent or inappropriate, then the user cannot easily use the device.",
          "insight": "Norman's system image principle is directly testable: if the user cannot describe the service after seeing its interface, the system image has failed."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 20 participants unfamiliar with Split Lease. Show the landing page for exactly 5 seconds. Ask: 'What does this service do?' and 'Who is this service for?' Record open-ended responses. A second round shows the page for 30 seconds and asks the same questions, measuring whether extended exposure improves comprehension.",
      "success_criteria": "At least 70% of participants can correctly identify that Split Lease offers part-time or partial-week housing after a 5-second exposure. At least 85% after 30 seconds. Zero participants should misidentify it as a real estate brokerage, hotel, or traditional Airbnb-style rental.",
      "failure_meaning": "If fewer than 70% comprehend the model at 5 seconds, the landing page headline and identity signifier are failing to communicate the core concept. This means the system image is still incoherent and the platform will continue to depend on human intermediaries for every conversion.",
      "implementation_hint": "Use a remote unmoderated tool like Maze or UsabilityHub. Recruit from a general population panel, not existing users. Screen out anyone who has used non-traditional housing platforms. Run both the 5-second and 30-second variants to diagnose whether the failure is in immediate comprehension or in extended processing."
    },
    {
      "id": "tests-002",
      "type": "validation_strategy",
      "title": "Trust Signifier Visibility and Conversion Impact Test",
      "validates_element": "works-002",
      "journey_phases": ["listing_evaluation", "negotiation", "acceptance"],
      "problem": "Trust signifiers (verified photos, cancellation guarantee, host identity card) may be present but not perceived or may not drive conversion. Susan demanded physical inspection three times; the substitute signifiers must be visible enough and credible enough to replace that demand.",
      "solution": "Run an A/B test comparing the current listing page against the redesigned page with the layered trust verification stack (verified photo badge, cancellation guarantee badge, host identity card above the fold). Measure listing-to-proposal conversion rate for first-time unassisted guests.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 3:42-3:58",
          "type": "guest_call",
          "quote": "If I like the room in the bathroom, I can, should have pictures. And she said, it's okay. I'll give you the deposit right there.",
          "insight": "Susan explicitly coupled photos with readiness to pay. The A/B test measures whether surfacing verified photos and the guarantee badge produces the same conversion readiness at scale."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Affordances",
          "type": "book",
          "quote": "To be effective, affordances and anti-affordances have to be discoverable -- perceivable.",
          "insight": "The test validates discoverability: if the trust signifiers are present but conversion does not improve, they are not being perceived as trust evidence."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "Split traffic 50/50 between the current listing page and the redesigned page with the trust triad above the fold. Run for 4 weeks or until statistical significance is reached. Primary metric: listing-to-proposal conversion rate for first-time guests who have not spoken to an agent. Secondary metrics: time on listing page, scroll depth, photo gallery engagement rate.",
      "success_criteria": "The redesigned page achieves a listing-to-proposal conversion rate at least 25% higher than the current page for first-time unassisted guests. Photo gallery engagement rate exceeds 60%. Scroll depth shows the trust triad elements are viewed by at least 80% of visitors.",
      "failure_meaning": "If conversion does not improve significantly, either the trust signifiers are not being perceived (placement issue), not being believed (credibility issue), or trust is not the primary barrier (the problem is elsewhere in the funnel). Scroll depth and engagement data will diagnose which.",
      "implementation_hint": "Ensure the A/B test only measures first-time, unassisted visitors. Exclude users who arrived via agent referral links, since those users have already received human trust priming. Track photo gallery interactions (swipe count, zoom-in) as a proxy for the inspection behavior Susan described."
    },
    {
      "id": "tests-003",
      "type": "validation_strategy",
      "title": "Natural-Language Mapping Completion Rate Test",
      "validates_element": "works-003",
      "journey_phases": ["search", "proposal_creation"],
      "problem": "The preset-based selectors may not match the vocabulary or mental models of real guests. Susan thought in months and weekday names, but other guests may use different frames. If the presets do not cover common patterns, guests will abandon proposal creation.",
      "solution": "Run a moderated usability test where participants configure a stay proposal using the new preset-based selectors. Measure task completion rate, time to completion, and error rate. Follow up with a think-aloud to identify vocabulary mismatches.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 0:43",
          "type": "guest_call",
          "quote": "Like seven nights a week?",
          "insight": "Susan defaulted to full-time because the non-continuous model had no signifier in her vocabulary. The usability test measures whether the presets successfully communicate and enable the part-time model without human translation."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Mapping",
          "type": "book",
          "quote": "Natural mapping, taking advantage of spatial analogies, leads to immediate understanding.",
          "insight": "Norman's mapping principle is testable: if the controls exploit natural mappings, task completion should be fast and error-free. If not, participants will struggle with the translation from their vocabulary to the platform's input format."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 15 participants. Give each a scenario: 'You need a room in NYC for weekdays only, for about 2 months, starting next month.' Ask them to configure a proposal using the preset-based selectors. Measure: time to first correct configuration, number of errors (selecting wrong presets, confusion about night count), and whether participants use the translation summary bar. Follow with think-aloud debrief.",
      "success_criteria": "At least 80% of participants complete the configuration in under 60 seconds. Zero participants express confusion about the relationship between their selection and the night count. At least 70% use the translation summary bar to verify their configuration.",
      "failure_meaning": "If completion takes longer than 60 seconds, the presets do not match guests' natural vocabulary or the mapping between presets and summary is not immediately clear. If participants ignore the summary bar, it is not visually prominent enough or they do not understand its purpose.",
      "implementation_hint": "Include at least 3 participants whose housing need does not match any preset (e.g., 'Tuesday through Thursday every other week') to test the custom input path. Record screen interactions to identify where hesitation occurs. Pay special attention to whether participants understand what 'Weekdays' means in context."
    },
    {
      "id": "tests-004",
      "type": "validation_strategy",
      "title": "Negotiation Feedback Gap Tolerance Test",
      "validates_element": "works-004",
      "journey_phases": ["negotiation", "acceptance"],
      "problem": "The heartbeat notification intervals (6h, 12h, 24h) may be too frequent (annoying) or too infrequent (trust-eroding). The right interval depends on guest expectations, which vary. Too much feedback becomes the 'backseat driver' Norman warns about.",
      "solution": "Run a staged field test with three notification frequency variants: high frequency (4h/8h/16h), medium frequency (6h/12h/24h as designed), and low frequency (12h/24h/48h). Measure guest engagement during the waiting period and proposal abandonment rate.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 2:42-3:05",
          "type": "guest_call",
          "quote": "Can you hear me, Susan? Me? Yep. Can you hear me?",
          "insight": "A 20-second feedback gap on the call caused scrambling. The field test measures the digital equivalent: how long can the platform go silent before guests disengage or abandon?"
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Feedback",
          "type": "book",
          "quote": "Too much feedback can be even more annoying than too little.",
          "insight": "Norman warns about both extremes. The three-variant test identifies the optimal frequency by measuring both abandonment (too little) and notification dismissal/muting (too much)."
        }
      ],
      "priority": "high",
      "validation_method": "a_b_test",
      "test_description": "Randomly assign guests who submit proposals to one of three heartbeat notification frequency groups. Run for 6 weeks. Track: (1) proposal abandonment rate (guest withdraws before host responds), (2) guest engagement during waiting (app opens, status checks), (3) notification open rate and dismissal rate, (4) time from host response to guest action on the response.",
      "success_criteria": "Identify the frequency variant that produces the lowest proposal abandonment rate with a notification dismissal rate below 20%. The optimal variant should show guest engagement (status checks) distributed evenly across the waiting period rather than clustered at submission and abandonment.",
      "failure_meaning": "If all three variants show similar abandonment rates, feedback frequency is not the primary driver of waiting-period engagement -- the issue may be the content of the notifications rather than their timing. If the high-frequency variant has the lowest abandonment but the highest dismissal rate, guests want to know but are annoyed by how they are told.",
      "implementation_hint": "Segment results by whether the guest is a first-time user or a returning user, since first-time guests may need more frequent reassurance. Also segment by whether the guest arrived via agent referral (already has human trust priming) or self-service (no prior trust foundation)."
    },
    {
      "id": "tests-005",
      "type": "validation_strategy",
      "title": "Proxy Booking Discoverability and Completion Test",
      "validates_element": "works-005",
      "journey_phases": ["proposal_creation", "acceptance"],
      "problem": "The proxy booking toggle may not be discoverable, or the dual-profile form may be confusing. If Susan-type users cannot find or complete the proxy flow, they will either abandon or create incorrect single-user bookings.",
      "solution": "Run a moderated usability test where participants are given the scenario of booking on behalf of a friend. Measure whether they discover the proxy toggle without prompting, and whether they can complete the dual-profile form.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 2:34",
          "type": "guest_call",
          "quote": "I like to see the place and I can give you a deposit on her behalf.",
          "insight": "Susan explicitly describes the proxy pattern. The test measures whether the platform makes this pattern discoverable and completable without human guidance."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Signifiers",
          "type": "book",
          "quote": "People need some way of understanding the product or service they wish to use, some sign of what it is for.",
          "insight": "If the proxy toggle is not perceived as a signifier for 'booking for someone else,' users will not find it. The test measures signifier visibility."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 12 participants. Scenario: 'Your friend is coming to NYC from another state for medical appointments. She needs a room for weekdays for 2 months. You want to book and pay on her behalf.' Do not mention the proxy toggle. Observe whether participants find it, how they interpret it, and whether they can complete both profiles. Measure: discovery rate (found without prompting), time to complete dual-profile form, error rate on role assignment.",
      "success_criteria": "At least 75% of participants discover the proxy toggle without prompting. At least 80% of those who find it complete both profiles correctly. No participant creates a booking under their own name when intending to book for someone else.",
      "failure_meaning": "If discovery rate is below 75%, the toggle is not visually prominent enough or is positioned too late in the flow. If completion rate is low but discovery is high, the dual-profile form is confusing -- the split between coordinator and occupant information is not clear.",
      "implementation_hint": "Include a control group that is explicitly told 'look for the option to book for someone else' to measure the gap between discoverability (unguided) and usability (guided). If the gap is large, the feature works but cannot be found."
    },
    {
      "id": "tests-006",
      "type": "validation_strategy",
      "title": "Move-In Knowledge-in-the-World Adequacy Test",
      "validates_element": "works-006",
      "journey_phases": ["move_in"],
      "problem": "The chronological House Manual may not cover what guests actually need at move-in, or the sequence may not match the real physical experience. Susan's friend would arrive from Oregon with no local knowledge. If the manual fails at any step, the guest is stranded.",
      "solution": "Run a contextual inquiry with 8 guests during their actual first-night move-in. Observe whether they use the House Manual, which steps they access, where they get stuck, and whether the verification checklist matches their actual concerns.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 2:22",
          "type": "guest_call",
          "quote": "I have to go and see the space and just to make sure that everything is legit.",
          "insight": "Susan's demand for verification is the emotional driver of the move-in experience. The contextual inquiry tests whether the digital checklist satisfies this demand in the real physical environment."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Discoverability",
          "type": "book",
          "quote": "Two of the most important characteristics of good design are discoverability and understanding.",
          "insight": "At move-in, every action must be discoverable from the House Manual. The contextual inquiry reveals which actions are NOT discoverable -- gaps between the manual's content and the guest's real needs."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 8 guests who are checking into a Split Lease property for the first time. With their consent, observe their first 30 minutes via screen recording (phone) and brief check-in messages. Track: which House Manual steps they access, in what order, how long they spend per step, whether they complete the verification checklist, and any questions or issues not covered by the manual.",
      "success_criteria": "At least 75% of guests access the House Manual within 10 minutes of arrival. At least 60% complete the verification checklist. No guest encounters a critical issue (cannot access building, cannot find room, no Wi-Fi) that the manual fails to address. Guests rate the manual's helpfulness at 4+ out of 5.",
      "failure_meaning": "If guests do not access the manual, the pre-arrival notification failed to surface it. If they access it but skip the checklist, the checklist feels like a formality rather than a useful tool. If they encounter issues not covered, the manual has content gaps that require updating from real move-in data.",
      "implementation_hint": "Start with a small sample (8 guests) to identify major gaps before scaling. Collect qualitative feedback: 'Was anything missing?' and 'What would have helped?' Use this to iterate on the manual's content and sequence before broader rollout."
    },
    {
      "id": "tests-007",
      "type": "validation_strategy",
      "title": "Trust Verification Stack Visual Scan Test",
      "validates_element": "communicates-001",
      "journey_phases": ["listing_evaluation"],
      "problem": "The information hierarchy may not place trust signifiers where the guest's eye actually lands first. If guests scan past the verification badge or the cancellation guarantee, the layered trust stack fails as an information architecture even if its content is correct.",
      "solution": "Run an eye-tracking study on the redesigned listing page to verify that the trust triad (verified photos, guarantee badge, host card) occupies the first 6 seconds of the visual scan path.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 2:14-2:22",
          "type": "guest_call",
          "quote": "I do have to see the room before she puts her tickets.",
          "insight": "Susan's trust demand must be addressed by the first visual elements the guest perceives. Eye tracking validates whether the information hierarchy places trust evidence where the eye actually goes."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Signifiers",
          "type": "book",
          "quote": "Signifiers must be perceivable, else they fail to function.",
          "insight": "Norman's perceivability requirement is directly measurable with eye tracking: if the signifier is not fixated within the first scan, it is not perceivable in the practical sense."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 15 participants. Show the redesigned listing page while tracking eye movements. Measure: (1) time to first fixation on verified photo badge, (2) time to first fixation on cancellation guarantee badge, (3) time to first fixation on host identity card. Also measure dwell time on each trust element and the overall scan sequence.",
      "success_criteria": "All three trust triad elements receive a fixation within 6 seconds of page load for at least 80% of participants. The verified photo badge is fixated first (within 2 seconds). The guarantee badge is fixated within 4 seconds. The host card within 6 seconds. This matches the designed scan order.",
      "failure_meaning": "If the trust elements are not fixated within 6 seconds, they are positioned outside the natural scan path and must be repositioned. If the scan order is different from designed (e.g., price is fixated before the guarantee badge), the visual hierarchy is not establishing trust-first ordering.",
      "implementation_hint": "Use a remote eye-tracking tool or lab-based setup. Include both mobile and desktop variants since the visual hierarchy differs by viewport. Compare scan patterns between participants primed with a trust-anxious scenario ('You are booking for a friend flying from Oregon') and a neutral scenario to see if anxiety changes the scan path."
    },
    {
      "id": "tests-008",
      "type": "validation_strategy",
      "title": "Platform Identity Signifier Recall Test",
      "validates_element": "communicates-002",
      "journey_phases": ["discovery", "search", "listing_evaluation"],
      "problem": "The persistent identity signifier ('Book the nights you need') may not build recognition through repetition if it blends into the background or competes with listing-specific content. If guests stop perceiving it, the system image anchor is lost.",
      "solution": "Run a recall test after a simulated multi-page browsing session. Measure whether participants can recall the platform's core proposition after viewing 3-5 pages.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 0:43",
          "type": "guest_call",
          "quote": "Like seven nights a week?",
          "insight": "Susan's confusion persisted even after Bryant explained the model once. The recall test measures whether repeated exposure through the persistent signifier builds retention of the core concept."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Signifiers",
          "type": "book",
          "quote": "People need some way of understanding the product or service they wish to use.",
          "insight": "The persistent signifier is meant to serve this function on every page. The recall test validates whether the signifier achieves comprehension that persists across pages."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 15 participants. Ask them to browse the platform as if looking for housing: view the landing page, use search, open 2-3 listings. After browsing, close the browser and ask: 'In your own words, what does this platform offer that is different from Airbnb or traditional rentals?' Code responses for mention of partial-week, specific-nights, or non-continuous concepts.",
      "success_criteria": "At least 80% of participants mention the partial-week or specific-nights model in their response. At least 50% use language that closely mirrors the signifier's vocabulary ('book the nights you need' or equivalent). Zero participants describe it as a traditional short-term rental platform.",
      "failure_meaning": "If recall is below 80%, the signifier is not registering through repetition. This could mean it is too subtle visually, positioned where it is skipped, or overshadowed by listing-specific content. If participants recall the concept but use completely different language, the signifier's specific wording is not sticky.",
      "implementation_hint": "Compare with a control group that browses the platform without the persistent signifier (using only the landing page headline) to measure the incremental value of cross-page repetition."
    },
    {
      "id": "tests-009",
      "type": "validation_strategy",
      "title": "Duration/Schedule Preset Coverage Analytics",
      "validates_element": "communicates-003",
      "journey_phases": ["search", "proposal_creation"],
      "problem": "The four schedule presets (Weekdays, Weekends, Full Week, Custom) and four duration presets (1 month, 2 months, 3+ months, Custom) may not cover the actual distribution of guest needs. If too many guests end up in 'Custom,' the natural-language mapping has failed for the majority.",
      "solution": "Instrument the preset selectors with analytics to track which presets are selected, how often Custom is used, and whether guests who use Custom complete their proposals at the same rate as those who use presets.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 3:58",
          "type": "guest_call",
          "quote": "It's Monday to Friday will be better.",
          "insight": "Susan's need maps directly to the 'Weekdays' preset. Analytics will reveal what percentage of real guests have needs that map to presets versus requiring Custom configuration."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Mapping",
          "type": "book",
          "quote": "A device is easy to use when the set of possible actions is visible.",
          "insight": "If the presets cover common patterns, most guests will find their pattern visible. If Custom usage exceeds 30%, the visible options are not matching actual demand."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "Implement event tracking on all preset selections: which schedule preset was tapped, which duration preset was tapped, whether Custom was used, and whether the guest switched from a preset to Custom (indicating the preset was tried but insufficient). Track proposal completion rate by preset combination. Run for 8 weeks to capture sufficient volume.",
      "success_criteria": "Custom usage below 25% for both schedule and duration selectors. Proposal completion rate for preset users is at least 20% higher than for Custom users. The top 3 preset combinations cover at least 70% of all proposals.",
      "failure_meaning": "If Custom usage exceeds 25%, the presets do not match the actual distribution of guest needs and must be expanded. If Custom users complete proposals at the same rate as preset users, the Custom path is well-designed but the presets may be unnecessary. If Custom users have significantly lower completion rates, the Custom input path needs simplification.",
      "implementation_hint": "Log the specific Custom configurations guests enter to identify the most common patterns that could become new presets. Pay attention to guests who start with Custom but switch to a preset -- this suggests the preset labels were not initially understood."
    },
    {
      "id": "tests-010",
      "type": "validation_strategy",
      "title": "Negotiation Timeline State Clarity Test",
      "validates_element": "communicates-004",
      "journey_phases": ["negotiation", "acceptance"],
      "problem": "The timeline's discrete states (Proposal Sent, Host Reviewing, Host Responded, Your Turn, Agreed) may not be immediately interpretable. Guests may not understand which state they are in or what action they need to take, especially for counter-offers.",
      "solution": "Run a comprehension test showing static timeline screenshots at each state. Measure whether participants can correctly identify the current state and what action is required.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 6:50-6:57",
          "type": "guest_call",
          "quote": "I'll be in contact with Rita. And like I said, I'll keep you updated.",
          "insight": "The timeline must replace Bryant's verbal promise with a self-explanatory visual. The comprehension test measures whether the timeline achieves this."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Feedback",
          "type": "book",
          "quote": "Feedback must be immediate... Feedback must also be informative.",
          "insight": "Norman requires feedback to be informative, not just present. The test validates that each timeline state communicates both current status and required action."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 15 participants. Show 5 screenshots of the negotiation timeline, each at a different state. For each, ask: 'What is happening right now?' and 'What should you do next?' Record accuracy of responses. Include the counter-offer state with a diff display to test whether participants can identify what changed.",
      "success_criteria": "At least 85% of participants correctly identify the current state in each screenshot. At least 80% correctly identify the required action (wait, respond, review counter-offer). For the counter-offer state, at least 75% can identify the specific terms that changed.",
      "failure_meaning": "If state identification is below 85%, the visual differentiation between states is insufficient. If action identification is low but state identification is high, the 'next step' signifier is not prominent enough. If counter-offer diff comprehension is low, the diff display needs redesign.",
      "implementation_hint": "Test both the timeline in isolation and embedded in the full proposal page to check whether surrounding content distracts from the timeline's signal."
    },
    {
      "id": "tests-011",
      "type": "validation_strategy",
      "title": "Proxy Booking Notification Relevance Check",
      "validates_element": "communicates-005",
      "journey_phases": ["acceptance", "active_lease"],
      "problem": "Role-specific notifications may deliver irrelevant information to the wrong role (coordinator receives check-in details, occupant receives payment info) or may miss critical updates for one role. Norman's feedback must be role-appropriate.",
      "solution": "Run a notification content audit with proxy booking pairs. Send simulated notifications for key events and ask each role to rate relevance and identify missing information.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 3:58-4:31",
          "type": "guest_call",
          "quote": "She's going to be out most of the day, you know, she has done appointments and all I'm setting up all appointments for her.",
          "insight": "Susan needs logistics and scheduling updates; her friend needs space and check-in updates. The audit tests whether notifications correctly route information to the right role."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Feedback",
          "type": "book",
          "quote": "Feedback has to be planned... prioritized, so that unimportant information is presented in an unobtrusive fashion.",
          "insight": "Information that is important to one role is unimportant to the other. The audit tests whether the notification system respects this role-based priority."
        }
      ],
      "priority": "medium",
      "validation_method": "manual_review",
      "test_description": "Create 10 event scenarios (proposal accepted, host counter-offer, payment processed, check-in details sent, date change requested, cleaning reminder, etc.). For each event, draft the coordinator notification and the occupant notification. Show each to 6 proxy booking pairs (12 people total) and ask: 'Is this information relevant to you?' and 'Is anything missing?' Rate relevance on a 1-5 scale.",
      "success_criteria": "Average relevance rating of 4+ out of 5 for each role's notifications. No critical information gap identified by more than 2 of the 12 participants. Coordinator notifications never include check-in instructions; occupant notifications never include payment details (unless explicitly requested).",
      "failure_meaning": "If relevance ratings are below 4, the role-based routing is not differentiating enough. If coordinators report missing logistics info or occupants report missing space info, the notification templates have content gaps specific to each role.",
      "implementation_hint": "Draft notifications in parallel (coordinator version and occupant version of each event) before user testing. This forces explicit decisions about what each role needs for every event, making gaps visible during the drafting process."
    },
    {
      "id": "tests-012",
      "type": "validation_strategy",
      "title": "House Manual Chronological Sequence Validation",
      "validates_element": "communicates-006",
      "journey_phases": ["move_in"],
      "problem": "The 6-step sequence (Access, Verify, Connect, Safety Net, Settle, Explore) may not match the actual order of guest actions at move-in. If guests need Wi-Fi before they verify the space, or need house rules before they settle, the chronological structure creates friction instead of reducing it.",
      "solution": "Run a diary study with 10 guests during their first 24 hours. Ask them to log each action they take and when, then compare the actual action sequence to the designed manual sequence.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 0:55-1:08",
          "type": "guest_call",
          "quote": "Is it a full apartment or is it a bedroom? ... So it's a one bedroom work. Can she, what about a bathroom?",
          "insight": "Susan's question sequence reveals a natural priority order. The diary study tests whether the designed manual sequence matches the real action sequence for diverse guests."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Conceptual Models",
          "type": "book",
          "quote": "A good conceptual model allows us to predict the effects of our actions.",
          "insight": "The chronological House Manual IS the conceptual model for move-in. If the sequence does not match reality, the model fails to predict the guest's next action."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 10 guests checking in for their first night. Ask them to use a simple logging tool (voice memo or text) to note each action they take during their first hour: 'I entered the building,' 'I looked for the room,' 'I tried to connect to Wi-Fi,' etc. Compare the logged sequence to the designed 6-step sequence. Note any steps that guests perform out of order or steps that are missing from the manual.",
      "success_criteria": "At least 70% of guests follow the designed sequence for Steps 1-3 (Access, Verify, Connect). No guest reports a critical need (e.g., Wi-Fi for work, contacting the host) that the manual places later than Step 3. The cancellation guarantee (Step 4) is accessed or noticed by at least 50% of guests during the first night.",
      "failure_meaning": "If fewer than 70% follow Steps 1-3 in order, the designed sequence does not match the natural action pattern and must be reordered. If guests report needs placed after Step 3, those items must be moved earlier. If the cancellation guarantee is not noticed by 50%, its positioning needs to be reconsidered.",
      "implementation_hint": "Supplement diary data with House Manual access analytics: which steps are opened, in what order, and how long guests spend on each. This provides objective data to compare against self-reported behavior."
    },
    {
      "id": "tests-013",
      "type": "validation_strategy",
      "title": "Space Composition Label Comprehension Test",
      "validates_element": "communicates-007",
      "journey_phases": ["search", "listing_evaluation"],
      "problem": "The three-part space composition label (room type, bathroom, co-occupancy) may not be parsed correctly at scanning speed. Guests may not understand temporal co-occupancy descriptions like 'Host present evenings.' If the label is misunderstood, guests evaluate listings with incorrect expectations.",
      "solution": "Run a rapid comprehension test showing listing cards with various space composition labels. Measure whether participants can correctly interpret each label within 3 seconds.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 0:43-1:20",
          "type": "guest_call",
          "quote": "Is it a full apartment or is it a bedroom? ... What about a bathroom? ... Would somebody else be in the place when she's there?",
          "insight": "Susan asks these three questions in rapid sequence. The label must answer all three in a single scan line. The test measures whether it does."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Signifiers",
          "type": "book",
          "quote": "Signifiers must be perceivable, else they fail to function.",
          "insight": "Perceivability requires correct interpretation, not just visibility. The test measures whether the label is correctly interpreted, especially the temporal co-occupancy component."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 20 participants. Show 8 listing cards, each with a different space composition label. For each card, show for 3 seconds then ask: 'Will you have a private room?' 'Will you share a bathroom?' 'Will anyone else be in the apartment?' Measure accuracy of responses, especially for the co-occupancy descriptor.",
      "success_criteria": "At least 90% accuracy on room type and bathroom interpretation. At least 75% accuracy on co-occupancy interpretation. No participant interprets 'Host present evenings' as 'host is always there' or 'you are never alone.'",
      "failure_meaning": "If accuracy is below thresholds, the label's wording is ambiguous. Room type and bathroom are binary and should achieve near-perfect accuracy; if not, the labels themselves are confusing. Co-occupancy is the highest-risk component; if accuracy is below 75%, the temporal descriptors need reworking.",
      "implementation_hint": "Test the most ambiguous labels: 'Host present weekends,' 'Shared with 1 other guest,' and 'No other occupants.' These represent the three co-occupancy patterns and should be distinguishable at scanning speed."
    },
    {
      "id": "tests-014",
      "type": "validation_strategy",
      "title": "Trust Triad Above-the-Fold Visibility Audit",
      "validates_element": "looks-001",
      "journey_phases": ["listing_evaluation"],
      "problem": "The trust triad (verified photo badge, cancellation guarantee, host identity card) may not all fit above the fold on mobile devices, which is the primary use case for guests evaluating listings. If any element is pushed below the fold, the designed scan order breaks.",
      "solution": "Run an automated viewport audit across the top 10 mobile device sizes. Verify that all three trust elements are visible on initial load without scrolling.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 3:42-3:58",
          "type": "guest_call",
          "quote": "If I like the room in the bathroom, I can, should have pictures.",
          "insight": "Susan needs photos as the primary trust mechanism. If the verified photo badge is not visible on load, the trust sequence does not begin."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Signifiers",
          "type": "book",
          "quote": "Signifiers must be perceivable, else they fail to function.",
          "insight": "Below-the-fold signifiers are effectively not perceivable on first impression. The viewport audit ensures perceivability across real device sizes."
        }
      ],
      "priority": "high",
      "validation_method": "automated",
      "test_description": "Use a responsive testing tool (BrowserStack, Playwright viewport testing) to capture screenshots of the listing page on the 10 most common mobile viewport sizes (iPhone SE, iPhone 14 Pro, Galaxy S23, Pixel 7, etc.). For each screenshot, verify: (1) verified photo badge is visible, (2) cancellation guarantee badge is visible, (3) host identity card is visible. Document any viewport where an element is cut off or hidden.",
      "success_criteria": "All three trust elements are fully visible without scrolling on at least 90% of tested viewport sizes. On the remaining 10%, at least two of three are visible. No viewport hides all three elements.",
      "failure_meaning": "If elements are cut off on popular viewports, the listing page layout needs responsive adjustments. If the photo gallery consumes too much vertical space, the guarantee badge and host card are pushed below the fold -- the gallery may need height constraints.",
      "implementation_hint": "Run this as part of CI/CD pipeline for any listing page changes. Include both portrait and landscape orientations. Test with and without the browser's address bar visible (which reduces viewport height by 60-80px on mobile)."
    },
    {
      "id": "tests-015",
      "type": "validation_strategy",
      "title": "Schedule Badge Recognition Test",
      "validates_element": "looks-002",
      "journey_phases": ["discovery", "search"],
      "problem": "The persistent schedule badge ('Weeknights,' 'Mon-Fri,' 'Flexible') on listing cards may not be recognized as a platform-level signifier. Guests may interpret it as a listing-specific feature rather than a system-wide model indicator, defeating its purpose as a system image anchor.",
      "solution": "Run a categorization test where participants see listing cards with and without the schedule badge and identify what makes the listings different from traditional rentals.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 4:55",
          "type": "guest_call",
          "quote": "Real estate Agent.",
          "insight": "Susan had no visual cue that differentiated Split Lease from a traditional service. The badge must make this differentiation visible at the listing card level."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 The System Image",
          "type": "book",
          "quote": "The entire burden of communication is on the system image.",
          "insight": "The schedule badge is part of the system image. If it is not recognized as a platform-level signifier, it fails to communicate the system's identity."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 15 participants. Show two sets of listing cards: Set A includes the schedule badge, Set B omits it. Ask participants who view Set A: 'What makes these listings different from Airbnb listings?' Ask Set B the same question. Compare responses. Then show both sets to all participants and ask them to identify the badge and explain what it means.",
      "success_criteria": "Participants who saw Set A mention partial-week or specific-night availability at least 60% more often than Set B participants. At least 70% of participants can correctly explain what the schedule badge means when asked directly.",
      "failure_meaning": "If Set A and Set B produce similar responses, the badge is not being noticed or is not communicating the part-time model. If participants notice the badge but cannot explain it, the label text ('Weeknights') is not self-explanatory enough.",
      "implementation_hint": "Use between-subjects design for the comparison (different participants see Set A vs. Set B) to avoid priming effects. Then switch to within-subjects for the badge identification task."
    },
    {
      "id": "tests-016",
      "type": "validation_strategy",
      "title": "Space Composition Label Visual Scan Priority Test",
      "validates_element": "looks-003",
      "journey_phases": ["search"],
      "problem": "The space composition label is designed to be the first text element guests read after the photo on listing cards. But photo thumbnails, price, and location are competing for the same scan position. If the label is not read first, the designed visual hierarchy is not working.",
      "solution": "Run a card-sorting study with eye tracking to measure the actual scan order on redesigned listing cards.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 0:43-1:20",
          "type": "guest_call",
          "quote": "Is it a full apartment or is it a bedroom?",
          "insight": "Susan's first question is about space type, not price or location. The visual hierarchy must match this natural priority. Eye tracking validates whether it does."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Conceptual Models",
          "type": "book",
          "quote": "When the system image is incoherent or inappropriate, then the user cannot easily use the device.",
          "insight": "If the visual hierarchy presents price before space composition, the system image implies 'budget-first' when shared housing guests need 'sharing-first.'"
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 10 participants. Show redesigned listing cards with eye tracking. Present a search results page with 6-8 listing cards. Measure the scan order for each card: which text element receives the first fixation after the photo. Record whether space composition, price, or location is fixated first.",
      "success_criteria": "The space composition label receives the first text fixation (after the photo) on at least 60% of card views. Price fixation as the first text element should be below 30%.",
      "failure_meaning": "If price consistently receives first fixation over the space composition label, the visual weight of the label is insufficient to compete with price. The label may need bolder typography, a background pill, or repositioning.",
      "implementation_hint": "Test with realistic listing content (real photos, real prices) to ensure the scan pattern reflects actual use, not laboratory artifact."
    },
    {
      "id": "tests-017",
      "type": "validation_strategy",
      "title": "Duration/Schedule Selector Real-Time Feedback Responsiveness Audit",
      "validates_element": "looks-004",
      "journey_phases": ["proposal_creation"],
      "problem": "The real-time translation summary bar must update within 200ms of each selection change. If the update is delayed, laggy, or produces a flash of incorrect content, the direct-manipulation feel is broken. The guest loses the sense that her choices produce immediate consequences.",
      "solution": "Run an automated performance test measuring the time from preset tap to summary bar text update across multiple device types and network conditions.",
      "evidence": [
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Feedback",
          "type": "book",
          "quote": "Feedback must be immediate: even a delay of a tenth of a second can be disconcerting.",
          "insight": "Norman's 100ms threshold applies to the visual feedback on the preset buttons. The 200ms threshold applies to the summary bar text update. Both must be validated instrumentally."
        },
        {
          "source": "Susan  Bryant customer call.txt, 1:35-2:00",
          "type": "guest_call",
          "quote": "Rita would require a 30 night minimum... five nights a week for like two months would be something that would, could work.",
          "insight": "Bryant computed the translation in real time during conversation. The summary bar must match this responsiveness. Automated timing validation ensures it does."
        }
      ],
      "priority": "medium",
      "validation_method": "automated",
      "test_description": "Using a performance testing framework (Lighthouse, custom Playwright timing tests), measure: (1) time from preset tap event to button visual state change (target: under 100ms), (2) time from preset tap to summary bar text update (target: under 200ms), (3) time from duration change to price recalculation display (target: under 500ms if server-side, under 200ms if client-side). Run on 5 device profiles (low-end Android, mid-range Android, iPhone SE, iPhone 14, desktop) and 3 network conditions (4G, 3G, offline-capable).",
      "success_criteria": "Button state change under 100ms on all devices. Summary text update under 200ms on 4G and desktop. Price calculation under 500ms when requiring server fetch. All client-side calculations under 200ms regardless of device. No visible flicker or flash of incorrect content during transitions.",
      "failure_meaning": "If any threshold is exceeded, the interaction feels sluggish and breaks the direct-manipulation metaphor. If low-end devices fail, the animations may need simplification or the calculations need optimization.",
      "implementation_hint": "Include this in the CI/CD pipeline as a performance regression test. Set alerts if any timing metric regresses by more than 50ms."
    },
    {
      "id": "tests-018",
      "type": "validation_strategy",
      "title": "Negotiation Timeline Urgency Differentiation Test",
      "validates_element": "looks-005",
      "journey_phases": ["negotiation"],
      "problem": "The urgency color differentiation (purple for routine states, coral for action-required states) may not be perceived as urgency by all users. Color alone is insufficient for accessibility. If the 'Your Turn' state does not capture attention more than the 'Host Reviewing' state, the priority system fails.",
      "solution": "Run a rapid attention test showing different timeline states side by side. Measure which state captures attention first and whether participants can rank states by urgency.",
      "evidence": [
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Feedback",
          "type": "book",
          "quote": "Feedback must also be prioritized, so that unimportant information is presented in an unobtrusive fashion, but important signals are presented in a way that does capture attention.",
          "insight": "Norman's prioritization principle demands that high-urgency states be visually distinct from low-urgency states. The test validates whether the color and styling differentiation achieves this."
        },
        {
          "source": "Susan  Bryant customer call.txt, 3:05-3:16",
          "type": "guest_call",
          "quote": "I do have to see the room and the bathroom first. So, um, it wouldn't be... However, there would be something set up where if you come the first night...",
          "insight": "The negotiation involves action-required moments (Susan must respond to a counter-offer). The timeline must visually signal these moments with enough urgency to prompt action."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 15 participants. Show 4 timeline screenshots side by side: (1) Proposal Sent (neutral), (2) Host Reviewing (pulse), (3) Your Turn (coral accent), (4) Accepted (purple). Ask: 'Which one requires you to take action?' Also test with color-blind simulation filters (protanopia, deuteranopia) to verify accessibility.",
      "success_criteria": "At least 90% of participants correctly identify the 'Your Turn' state as the action-required state within 3 seconds. Under color-blind simulation, at least 80% still identify it correctly (validated by the bold label and left-border accent, not just color). All participants can rank the four states from 'no action needed' to 'immediate action.'",
      "failure_meaning": "If identification is below 90%, the coral accent is not distinct enough from the purple. If color-blind simulation drops accuracy below 80%, the non-color differentiation (bold label, border accent) is insufficient and needs strengthening.",
      "implementation_hint": "Test the 'Your Turn' state with and without animation (pulse) to isolate whether motion or color is the primary attention mechanism. Ensure the test includes participants with color vision deficiency."
    },
    {
      "id": "tests-019",
      "type": "validation_strategy",
      "title": "House Manual Cancellation Banner Visibility and Comprehension Test",
      "validates_element": "looks-006",
      "journey_phases": ["move_in"],
      "problem": "The persistent cancellation guarantee banner at the bottom of the viewport may be perceived as a cookie banner or navigation element and dismissed without reading. If the guest does not understand they have a cancellation safety net, the entire trust architecture collapses at the moment it matters most.",
      "solution": "Run a task-based usability test where participants navigate the House Manual and are asked to identify their options if the space does not match the listing.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 3:16",
          "type": "guest_call",
          "quote": "There would be something set up where if you come the first night and it's not something that would work, you have the flexibility to cancel.",
          "insight": "Bryant's verbal guarantee is the trust mechanism that made the booking possible. The banner must ensure this guarantee is perceived at move-in. If guests dismiss it as UI noise, the guarantee is invisible when it matters most."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Affordances",
          "type": "book",
          "quote": "Affordances exist even if they are not visible. For designers, their visibility is critical.",
          "insight": "The cancellation affordance exists in policy but must be visible in the UI. The test validates visibility at the exact moment (move-in) when the guest needs to know about it."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 10 participants. Present the House Manual on a mobile device. Ask them to navigate through Steps 1-3 (Access, Verify, Connect). Then ask: 'Imagine the room does not match the photos. What can you do?' Observe whether they reference the cancellation banner without being directed to it. If not, ask: 'Did you notice anything at the bottom of the screen?' Measure: (1) unprompted mention of cancellation, (2) prompted recognition of the banner, (3) comprehension of the cancellation terms.",
      "success_criteria": "At least 60% mention the cancellation option unprompted when asked what they can do. At least 90% recognize the banner when prompted. At least 85% correctly understand the terms (24-hour window, how to exercise it).",
      "failure_meaning": "If unprompted mention is below 60%, the banner is being treated as background noise. If prompted recognition is below 90%, the banner is not visually distinct enough from other UI elements. If comprehension is low despite recognition, the banner's copy needs rewriting.",
      "implementation_hint": "Compare with a variant where the cancellation guarantee is presented as an inline step (Step 4) rather than a persistent banner, to test whether integration into the sequence is more effective than persistence."
    },
    {
      "id": "tests-020",
      "type": "validation_strategy",
      "title": "Proxy Booking Role Color Differentiation Test",
      "validates_element": "looks-007",
      "journey_phases": ["active_lease"],
      "problem": "The role-colored zones (purple for coordinator, blue for occupant) may not be perceived as role indicators. Users may interpret the color difference as decorative rather than semantic, or the distinction may be too subtle to register.",
      "solution": "Run a card-sorting test where participants assign information items to 'my role' versus 'the other person's role' based on zone color.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 2:34",
          "type": "guest_call",
          "quote": "I can give you a deposit on her behalf.",
          "insight": "Susan and her friend have different roles. The visual color coding must make these roles immediately distinguishable so neither person sees irrelevant information as primary."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Signifiers",
          "type": "book",
          "quote": "People need some way of understanding... what is happening, and what the alternative actions are.",
          "insight": "The role colors are signifiers for 'this section is about YOUR role.' If the signifier is not perceived, users must read every label to determine relevance."
        }
      ],
      "priority": "low",
      "validation_method": "usability_test",
      "test_description": "Recruit 10 participants. Show a Stays Manager screen with both coordinator (purple-tinted) and occupant (blue-tinted) information zones. Present 8 information items (payment receipt, check-in code, date change history, House Manual link, etc.) and ask: 'Which section would this belong in -- yours or your friend's?' Measure whether participants use color zones as the sorting cue.",
      "success_criteria": "At least 70% of participants sort items correctly based on the zone color coding. At least 80% can verbalize the role distinction ('purple is my stuff, blue is my friend's stuff') when asked to describe the color pattern.",
      "failure_meaning": "If sorting accuracy is below 70%, the color differentiation is too subtle or the association between color and role is not learned from the interface. If verbalization is low, participants may be sorting based on labels, not colors, meaning the color serves no functional purpose.",
      "implementation_hint": "Test with and without the label prefixes ('Your' vs. 'Guest') to isolate whether color or labels carry the role-differentiation signal. If labels alone achieve the same sorting accuracy, the color coding may be decorative rather than functional."
    },
    {
      "id": "tests-021",
      "type": "validation_strategy",
      "title": "First-Night Cancellation Guarantee Phase Adaptation Test",
      "validates_element": "behaves-001",
      "journey_phases": ["listing_evaluation", "acceptance", "move_in"],
      "problem": "The cancellation guarantee adapts its interaction pattern across phases (badge at listing evaluation, confirmed term at acceptance, persistent banner at move-in). If the adaptation is not perceived as the same guarantee in different forms, the guest may not realize they have the same safety net throughout the journey.",
      "solution": "Run a longitudinal scenario test where participants progress through the full journey and are asked at each phase to describe their cancellation rights.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 3:16",
          "type": "guest_call",
          "quote": "There would be something set up where if you come the first night and it's not something that would work, you have the flexibility to cancel.",
          "insight": "Bryant stated the guarantee once; Susan remembered it. The platform states it at multiple phases -- the test validates whether the phased presentation reinforces retention or creates confusion."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Feedback",
          "type": "book",
          "quote": "Feedback has to be planned.",
          "insight": "The guarantee's phased visibility is planned feedback. The test validates whether the plan achieves consistent awareness across phases."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 8 participants. Walk each through a simulated journey: listing evaluation, proposal, acceptance, move-in. At each phase, pause and ask: 'Can you cancel this booking? If so, when and how?' Track whether awareness of the cancellation guarantee is consistent across phases or drops at any point.",
      "success_criteria": "At least 85% of participants correctly describe the cancellation guarantee at each of the 4 phases. No participant reports being unaware of the guarantee at the move-in phase (the most critical moment). Awareness should not decrease from listing evaluation to move-in.",
      "failure_meaning": "If awareness drops at any phase, the guarantee's visual presentation at that phase is insufficient. If participants describe different terms at different phases, the phased adaptation is creating confusion rather than reinforcement.",
      "implementation_hint": "Use a clickable prototype that simulates the full journey in a 15-20 minute session. Include realistic waiting periods (simulated) between negotiation and acceptance to test whether the guarantee is recalled after a gap."
    },
    {
      "id": "tests-022",
      "type": "validation_strategy",
      "title": "Proposal Timeline Heartbeat Notification Engagement Test",
      "validates_element": "behaves-002",
      "journey_phases": ["negotiation"],
      "problem": "The heartbeat notifications ('Rita has not yet responded. Most hosts respond within 6 hours.') may be ignored, dismissed, or may cause anxiety rather than reassurance. The right tone and timing of intermediate feedback determines whether the guest stays engaged or abandons.",
      "solution": "Run a field test tracking notification open rates, app return rates, and abandonment rates correlated with heartbeat notification delivery.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 6:50-6:57",
          "type": "guest_call",
          "quote": "I'll be in contact with Rita. And like I said, I'll keep you updated.",
          "insight": "Bryant's promise of updates is the human equivalent of heartbeat notifications. The field test measures whether the digital version maintains the same engagement."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Feedback",
          "type": "book",
          "quote": "Feedback must be immediate... If the delay is too long, people often give up.",
          "insight": "The heartbeat notification is the mechanism that prevents 'giving up' during asynchronous waiting. Its effectiveness is measurable through engagement metrics."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "For all proposals submitted over an 8-week period, track: (1) heartbeat notification delivery timestamps, (2) notification open/view rates, (3) app return within 1 hour of notification, (4) proposal abandonment (guest withdraws or stops engaging) relative to heartbeat delivery. Segment by first-time vs. returning guests.",
      "success_criteria": "Heartbeat notification open rate above 40%. App return within 1 hour of heartbeat delivery above 25%. Proposal abandonment rate for guests who receive and open heartbeats is at least 30% lower than for guests who receive but do not open them (organic engagement comparison).",
      "failure_meaning": "If open rate is below 40%, the notifications are not compelling or are being filtered. If app return is low despite opens, the notification content does not motivate re-engagement. If abandonment rates are similar regardless of heartbeat engagement, the notifications are not serving a retention function.",
      "implementation_hint": "A/B test the notification copy: factual ('Rita has not responded yet. Average response time: 6 hours.') versus social ('Most hosts respond within 6 hours. We will notify you immediately.') to identify which framing drives higher engagement."
    },
    {
      "id": "tests-023",
      "type": "validation_strategy",
      "title": "Duration/Schedule Selector Coupled Update Smoothness Test",
      "validates_element": "behaves-003",
      "journey_phases": ["search", "proposal_creation"],
      "problem": "The coupled input system (schedule change cascades to night count cascades to price) may produce visual artifacts: flickering, intermediate incorrect values, or race conditions when the guest taps presets rapidly. These artifacts break the direct-manipulation feel.",
      "solution": "Run a rapid interaction stress test where testers switch presets rapidly and observe the summary bar's behavior.",
      "evidence": [
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Feedback",
          "type": "book",
          "quote": "Feedback must be immediate: even a delay of a tenth of a second can be disconcerting.",
          "insight": "Each preset tap must produce correct, stable feedback within 200ms. Race conditions during rapid input would violate this principle."
        },
        {
          "source": "Susan  Bryant customer call.txt, 1:35-2:00",
          "type": "guest_call",
          "quote": "Rita would require a 30 night minimum... five nights a week for like two months.",
          "insight": "The summary bar must consistently show correct calculations even when the guest experiments with different combinations. Incorrect intermediate values would undermine the trust the summary is meant to build."
        }
      ],
      "priority": "medium",
      "validation_method": "automated",
      "test_description": "Write an automated test script that rapidly switches between all preset combinations (Weekdays + 1 month, Weekends + 2 months, Full Week + 3 months, etc.) with 100ms intervals between taps. For each final state, verify: (1) the summary bar shows the correct night count, (2) the correct price, (3) no intermediate incorrect value persists for more than 100ms. Run on 5 device profiles.",
      "success_criteria": "Zero incorrect final states across all preset combinations. No intermediate incorrect value visible for more than 100ms. No visual artifacts (flickering, layout shifts) during rapid switching. All calculations are deterministic: the same input always produces the same output.",
      "failure_meaning": "If incorrect final states appear, there is a race condition in the cascading update logic. If intermediate values linger, the debouncing is not working correctly. If layout shifts occur, the summary bar's dimensions are not stable during content updates.",
      "implementation_hint": "Implement as an end-to-end test in the CI/CD pipeline. Add a visual regression capture at each final state to detect layout shifts that automated value checks might miss."
    },
    {
      "id": "tests-024",
      "type": "validation_strategy",
      "title": "Move-In Verification Checklist Completion and Flag Resolution Test",
      "validates_element": "behaves-004",
      "journey_phases": ["move_in"],
      "problem": "The branching verification checklist (confirm vs. flag) may be confusing at move-in when the guest is physically disoriented and cognitively loaded. The flag sub-flow (text input, photo upload, contact host / review cancellation) may be too complex for an anxious guest standing in an unfamiliar room.",
      "solution": "Run a scenario-based usability test where participants complete the checklist in a simulated move-in context, including at least one flagged item.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 2:22",
          "type": "guest_call",
          "quote": "I have to go and see the space and just to make sure that everything is legit.",
          "insight": "The checklist is the digital enactment of Susan's demand. The test validates whether it achieves the verification Susan insisted on without adding friction at a high-stress moment."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Discoverability",
          "type": "book",
          "quote": "Is it possible to even figure out what actions are possible and where and how to perform them?",
          "insight": "At move-in, each checklist action (confirm, flag, contact host, review cancellation) must be discoverable. The test measures whether guests can figure out the branching paths under realistic stress."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 10 participants. Present the checklist on a mobile device. Give the scenario: 'You just arrived. The room looks right, but the bathroom is shared, not private as listed. Go through the checklist.' Measure: (1) time to complete 4 confirm items, (2) whether they successfully flag the bathroom item, (3) whether they navigate to the contact host or review cancellation path, (4) qualitative feedback on complexity.",
      "success_criteria": "At least 80% complete all confirm items in under 2 minutes. At least 85% successfully flag the discrepant item. At least 70% navigate to either 'Contact Host' or 'Review Cancellation' from the flag sub-flow without assistance. Qualitative feedback shows the checklist felt helpful rather than burdensome.",
      "failure_meaning": "If confirm completion is slow, the one-item-at-a-time flow is too paced for move-in urgency. If flagging success is below 85%, the flag action is not discoverable alongside the confirm action. If sub-flow navigation fails, the branching paths are too complex for move-in cognitive load.",
      "implementation_hint": "Simulate realistic conditions: ask participants to stand up and walk around while using the phone, mimicking the physical activity of inspecting a room. This tests whether the checklist works under the divided attention of move-in."
    },
    {
      "id": "tests-025",
      "type": "validation_strategy",
      "title": "Proxy Booking Role Routing Accuracy Test",
      "validates_element": "behaves-005",
      "journey_phases": ["active_lease"],
      "problem": "The role-aware interaction routing may deliver incorrect actions to the wrong role (coordinator sees check-in button, occupant sees payment controls) or may fail to notify both roles when both need to know about an event.",
      "solution": "Run a QA walkthrough of all booking events in proxy mode, verifying that each event routes actions and notifications to the correct role.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 3:58-4:31",
          "type": "guest_call",
          "quote": "I'm setting up all appointments for her.",
          "insight": "Susan manages logistics; her friend occupies. The QA test verifies that the system correctly constrains actions to each role and does not mix scheduling tools with check-in flows."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Signifiers",
          "type": "book",
          "quote": "People need some way of understanding... what the alternative actions are.",
          "insight": "Each role's available actions must be signified correctly. A QA walkthrough tests the system's role-routing logic exhaustively."
        }
      ],
      "priority": "medium",
      "validation_method": "manual_review",
      "test_description": "Create a test proxy booking with coordinator and occupant accounts. Walk through 15 events: proposal submission, host response, acceptance, payment, check-in details delivery, move-in, date change request, date change confirmation, cleaning reminder, review prompt, payment receipt, host message, coordinator message, occupant message, end-of-stay summary. For each event, verify: (1) the correct role receives the primary notification, (2) the other role receives an appropriate secondary notification if applicable, (3) actions are correctly constrained to each role's permissions.",
      "success_criteria": "Zero events route the primary notification to the wrong role. Zero events present an action button to a role that should not have that action. Both roles receive appropriate notifications for shared events (e.g., booking acceptance). No notification includes information irrelevant to the receiving role.",
      "failure_meaning": "Any routing error indicates a logic bug in the role-awareness system. If shared events only notify one role, the dual-notification system is incomplete. If actions are available to the wrong role, the permission constraints are not properly scoped.",
      "implementation_hint": "Maintain this as a regression test suite that runs on every release. Add new test events as the proxy booking feature evolves."
    },
    {
      "id": "tests-026",
      "type": "validation_strategy",
      "title": "Interactive Onboarding Moment Effectiveness Test",
      "validates_element": "behaves-006",
      "journey_phases": ["discovery", "search"],
      "problem": "The three progressive onboarding moments (first-listing overlay, search hint, proposal cycling example) may be perceived as intrusive, confusing, or may not effectively teach the non-continuous model. If dismissed without comprehension, they fail their teaching purpose.",
      "solution": "Run a before/after comprehension test around each onboarding moment to measure whether it improves understanding of the part-time model.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 0:43",
          "type": "guest_call",
          "quote": "Like seven nights a week?",
          "insight": "Susan's default is full-time. The onboarding moments must correct this default. The before/after test measures whether each moment shifts comprehension from full-time to part-time."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 The System Image",
          "type": "book",
          "quote": "When the system image is incoherent or inappropriate, then the user cannot easily use the device.",
          "insight": "The onboarding moments build the system image through interaction. The test validates whether each moment contributes to a coherent image."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 12 participants. Before they interact with the platform, ask: 'What do you think this platform offers?' Then walk them through each onboarding moment sequentially. After each moment, ask: 'Based on what you just saw, what does this platform offer?' Track comprehension improvement after each moment. Also measure: was the onboarding perceived as helpful or intrusive?",
      "success_criteria": "Comprehension of the part-time model increases by at least 30 percentage points from pre-interaction to post-first-listing-overlay. At least 70% of participants rate the onboarding moments as helpful rather than intrusive. The cycling proposal example is rated as the most clarifying of the three moments.",
      "failure_meaning": "If comprehension does not improve after the first-listing overlay, its copy is not effective at teaching the model. If onboarding is rated as intrusive, the moments need to be less interruptive or shorter. If the cycling example is not rated as most clarifying, the proposal summary bar is not effectively demonstrating the translation mechanic.",
      "implementation_hint": "Test each moment in isolation (first-listing only, search hint only, cycling example only) in addition to the cumulative sequence, to identify which moments carry the most teaching value and which could be eliminated."
    },
    {
      "id": "tests-027",
      "type": "validation_strategy",
      "title": "Discovery Comprehension Relief Sentiment Test",
      "validates_element": "feels-001",
      "journey_phases": ["discovery"],
      "problem": "The emotional target is 'clarity' -- the feeling of 'oh, I get it.' But comprehension relief is difficult to measure directly. The landing page copy may be clear without producing the positive emotional response of 'I feel smart for getting this quickly.'",
      "solution": "Run a sentiment survey immediately after first landing page exposure. Measure both comprehension (correct/incorrect) and emotional response (confused, neutral, clear, excited).",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 4:55",
          "type": "guest_call",
          "quote": "Real estate Agent.",
          "insight": "Susan's emotional state at discovery was disorientation. The test measures whether the redesigned landing page replaces disorientation with clarity."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Discoverability",
          "type": "book",
          "quote": "Two of the most important characteristics of good design are discoverability and understanding.",
          "insight": "Understanding has an emotional dimension. The sentiment test captures whether understanding produces a positive emotional response (clarity) or a neutral one (comprehension without engagement)."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 20 participants. Show the landing page for 10 seconds. Ask: (1) 'What does this service do?' (comprehension), (2) 'How did you feel when you first saw the page? (a) Confused, (b) Neutral, (c) I got it immediately, (d) Intrigued' (sentiment). Also A/B test the headline: variant A ('Book the nights you need. Skip the ones you don't.') versus variant B (a more conventional alternative) to isolate the headline's contribution to comprehension relief.",
      "success_criteria": "At least 70% select 'I got it immediately' or 'Intrigued.' Fewer than 10% select 'Confused.' Comprehension accuracy above 75%. The designed headline (variant A) outperforms variant B on both comprehension and positive sentiment by at least 15 percentage points.",
      "failure_meaning": "If 'Confused' exceeds 10%, the landing page is still incoherent for a significant minority. If comprehension is high but sentiment is neutral, the page communicates the model without producing the emotional payoff of clarity. If variant A underperforms, the casual, direct tone may not resonate with the target audience.",
      "implementation_hint": "Include demographic diversity in the sample to test whether the headline's casual tone resonates across age groups, housing-search experience levels, and cultural backgrounds."
    },
    {
      "id": "tests-028",
      "type": "validation_strategy",
      "title": "Protective Vigilance Acknowledgment Test",
      "validates_element": "feels-002",
      "journey_phases": ["listing_evaluation"],
      "problem": "The evidence-based trust tone (matter-of-fact, not reassuring) may feel cold or impersonal to some guests, while the cheerful reassurance it replaces may feel fake to vigilant guests like Susan. The right tone depends on the guest's trust state.",
      "solution": "Run a split copy test showing the listing page with evidence-based copy versus reassurance-based copy to two groups. Measure trust perception and willingness to proceed.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 2:22",
          "type": "guest_call",
          "quote": "You know how sometimes people give you all kind of run around.",
          "insight": "Susan's prior scam experience makes her resistant to reassurance. The test measures whether evidence-based copy outperforms reassurance copy for trust-anxious guests."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Affordances",
          "type": "book",
          "quote": "To be effective, affordances and anti-affordances have to be discoverable -- perceivable.",
          "insight": "Trust signifiers must be perceived as evidence, not marketing. The split test measures which copy style achieves this perception."
        }
      ],
      "priority": "medium",
      "validation_method": "a_b_test",
      "test_description": "Create two listing page variants. Variant A (evidence-based): 'Verified by Split Lease. Photos checked Feb 10, 2026. Cancel within 24 hours if the space does not match.' Variant B (reassurance-based): 'Rest easy! We have personally verified this beautiful space so you can book with confidence.' Show each variant to 100 participants. Measure: (1) 'How trustworthy does this listing feel?' (1-5), (2) 'Would you proceed with booking?' (yes/maybe/no), (3) open-ended: 'What would make you feel more confident?'",
      "success_criteria": "Variant A achieves a higher average trust score (by at least 0.5 points) and a higher 'Would proceed' rate (by at least 15 percentage points) than Variant B. Open-ended responses for Variant A request more evidence (more photos, more reviews) rather than more reassurance, confirming the evidence-seeking mental model.",
      "failure_meaning": "If Variant B outperforms, the evidence-based tone is too clinical for the audience and needs warmth added. If both perform equally, the tone is less important than the information content, and the focus should shift from copy style to information completeness.",
      "implementation_hint": "Segment results by self-reported housing experience: frequent Airbnb users, first-time shared housing seekers, and users who have experienced housing scams. The tone effectiveness may vary by segment."
    },
    {
      "id": "tests-029",
      "type": "validation_strategy",
      "title": "Agency Sensation During Proposal Configuration Test",
      "validates_element": "feels-003",
      "journey_phases": ["proposal_creation"],
      "problem": "The emotional target is 'agency' -- the feeling of being in control. But the preset-based selectors constrain choices by design. If guests feel the presets limit rather than empower them, the emotional target is missed even if the functional design is correct.",
      "solution": "Run a post-task emotional survey after participants complete a proposal configuration. Measure perceived control and choice satisfaction.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 2:34",
          "type": "guest_call",
          "quote": "I like to see the place and I can give you a deposit on her behalf.",
          "insight": "Susan offers terms on her own initiative. She expects to be in control of the proposal. The survey measures whether the preset-based interface supports or undermines this sense of control."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Mapping",
          "type": "book",
          "quote": "A device is easy to use when the set of possible actions is visible.",
          "insight": "Visible actions produce the feeling of control. The test validates whether the presets make the available actions feel visible and abundant rather than restricted."
        }
      ],
      "priority": "medium",
      "validation_method": "usability_test",
      "test_description": "Recruit 12 participants. After they complete a proposal configuration using the preset selectors, administer a 5-question survey: (1) 'I felt in control of the configuration' (1-5 agree), (2) 'The options matched my needs' (1-5), (3) 'I could see the impact of my choices immediately' (1-5), (4) 'I felt the system understood what I wanted' (1-5), (5) open-ended: 'Was there anything you wanted to do but could not?'",
      "success_criteria": "Average score of 4+ on questions 1-4. At least 75% of participants report no unmet needs in the open-ended response. The 'impact of choices' question scores highest (4.5+), validating that the real-time translation summary produces the agency feeling.",
      "failure_meaning": "If 'in control' scores below 4, the presets feel restrictive rather than empowering. If 'options matched needs' is low, the preset selection is too narrow. If 'impact of choices' is low, the real-time summary is not providing the immediate feedback that produces the sense of agency.",
      "implementation_hint": "Include participants with non-standard needs (e.g., 3 nights per week, alternating weeks) to test whether the Custom path feels as empowering as the presets."
    },
    {
      "id": "tests-030",
      "type": "validation_strategy",
      "title": "Acceptance Screen Confidence and Next-Step Clarity Test",
      "validates_element": "feels-004",
      "journey_phases": ["acceptance"],
      "problem": "The emotional target is 'grounded confidence' -- knowing exactly what was committed to and what happens next. If the acceptance screen omits key terms, fails to restate the cancellation guarantee, or uses vague next-step language, the guest leaves the screen feeling uncertain rather than confident.",
      "solution": "Run a comprehension and sentiment test on the acceptance confirmation screen. Measure whether guests can recall key terms and next steps immediately after viewing.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 6:50-6:57",
          "type": "guest_call",
          "quote": "So I'll reach out to your father since, let me see if I can get my friend.",
          "insight": "The call ends with ambiguous next steps. The acceptance screen must prevent this ambiguity. The test measures whether guests leave the screen with clear, accurate knowledge of what happens next."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Conceptual Models",
          "type": "book",
          "quote": "Good conceptual models are the key to understandable, enjoyable products.",
          "insight": "The acceptance screen IS the conceptual model for what comes next. The test validates whether this model is complete and correctly understood."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Recruit 12 participants. Show the acceptance confirmation screen for 15 seconds. Then hide it and ask: (1) 'What schedule did you book?' (2) 'What is the total cost?' (3) 'Can you cancel? If so, when?' (4) 'What happens before your first night?' Measure accuracy of recall for each question.",
      "success_criteria": "At least 85% correctly recall the schedule. At least 80% recall the total cost within 10% accuracy. At least 90% know they can cancel within 24 hours. At least 75% can describe at least one specific next step (e.g., 'check-in details arrive 24 hours before').",
      "failure_meaning": "If schedule or cost recall is low, the terms summary is not prominent enough. If cancellation guarantee recall is below 90%, the guarantee is not sufficiently emphasized at the moment of commitment. If next-step recall is low, the 'What happens next' timeline is not communicating effectively.",
      "implementation_hint": "Compare with a control that shows a more conventional acceptance screen (less structured, no timeline) to measure the incremental value of the designed acceptance format."
    },
    {
      "id": "tests-031",
      "type": "validation_strategy",
      "title": "Move-In Safety Feeling Assessment",
      "validates_element": "feels-005",
      "journey_phases": ["move_in"],
      "problem": "The emotional target is 'safety' -- feeling guided, protected, and able to exit. But the move-in experience is physical and emotional, not just digital. The House Manual and checklist may work perfectly on screen but fail to produce the feeling of safety if the guest is overwhelmed by the physical environment.",
      "solution": "Administer a brief emotional check-in survey to guests 2 hours after their first check-in. Measure emotional state and perceived support.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 3:42",
          "type": "guest_call",
          "quote": "I understand that she's going to come from Oregon. I don't want to have to book a ticket and come if...",
          "insight": "Susan's anxiety is specifically about her friend being alone at move-in. The emotional check-in measures whether the friend (or a similar guest) actually feels supported and safe after arriving."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Discoverability",
          "type": "book",
          "quote": "Discoverability: Is it possible to even figure out what actions are possible?",
          "insight": "Safety at move-in comes from discoverability: the guest can figure out what to do at each step. The emotional survey captures whether this discoverability translated into a felt sense of safety."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "Send a brief in-app survey to guests 2 hours after their first check-in. Questions: (1) 'How supported did you feel during check-in?' (1-5), (2) 'Did you know what to do at each step?' (1-5), (3) 'Did you know you could cancel if the space did not match?' (yes/no), (4) 'What would have made you feel more supported?' (open-ended). Run for 4 weeks, targeting at least 30 responses.",
      "success_criteria": "Average 'supported' rating of 4+. Average 'knew what to do' rating of 4+. At least 80% aware of the cancellation option. Open-ended responses focus on minor improvements (e.g., 'better Wi-Fi instructions') rather than fundamental safety concerns (e.g., 'I felt abandoned').",
      "failure_meaning": "If 'supported' is below 4, the House Manual is not serving as the emotional surrogate for a trusted companion. If 'knew what to do' is below 4, the chronological sequence has gaps or the guest did not access the manual. If cancellation awareness is below 80%, the persistent banner is not working at move-in.",
      "implementation_hint": "Time the survey carefully: 2 hours is enough for the guest to have completed check-in and the verification checklist, but early enough that the experience is fresh. Do not survey at check-in itself -- the guest is too busy."
    },
    {
      "id": "tests-032",
      "type": "validation_strategy",
      "title": "Active Lease Notification Fatigue Monitoring",
      "validates_element": "feels-006",
      "journey_phases": ["active_lease"],
      "problem": "Over an 8-week stay with 5 nights per week, the guest receives 8+ weekly confirmations, cleaning reminders, and review prompts. If these are too frequent or too loud, the guest develops notification fatigue and disengages from the platform entirely. Norman warns that too much feedback is as harmful as too little.",
      "solution": "Monitor notification engagement metrics over the full length of multi-week stays. Track whether engagement decays over time and at what week fatigue sets in.",
      "evidence": [
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Feedback",
          "type": "book",
          "quote": "Too much feedback can be even more annoying than too little.",
          "insight": "Norman's warning directly applies to the active lease phase. The monitoring system measures whether the platform's feedback volume reaches the annoyance threshold."
        },
        {
          "source": "Susan  Bryant customer call.txt, 4:31",
          "type": "guest_call",
          "quote": "She could stay with a friend of mine if he need the space on the weekend, that would work.",
          "insight": "The friend has an alternative. If notification fatigue pushes the platform's friction above the alternative's friction, the guest defects. Engagement decay is the leading indicator."
        }
      ],
      "priority": "medium",
      "validation_method": "analytics",
      "test_description": "For all guests with stays of 4+ weeks, track weekly: (1) notification open rate, (2) notification dismissal rate, (3) notification mute/disable rate, (4) Stays Manager visit frequency, (5) overall app engagement. Plot these metrics week-over-week to identify the decay curve. Segment by notification type (confirmation, cleaning reminder, review prompt, payment receipt).",
      "success_criteria": "Notification open rate does not decline by more than 20% from week 1 to week 4. Notification mute rate stays below 10% across the full stay. Stays Manager visit frequency remains stable (within 15% variance) throughout the stay. No notification type has a mute rate above 15%.",
      "failure_meaning": "If open rate declines by more than 20%, notification volume or urgency is excessive. If mute rate exceeds 10%, specific notification types are annoying and need to be quieter. If Stays Manager visits decline sharply, the guest is disengaging from the platform overall -- a precursor to defection.",
      "implementation_hint": "Build a dashboard that shows these metrics in real time, segmented by stay duration (4-week vs. 8-week vs. 12-week stays). Use the data to dynamically adjust notification frequency: if engagement is declining, automatically reduce notification volume for that user."
    },
    {
      "id": "tests-033",
      "type": "validation_strategy",
      "title": "Proxy Coordinator Recognition Sentiment Test",
      "validates_element": "feels-007",
      "journey_phases": ["proposal_creation", "active_lease"],
      "problem": "The emotional target is 'recognized' -- the coordinator feels seen and valued as a partner in the booking. But the platform may functionally support proxy booking without producing this emotional payoff. If Susan completes the booking but feels like an afterthought, the emotional design has failed.",
      "solution": "Run a post-booking sentiment survey for proxy coordinators. Measure whether they feel recognized, valued, and included or whether they feel like they are working around the system.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 0:11",
          "type": "guest_call",
          "quote": "I have a friend of mine who was coming from Oregon.",
          "insight": "Susan introduces herself through her friend's need. Her identity in this booking is 'helper.' The survey measures whether the platform honors this identity."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Signifiers",
          "type": "book",
          "quote": "People need some way of understanding the product or service they wish to use.",
          "insight": "The coordinator needs to understand that the product was built for her role, not adapted as an afterthought. The survey captures whether this understanding is achieved."
        }
      ],
      "priority": "low",
      "validation_method": "usability_test",
      "test_description": "After proxy coordinators complete a booking, send a brief survey: (1) 'Did the platform feel designed for someone booking on behalf of another person?' (1-5), (2) 'Did you feel included and informed throughout the process?' (1-5), (3) 'Would you use this platform to book for someone else again?' (yes/maybe/no), (4) 'What would have made you feel more included?' (open-ended). Target 20 responses over 8 weeks.",
      "success_criteria": "Average score of 4+ on questions 1-2. At least 80% say 'yes' to repeat use. Open-ended responses suggest minor improvements rather than fundamental role-exclusion complaints.",
      "failure_meaning": "If 'designed for this' scores below 4, the proxy flow still feels like a workaround. If 'included and informed' is low, the coordinator notifications are insufficient. If repeat use intent is below 80%, the experience was functional but not emotionally satisfying.",
      "implementation_hint": "Compare proxy coordinator satisfaction with the occupant's satisfaction for the same booking to identify whether both roles are equally served or one is neglected."
    },
    {
      "id": "tests-034",
      "type": "validation_strategy",
      "title": "End-to-End Guest Journey Conversion Funnel Analysis",
      "validates_element": "journey-level",
      "journey_phases": ["discovery", "search", "listing_evaluation", "proposal_creation", "negotiation", "acceptance", "move_in", "active_lease"],
      "problem": "Individual elements may validate successfully in isolation but fail to produce a coherent journey experience. The cumulative effect of all 33 elements across 8 phases must be measured as an integrated system, not just as individual components. A guest who clears each phase's test but abandons the overall journey reveals a coherence failure that phase-level tests cannot detect.",
      "solution": "Instrument the full guest funnel with phase-to-phase conversion tracking, measuring dropout rates at each transition and correlating dropouts with the specific elements encountered.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt (full transcript)",
          "type": "guest_call",
          "quote": "Multiple timestamps across the full call demonstrate that Susan's journey is one continuous decision arc, not isolated phases.",
          "insight": "The call reveals that trust built in one phase carries into the next. The funnel analysis measures whether the platform's phased element design produces this same cumulative trust-building effect."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 The System Image",
          "type": "book",
          "quote": "When the system image is incoherent or inappropriate, then the user cannot easily use the device.",
          "insight": "Coherence is a system-level property, not a component-level property. The funnel analysis is the system-level test that validates coherence."
        }
      ],
      "priority": "high",
      "validation_method": "analytics",
      "test_description": "Implement end-to-end conversion tracking for the guest journey. For each phase transition (discovery-to-search, search-to-listing-evaluation, evaluation-to-proposal, proposal-to-negotiation, negotiation-to-acceptance, acceptance-to-move-in, move-in-to-active-lease), track: (1) conversion rate, (2) time between phases, (3) dropout rate, (4) which elements were engaged before dropout. Run continuously. Segment by: first-time vs. returning, agent-referred vs. organic, proxy vs. self-booking.",
      "success_criteria": "Discovery-to-search: above 50%. Search-to-listing-evaluation: above 40%. Evaluation-to-proposal: above 15% for unassisted guests. Proposal-to-acceptance: above 40%. Acceptance-to-move-in: above 90%. Move-in first-night cancellation rate: below 5%. Active lease retention past week 2: above 85%.",
      "failure_meaning": "Any phase-to-phase conversion rate below the target indicates that the transition between phases is losing guests. The element engagement data before dropout identifies which specific elements are associated with abandonment. A high acceptance-to-move-in conversion but high first-night cancellation rate indicates that listing signifiers over-promise relative to reality.",
      "implementation_hint": "Build a funnel dashboard that is reviewed weekly. Set automated alerts when any phase-to-phase conversion drops below the target by more than 10 percentage points. Correlate drops with recent design changes to identify regression."
    },
    {
      "id": "tests-035",
      "type": "validation_strategy",
      "title": "Cross-Phase Trust Continuity Assessment",
      "validates_element": "journey-level",
      "journey_phases": ["listing_evaluation", "negotiation", "acceptance", "move_in"],
      "problem": "Trust is built across phases, not within them. The coherence report identifies that trust verification demands recur across phases (listing evaluation, negotiation, acceptance, move-in) because the platform lacks structural signifiers that carry trust from one phase to the next. If trust built during listing evaluation does not carry into negotiation and move-in, the guest must rebuild trust at each phase -- an exhausting and abandonment-prone experience.",
      "solution": "Run a longitudinal trust perception survey at 4 points in the journey: after listing evaluation, after negotiation, after acceptance, and after first night. Measure whether trust increases, stays stable, or drops across phases.",
      "evidence": [
        {
          "source": "Susan  Bryant customer call.txt, 2:14, 3:05, 3:42",
          "type": "guest_call",
          "quote": "Susan demands verification three separate times across the call.",
          "insight": "Susan's repeated demands show that trust is not built once and retained -- it must be continuously reinforced. The longitudinal survey tests whether the platform's phased trust reinforcement (badge at evaluation, confirmed term at negotiation, restatement at acceptance, banner at move-in) achieves the desired cumulative effect."
        },
        {
          "source": "donnorman-affordances-signifiers-feedback.txt, Ch.1 Signifiers",
          "type": "book",
          "quote": "Signifiers must be perceivable, else they fail to function.",
          "insight": "Trust signifiers must be perceivable at EVERY phase, not just at listing evaluation. The longitudinal survey tests whether perceivability is maintained across the full trust-critical journey segment."
        }
      ],
      "priority": "high",
      "validation_method": "usability_test",
      "test_description": "For guests progressing through the journey, administer a 2-question trust survey at 4 points: (1) after viewing a listing, (2) after submitting a proposal, (3) after acceptance, (4) after the first night. Questions: 'How confident are you that this space is as described?' (1-5) and 'How protected do you feel if something goes wrong?' (1-5). Target 30 guests completing all 4 touchpoints over 12 weeks.",
      "success_criteria": "Trust scores increase or remain stable from phase to phase for at least 80% of guests. Average trust score at move-in (phase 4) is at least 4.0. No phase shows a statistically significant trust decrease from the preceding phase. The 'protected' question scores above 4.0 at all phases after listing evaluation.",
      "failure_meaning": "If trust drops at any transition, the trust signifiers at that phase are not reinforcing the trust built in the prior phase. If trust drops specifically at move-in, the listing's signifiers over-promise relative to the physical reality. If the 'protected' score drops at any point, the cancellation guarantee is not being perceived as a continuous safety net.",
      "implementation_hint": "Embed the survey in the natural flow (e.g., a single-question prompt after key actions) to minimize survey fatigue. Keep each touchpoint to 30 seconds maximum. Offer a small incentive for completing all 4 touchpoints."
    }
  ]
}